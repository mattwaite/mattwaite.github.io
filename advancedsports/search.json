[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Sports Data Analysis",
    "section": "",
    "text": "1 Introduction\nThe 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic.\nFor the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner.\nHow close?\nThis is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved.\nThe specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here.\nFirst, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good.\nSecond, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good.\nThird, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away.\nUsing logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t.\nSo how much of a corner would the team need to turn?\nWith modeling, we can figure this out.\nWhat would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent?\nAs played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa).\nSo how close are they to turning the corner? That close."
  },
  {
    "objectID": "index.html#requirements-and-conventions",
    "href": "index.html#requirements-and-conventions",
    "title": "Advanced Sports Data Analysis",
    "section": "1.1 Requirements and Conventions",
    "text": "1.1 Requirements and Conventions\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nDon’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Advanced Sports Data Analysis",
    "section": "1.2 About this book",
    "text": "1.2 About this book\nThis book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the author is copyright Matt Waite 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\nAs such, the whole book – authored in Quarto – is open sourced on Github. Pull requests welcomed!"
  },
  {
    "objectID": "installations.html",
    "href": "installations.html",
    "title": "2  Installations",
    "section": "",
    "text": "You’re going to do things most of you aren’t used to doing with your computer in this class. In order to do that, you need to clean up your computer. I’ve seen what your computer looks like. It’s disgusting.\n\n2.0.1 Part 1: Update and patch your operating system\nOn a Mac:\n\nOpen System Preferences. Then click on Software Update:\n\n\n\nCheck and see if you have the latest version of the Mac OS installed. If your computer says “Your Mac is up to date”, then you’re good to go, regardless of what comes next.\n\n\n\n\nThe latest version of the Mac OS is called Ventura.\n\n\n\nIf you aren’t on Ventura and you can update to it, you should do it. This will take some time – hours, so don’t do it when you need your laptop – but it’s important for you and your computer to stay up to date on operating systems.\nWhen you’re done, make sure you click the Automatically keep my Mac up to date box and install those updates regularly. Don’t ignore them. Don’t snooze them. Install them.\nWith an up-to-date operating system, now install the command line tools. To do this, click on the magnifying glass in the top right of the screen and type terminal. Hit enter – the first entry is the terminal app.\nIn the terminal app, type xcode-select --install and hit enter. Let it run.\n\n\nOn Windows:\n\nType Updates into the Cortana search then click Check for updates\n\n\n\nAfter the search for updates completes, apply any that you have. Depending on if you’d done this recently or if you have automatic updates set, this might take a long time or go very quickly.\n\n\n3.  When you're done, make sure you set up automatic updates for your Windows machine and install those updates regularly. Don't ignore them. Don't snooze them. Install them.\n\n\n2.0.2 Part 2: Install R and R Studio\nOn a Mac:\n\nDownload the latest version of R from CRAN for Mac.\n\n\n\n\nIf it doesn’t open automatically, double click on the file that downloads to your downloads folder, click okay and accept the defaults and the license agreement.\nDownload the latest R Studio for Mac under Step 2. The number will be different from the screenshot below, but the process is the same.\nClick, hold and drag the RStudio icon into the Applications folder shortcut.\n\n\n\nTo get the RStudio icon to appear in your dock – you are going to use Rstudio for every single class we have this semester, so it would make sense – open a Finder window, go to your applications, open R Studio there, and then drag the icon to where you want it to appear in your dock. It will stay there after you have quit the program. To get rid of it after the semester is over, just drag the icon far enough out of the dock until you see a cloud icon appear.\n\nOn Windows:\n\nDownload R 4.2.2 from CRAN for Windows. The numbers in the screenshots below are 4.0.2, but the process is the same.\n\n\n\n\n\nOpen and run the executable, accept the defaults and license agreement.\n\n\n\nGo back to the screen where you downloaded the base R language, then download and install R Tools.\nDownload R Studio for Windows on step 2 of this page. Open the executable the same way you opened R. Hit next until it starts installing.\nYou can find it by typing RStudio into the Cortana search.\n\n\n\n2.0.3 Part 3: Installing R libraries\n\nOpen R Studio. It should show the Console view by default. We’ll talk a lot more about the console later.\nCopy and paste this into the console and hit enter:\n\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"lubridate\", \"janitor\", \"cowplot\", \"learnr\", \"remotes\", \"devtools\", \"hoopr\", \"nflfastR\", \"cfbfastR\", \"rvest\", \"Hmisc\", \"cluster\", \"tidymodels\", \"bonsai\"))\n\n\n\n2.0.4 Part 4: Install Slack\n\nInstall Slack on your computer and your phone (you can find Slack in whatever app store you use). The reason I want it on both is because you are going to ask me for help with code via Slack. Do not use screenshots unless specifically asked. I want you to copy and paste your code. You can’t do that on a phone. So you need the desktop version. But I can usually solve your problem within a few minutes if you respond right away, and I know that you have your phone on you and are checking it. So the desktop version is for work, the phone version is for notifications.\nEmail me the address you want connected to Slack. Use one you’ll actually check.\nWhen you get the Slack invitation email, log in to the class slack via the apps, not the website.\nAdd the #r channel for general help I’ll send to everyone in the channel and, if you want, the #jobstuff channel for news about jobs I come across."
  },
  {
    "objectID": "logistic-regression.html#the-basics",
    "href": "logistic-regression.html#the-basics",
    "title": "3  Modeling and logistic regression",
    "section": "3.1 The basics",
    "text": "3.1 The basics\nOne of the most common – and seemingly least rigorous – parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method – looking at how a team is playing, looking at the players, consulting their gut – but rarely ever do you hear of a sports pundit using a model.\nWe’re going to change that. Throughout this class, you’ll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not).\nThere are lots of problems in the world where the answer is not a number but a classification: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not?\nThese are problems of classification and there are algorithms we can use to estimate the probability that X will be the outcome. How likely is it that this team with these stats will win this game?\nWhere this gets interesting is in the middle.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\nlibrary(zoo)\nlibrary(gt)\nset.seed(1234)\n\nWhat we need to do here is get both sides of the game. We’ll start with getting the box scores and cleaning them up a bit. We’ll split the shooting columns into made and missed and turn everything into a number.\n\nteamgames <- load_mbb_team_box(seasons = 2015:2023) %>%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %>%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %>%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %>%\n  mutate_at(12:34, as.numeric)"
  },
  {
    "objectID": "logistic-regression.html#feature-engineering",
    "href": "logistic-regression.html#feature-engineering",
    "title": "3  Modeling and logistic regression",
    "section": "3.2 Feature engineering",
    "text": "3.2 Feature engineering\nFeature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good.\nA number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score?\nOne problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this:\nPossessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts)\nIf you look at the data we already have, however, you’ll see possessions are not actually in the data. Which is unfortunate. But we can calculate it pretty easily.\nThen we’ll use the possessions estimate formula to get that, so we can then calculate points per possession.\nWe’ll save that to a new dataframe called teamstats.\n\n3.2.1 Exercise 1: setting up your data\n\nteamstats <- teamgames %>% \n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = ?????_?????_attempted - offensive_rebounds + ????????? + (.475 * free_throws_attempted),\n    ppp = team_score/possessions\n  )\n\n\n\n\nNow we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar.\n\nSplit your data into training and testing data sets. A common split is 80/20.\nTrain the model on the training dataset.\nEvaluate the model on the training data.\nApply the model to the testing data.\nEvaluate the model on the test data.\n\nFrom there, it’s how you want to use the model. We’ll walk through a simple example here, using a simple model – a logistic regression model.\nWhat we’re trying to do here is predict which team will win given their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats BEFORE the game – what we knew about the team going into the game in question. We can do that using zoo and rolling means.\nA rolling mean is an average in a window of time. So if we averaged together the points per possession over 10 games, that’s a 10 game rolling mean. The first real mean would be games 1-10. Then the window would shift one game with game 11, and the average would be games 2-11. Then 3-12, 4-13 and so on.\nWe’ll add three new columns – the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage.\nThe problem we have to face here is that with our data, a rolling mean of games 6-15 would mean if we were trying to predict game 15, we couldn’t include game 15. We’d have to look at games 5-14. If we included game 15, it would mean we had God like abilities to predict the future.\nWe do not. Introducing the lag function. The lag function just takes the window of data and shifts it however many spots back you want to shift it. In our case, we want to shift it one game back. And we’re going to make a rolling window of 5 games.\n\n\n3.2.2 Exercise 2: Lagging\n\nrollingteamstats <- teamstats %>% \n  arrange(game_date) %>%\n  group_by(team_short_display_name, season) %>%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    team_rolling_ppp = rollmean(???(ppp, n=?), k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup()\n\n\n\n\nNow we need to do something that at first will seem kind of odd, but isn’t when you think about it. Our data has half of the box score – just one team. But a game has two teams in it. To get it, we need the team AND the opponent. How can you decide if a team is going to win if you don’t know who they are playing and if that team is any good or not? So we need to create two dataframes that have column names that indicate these stats are the team stats and these stats are the opponent stats. We can do that with some selecting and some renaming.\n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    ) %>% \n  na.omit()\n\nopponent_side <- team_side %>%\n  select(-opponent_id) %>% \n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_id)\n)\n\nNow we’ll join them together.\n\n\n3.2.3 Exercise 3: Joining the data together\n\ngames <- ????_side %>% inner_join(?????????_side)\n\n\n\nJoining, by = c(\"game_id\", \"opponent_id\", \"game_date\", \"season\")\n\n\nThe last problem to solve? Who won? We can add this with conditional logic. The other thing we’re doing here is we’re going to is we’re going to convert our new team_result column into a factor. What is a factor? A factor is a type of data in R that stores categorical values that have a limited number of differences. So wins and losses are a perfect factor. Modeling libraries are looking for factors so it can treat the differences in the data as categories, so that’s why we’re converting it here.\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n))) %>% na.omit()\n\nNow that we’ve done that, we need to look at the order of our factors.\n\n\n3.2.4 Exercise 4: Looking at the factors\nTo do that, we first need to know what R sees when it sees our team_result factor. Is a win first or is a loss first?\n\nlevels(games$????_??????)\n\n\n\n[1] \"L\" \"W\"\n\n\nThe order listed here is the order they are in. What this means is that our predictions will be done through the lens of losses. That doesn’t make intuitive sense to us. We want to know who will win! We can reorder the factors with relevel.\n\n\n3.2.5 Exercise 5: Releveling the factors\n\ngames$team_result <- relevel(games$team_result, ref=\"?\")\n\nlevels(games$team_result)\n\n\n\n[1] \"W\" \"L\"\n\n\nFor simplicity, let’s limit the number of columns we’re going to feed our model.\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>% na.omit()"
  },
  {
    "objectID": "logistic-regression.html#visualizing-the-decision-boundary",
    "href": "logistic-regression.html#visualizing-the-decision-boundary",
    "title": "3  Modeling and logistic regression",
    "section": "3.3 Visualizing the decision boundary",
    "text": "3.3 Visualizing the decision boundary\nThis is just one dimension of the data, but it can illustrate how this works. You can almost see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. You can read it like this: If this team shoots this well and the opponent shoots this well, most of the time this team wins. Or loses. It just depends on where the dot ends up.\nThat neatly captures the probabilities we’re looking at here.\n\nggplot() + \n  geom_point(\n    data=games, aes(x=team_rolling_ppp, y=opponent_rolling_ppp, color=team_result))"
  },
  {
    "objectID": "logistic-regression.html#the-logistic-regression",
    "href": "logistic-regression.html#the-logistic-regression",
    "title": "3  Modeling and logistic regression",
    "section": "3.4 The logistic regression",
    "text": "3.4 The logistic regression\nTo create a model, we have to go through a process. That process starts with splitting data where we know the outomes into two groups – training and testing. The training data is what we will use to create our model. The testing data is how we will determine how good it is. Then, going forward, our model can predict games we haven’t seen yet.\nTo do this, we’re going to first split our modelgames data into two groups – with 80 percent of it in one, 20 percent in the other. We do that by feeding our simplified dataframe into the initial_split function. Then we’ll explicitly name those into new dataframes called train and test.\n\n3.4.1 Exercise 6: What are we splitting?\n\nlog_split <- initial_split(?????????, prop = .8)\nlog_train <- training(log_split)\nlog_test <- testing(log_split)\n\n\n\n\nNow we have two dataframes – log_train and log_test – that we can now use for modeling.\nFirst step to making a model is to set what type of model this will be. We’re going to name our model object – log_mod works because this is a logistic regression model. We’ll use the logistic_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “glm”. The mode in our case is “classification” because we’re trying to classify something as a W or L. Later, we’ll use “regression” to predict numbers.\n\nlog_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nThe next step is to create a recipe. This is a series of steps we’ll use to put our data into our model. For example – what is predicting what? And what aren’t predictors and what are? And do we have to do any pre-processing of the data?\nThe first part of the recipe is the formula. In this case, we’re saying – in real words – team_result is approximately modeled by our predictors, which we represent as . which means all the stuff. Then, importantly, we say what isn’t a predictor next with update_role. So the team name, the game date and things like that are not predictors. So we need to tell it that. The last step is normalizing our numbers. With logistic regression, scale differences in numbers can skew things, so we’re going to turn everything into Z-scores.\n\nlog_recipe <- \n  recipe(team_result ~ ., data = log_train) %>% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %>%\n  step_normalize(all_predictors())\n\nsummary(log_recipe)\n\n# A tibble: 8 × 4\n  variable                    type    role      source  \n  <chr>                       <chr>   <chr>     <chr>   \n1 game_id                     numeric ID        original\n2 game_date                   date    ID        original\n3 team_short_display_name     nominal ID        original\n4 opponent_short_display_name nominal ID        original\n5 season                      numeric ID        original\n6 team_rolling_ppp            numeric predictor original\n7 opponent_rolling_ppp        numeric predictor original\n8 team_result                 nominal outcome   original\n\n\nNow we have enough for a workflow. A workflow is what we use to put it all together. In it, we add our model definition and our recipe.\n\n\n3.4.2 Exercise 7: Making a workflow\n\nlog_workflow <- \n  workflow() %>% \n  add_model(log_???) %>% \n  add_recipe(log_??????)\n\n\n\n\nAnd now we fit our model (this can take a few minutes).\n\nlog_fit <- \n  log_workflow %>% \n  fit(data = log_train)"
  },
  {
    "objectID": "logistic-regression.html#evaluating-the-fit",
    "href": "logistic-regression.html#evaluating-the-fit",
    "title": "3  Modeling and logistic regression",
    "section": "3.5 Evaluating the fit",
    "text": "3.5 Evaluating the fit\nWith logistic regression, there’s two things we’re looking at: The prediction and the probabilities. We can get those with two different fits and combine them together.\nFirst, you can see the predictions like this:\n\ntrainpredict <- log_fit %>% predict(new_data = log_train) %>%\n  bind_cols(log_train)\n\ntrainpredict\n\n# A tibble: 61,020 × 9\n   .pred_class   game_id game_date  team_short_display_… opponent_short_… season\n   <fct>           <int> <date>     <chr>                <chr>             <int>\n 1 W           401083976 2019-02-03 UNC Wilmington       James Madison      2019\n 2 L           400839389 2016-02-11 Nebraska             Wisconsin          2016\n 3 W           400988107 2018-02-10 Wichita State        UConn              2018\n 4 L           401373737 2022-02-25 UC Davis             UCSB               2022\n 5 W           401309755 2021-03-05 Cincinnati           Vanderbilt         2021\n 6 L           401377777 2022-02-27 Temple               Tulane             2022\n 7 L           400868397 2016-03-04 Manhattan            Marist             2016\n 8 L           400988599 2018-02-04 Seton Hall           Villanova          2018\n 9 W           401172379 2020-01-18 Towson               James Madison      2020\n10 W           400847285 2016-02-27 South Dakota St      Oral Roberts       2016\n# … with 61,010 more rows, and 3 more variables: team_rolling_ppp <dbl>,\n#   opponent_rolling_ppp <dbl>, team_result <fct>\n\n\nThen, we can just add it to trainpredict using bind_cols, which means we’re going to bind the columns of this new fit to the old trainpredict.\n\ntrainpredict <- log_fit %>% predict(new_data = log_train, type=\"prob\") %>%\n  bind_cols(trainpredict)\n\ntrainpredict\n\n# A tibble: 61,020 × 11\n   .pred_W .pred_L .pred_class   game_id game_date  team_short_display_name\n     <dbl>   <dbl> <fct>           <int> <date>     <chr>                  \n 1   0.688   0.312 W           401083976 2019-02-03 UNC Wilmington         \n 2   0.430   0.570 L           400839389 2016-02-11 Nebraska               \n 3   0.618   0.382 W           400988107 2018-02-10 Wichita State          \n 4   0.346   0.654 L           401373737 2022-02-25 UC Davis               \n 5   0.506   0.494 W           401309755 2021-03-05 Cincinnati             \n 6   0.289   0.711 L           401377777 2022-02-27 Temple                 \n 7   0.262   0.738 L           400868397 2016-03-04 Manhattan              \n 8   0.176   0.824 L           400988599 2018-02-04 Seton Hall             \n 9   0.715   0.285 W           401172379 2020-01-18 Towson                 \n10   0.622   0.378 W           400847285 2016-02-27 South Dakota St        \n# … with 61,010 more rows, and 5 more variables:\n#   opponent_short_display_name <chr>, season <int>, team_rolling_ppp <dbl>,\n#   opponent_rolling_ppp <dbl>, team_result <fct>\n\n\nThere’s several metrics to look at to evaluate the model on our training data, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. The inputs here are the data, the column that has the actual result, and the column with the prediction, called .pred_class.\n\n3.5.1 Exercise 8: Metrics\n\nmetrics(trainpredict, ????_??????, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.617\n2 kap      binary         0.233\n\n\nSo how accurate is our model? If we’re looking for perfection, we’re far from it. But if we’re looking to make straight up win loss bets … we’re doing okay!\nAnother way to look at the results is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy.\n\n\n3.5.2 Exercise 9: Confusion matrix\n\ntrainpredict %>%\n  conf_mat(????_result, .pred_?????)\n\n\n\n          Truth\nPrediction     W     L\n         W 18923 11744\n         L 11647 18706"
  },
  {
    "objectID": "logistic-regression.html#comparing-it-to-test-data",
    "href": "logistic-regression.html#comparing-it-to-test-data",
    "title": "3  Modeling and logistic regression",
    "section": "3.6 Comparing it to test data",
    "text": "3.6 Comparing it to test data\nNow we can apply our fit to the test data to see how robust it is. If the metrics are similar, that’s good – it means our model is robust. If the metrics change a lot, that’s bad. It means our model is guessing.\n\ntestpredict <- log_fit %>% predict(new_data = log_test) %>%\n  bind_cols(log_test)\n\ntestpredict <- log_fit %>% predict(new_data = log_test, type=\"prob\") %>%\n  bind_cols(testpredict)\n\nAnd now some metrics on the test data.\n\n3.6.1 Exercise 10: Testing\n\nmetrics(????predict, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.614\n2 kap      binary         0.229\n\n\nHow does that compare to our training data? Is it lower? Higher? Are the changes large – like are we talking about single digit changes or double digit changes? The less it changes, the better.\nAnd now the confusion matrix.\n\ntestpredict %>%\n  conf_mat(team_result, .pred_class)\n\n          Truth\nPrediction    W    L\n         W 4639 2954\n         L 2929 4734\n\n\nHow does that compare to the training data?"
  },
  {
    "objectID": "logistic-regression.html#how-well-did-it-do-with-nebraska",
    "href": "logistic-regression.html#how-well-did-it-do-with-nebraska",
    "title": "3  Modeling and logistic regression",
    "section": "3.7 How well did it do with Nebraska?",
    "text": "3.7 How well did it do with Nebraska?\nLet’s grab predictions for Nebraska from both our test and train data and take a look.\n\nnutrain <- trainpredict %>% filter(team_short_display_name == \"Nebraska\" &  season == 2023)\n\nnutest <- testpredict %>% filter(team_short_display_name == \"Nebraska\" & season == 2023)\n\nbind_rows(nutrain, nutest) %>% \n  arrange(game_date) %>% \n  select(.pred_W, .pred_class, team_result, team_short_display_name, opponent_short_display_name) %>% \n  gt()\n\n\n\n\n\n  \n  \n    \n      .pred_W\n      .pred_class\n      team_result\n      team_short_display_name\n      opponent_short_display_name\n    \n  \n  \n    0.5497281\nW\nW\nNebraska\nFlorida St\n    0.5973002\nW\nW\nNebraska\nBoston College\n    0.5175988\nW\nW\nNebraska\nCreighton\n    0.4341151\nL\nL\nNebraska\nIndiana\n    0.2667989\nL\nL\nNebraska\nPurdue\n    0.4810879\nL\nL\nNebraska\nKansas St\n    0.3176467\nL\nW\nNebraska\nQueens\n    0.3050453\nL\nW\nNebraska\nIowa\n    0.3458684\nL\nL\nNebraska\nMichigan St\n    0.4699218\nL\nW\nNebraska\nMinnesota\n    0.4310008\nL\nL\nNebraska\nIllinois\n    0.2072669\nL\nL\nNebraska\nPurdue\n    0.3605986\nL\nW\nNebraska\nOhio State\n    0.2536178\nL\nL\nNebraska\nPenn State\n    0.3102129\nL\nL\nNebraska\nNorthwestern\n    0.2234788\nL\nL\nNebraska\nMaryland\n    0.3468551\nL\nL\nNebraska\nIllinois\n  \n  \n  \n\n\n\n\nBy our rolling metrics, are there any surprises? Should we have beaten Creighton or Iowa?\nHow could you improve this?"
  }
]