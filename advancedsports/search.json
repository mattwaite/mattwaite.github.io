[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Sports Data Analysis",
    "section": "",
    "text": "1 Introduction\nThe 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic.\nFor the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner.\nHow close?\nThis is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved.\nThe specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here.\nFirst, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good.\nSecond, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good.\nThird, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away.\nUsing logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t.\nSo how much of a corner would the team need to turn?\nWith modeling, we can figure this out.\nWhat would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent?\nAs played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa).\nSo how close are they to turning the corner? That close."
  },
  {
    "objectID": "index.html#requirements-and-conventions",
    "href": "index.html#requirements-and-conventions",
    "title": "Advanced Sports Data Analysis",
    "section": "1.1 Requirements and Conventions",
    "text": "1.1 Requirements and Conventions\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nDon’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Advanced Sports Data Analysis",
    "section": "1.2 About this book",
    "text": "1.2 About this book\nThis book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the author is copyright Matt Waite 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\nAs such, the whole book – authored in Quarto – is open sourced on Github. Pull requests welcomed!"
  },
  {
    "objectID": "installations.html",
    "href": "installations.html",
    "title": "2  Installations",
    "section": "",
    "text": "You’re going to do things most of you aren’t used to doing with your computer in this class. In order to do that, you need to clean up your computer. I’ve seen what your computer looks like. It’s disgusting.\n\n2.0.1 Part 1: Update and patch your operating system\nOn a Mac:\n\nOpen System Preferences. Then click on Software Update:\n\n\n\nCheck and see if you have the latest version of the Mac OS installed. If your computer says “Your Mac is up to date”, then you’re good to go, regardless of what comes next.\n\n\n\n\nThe latest version of the Mac OS is called Ventura.\n\n\n\nIf you aren’t on Ventura and you can update to it, you should do it. This will take some time – hours, so don’t do it when you need your laptop – but it’s important for you and your computer to stay up to date on operating systems.\nWhen you’re done, make sure you click the Automatically keep my Mac up to date box and install those updates regularly. Don’t ignore them. Don’t snooze them. Install them.\nWith an up-to-date operating system, now install the command line tools. To do this, click on the magnifying glass in the top right of the screen and type terminal. Hit enter – the first entry is the terminal app.\nIn the terminal app, type xcode-select --install and hit enter. Let it run.\n\n\nOn Windows:\n\nType Updates into the Cortana search then click Check for updates\n\n\n\nAfter the search for updates completes, apply any that you have. Depending on if you’d done this recently or if you have automatic updates set, this might take a long time or go very quickly.\n\n\n3.  When you're done, make sure you set up automatic updates for your Windows machine and install those updates regularly. Don't ignore them. Don't snooze them. Install them.\n\n\n2.0.2 Part 2: Install R and R Studio\nOn a Mac:\n\nDownload the latest version of R from CRAN for Mac.\n\n\n\n\nIf it doesn’t open automatically, double click on the file that downloads to your downloads folder, click okay and accept the defaults and the license agreement.\nDownload the latest R Studio for Mac under Step 2. The number will be different from the screenshot below, but the process is the same.\nClick, hold and drag the RStudio icon into the Applications folder shortcut.\n\n\n\nTo get the RStudio icon to appear in your dock – you are going to use Rstudio for every single class we have this semester, so it would make sense – open a Finder window, go to your applications, open R Studio there, and then drag the icon to where you want it to appear in your dock. It will stay there after you have quit the program. To get rid of it after the semester is over, just drag the icon far enough out of the dock until you see a cloud icon appear.\n\nOn Windows:\n\nDownload R 4.2.2 from CRAN for Windows. The numbers in the screenshots below are 4.0.2, but the process is the same.\n\n\n\n\n\nOpen and run the executable, accept the defaults and license agreement.\n\n\n\nGo back to the screen where you downloaded the base R language, then download and install R Tools.\nDownload R Studio for Windows on step 2 of this page. Open the executable the same way you opened R. Hit next until it starts installing.\nYou can find it by typing RStudio into the Cortana search.\n\n\n\n2.0.3 Part 3: Installing R libraries\n\nOpen R Studio. It should show the Console view by default. We’ll talk a lot more about the console later.\nCopy and paste this into the console and hit enter:\n\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"lubridate\", \"janitor\", \"cowplot\", \"learnr\", \"remotes\", \"devtools\", \"hoopr\", \"nflfastR\", \"cfbfastR\", \"rvest\", \"Hmisc\", \"cluster\", \"tidymodels\", \"bonsai\"))\n\n\n\n2.0.4 Part 4: Install Slack\n\nInstall Slack on your computer and your phone (you can find Slack in whatever app store you use). The reason I want it on both is because you are going to ask me for help with code via Slack. Do not use screenshots unless specifically asked. I want you to copy and paste your code. You can’t do that on a phone. So you need the desktop version. But I can usually solve your problem within a few minutes if you respond right away, and I know that you have your phone on you and are checking it. So the desktop version is for work, the phone version is for notifications.\nEmail me the address you want connected to Slack. Use one you’ll actually check.\nWhen you get the Slack invitation email, log in to the class slack via the apps, not the website.\nAdd the #r channel for general help I’ll send to everyone in the channel and, if you want, the #jobstuff channel for news about jobs I come across."
  },
  {
    "objectID": "logistic-regression.html#the-basics",
    "href": "logistic-regression.html#the-basics",
    "title": "3  Modeling and logistic regression",
    "section": "3.1 The basics",
    "text": "3.1 The basics\nOne of the most common – and seemingly least rigorous – parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method – looking at how a team is playing, looking at the players, consulting their gut – but rarely ever do you hear of a sports pundit using a model.\nWe’re going to change that. Throughout this class, you’ll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not).\nThere are lots of problems in the world where the answer is not a number but a classification: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not?\nThese are problems of classification and there are algorithms we can use to estimate the probability that X will be the outcome. How likely is it that this team with these stats will win this game?\nWhere this gets interesting is in the middle.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\nlibrary(zoo)\nlibrary(gt)\nset.seed(1234)\n\nWhat we need to do here is get both sides of the game. We’ll start with getting the box scores and cleaning them up a bit. We’ll split the shooting columns into made and missed and turn everything into a number.\n\nteamgames <- load_mbb_team_box(seasons = 2015:2023) %>%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %>%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %>%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %>%\n  mutate_at(12:34, as.numeric)"
  },
  {
    "objectID": "logistic-regression.html#feature-engineering",
    "href": "logistic-regression.html#feature-engineering",
    "title": "3  Modeling and logistic regression",
    "section": "3.2 Feature engineering",
    "text": "3.2 Feature engineering\nFeature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good.\nA number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score?\nOne problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this:\nPossessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts)\nIf you look at the data we already have, however, you’ll see possessions are not actually in the data. Which is unfortunate. But we can calculate it pretty easily.\nThen we’ll use the possessions estimate formula to get that, so we can then calculate points per possession.\nWe’ll save that to a new dataframe called teamstats.\n\n3.2.1 Exercise 1: setting up your data\n\nteamstats <- teamgames %>% \n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = ?????_?????_attempted - offensive_rebounds + ????????? + (.475 * free_throws_attempted),\n    ppp = team_score/possessions\n  )\n\n\n\n\nNow we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar.\n\nSplit your data into training and testing data sets. A common split is 80/20.\nTrain the model on the training dataset.\nEvaluate the model on the training data.\nApply the model to the testing data.\nEvaluate the model on the test data.\n\nFrom there, it’s how you want to use the model. We’ll walk through a simple example here, using a simple model – a logistic regression model.\nWhat we’re trying to do here is predict which team will win given their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats BEFORE the game – what we knew about the team going into the game in question. We can do that using zoo and rolling means.\nA rolling mean is an average in a window of time. So if we averaged together the points per possession over 10 games, that’s a 10 game rolling mean. The first real mean would be games 1-10. Then the window would shift one game with game 11, and the average would be games 2-11. Then 3-12, 4-13 and so on.\nWe’ll add three new columns – the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage.\nThe problem we have to face here is that with our data, a rolling mean of games 6-15 would mean if we were trying to predict game 15, we couldn’t include game 15. We’d have to look at games 5-14. If we included game 15, it would mean we had God like abilities to predict the future.\nWe do not. Introducing the lag function. The lag function just takes the window of data and shifts it however many spots back you want to shift it. In our case, we want to shift it one game back. And we’re going to make a rolling window of 5 games.\n\n\n3.2.2 Exercise 2: Lagging\n\nrollingteamstats <- teamstats %>% \n  arrange(game_date) %>%\n  group_by(team_short_display_name, season) %>%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    team_rolling_ppp = rollmean(???(ppp, n=?), k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup()\n\n\n\n\nNow we need to do something that at first will seem kind of odd, but isn’t when you think about it. Our data has half of the box score – just one team. But a game has two teams in it. To get it, we need the team AND the opponent. How can you decide if a team is going to win if you don’t know who they are playing and if that team is any good or not? So we need to create two dataframes that have column names that indicate these stats are the team stats and these stats are the opponent stats. We can do that with some selecting and some renaming.\n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    ) %>% \n  na.omit()\n\nopponent_side <- team_side %>%\n  select(-opponent_id) %>% \n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_id)\n)\n\nNow we’ll join them together.\n\n\n3.2.3 Exercise 3: Joining the data together\n\ngames <- ????_side %>% inner_join(?????????_side)\n\n\n\nJoining, by = c(\"game_id\", \"opponent_id\", \"game_date\", \"season\")\n\n\nThe last problem to solve? Who won? We can add this with conditional logic. The other thing we’re doing here is we’re going to is we’re going to convert our new team_result column into a factor. What is a factor? A factor is a type of data in R that stores categorical values that have a limited number of differences. So wins and losses are a perfect factor. Modeling libraries are looking for factors so it can treat the differences in the data as categories, so that’s why we’re converting it here.\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n))) %>% na.omit()\n\nNow that we’ve done that, we need to look at the order of our factors.\n\n\n3.2.4 Exercise 4: Looking at the factors\nTo do that, we first need to know what R sees when it sees our team_result factor. Is a win first or is a loss first?\n\nlevels(games$????_??????)\n\n\n\n[1] \"L\" \"W\"\n\n\nThe order listed here is the order they are in. What this means is that our predictions will be done through the lens of losses. That doesn’t make intuitive sense to us. We want to know who will win! We can reorder the factors with relevel.\n\n\n3.2.5 Exercise 5: Releveling the factors\n\ngames$team_result <- relevel(games$team_result, ref=\"?\")\n\nlevels(games$team_result)\n\n\n\n[1] \"W\" \"L\"\n\n\nFor simplicity, let’s limit the number of columns we’re going to feed our model.\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>% na.omit()"
  },
  {
    "objectID": "logistic-regression.html#visualizing-the-decision-boundary",
    "href": "logistic-regression.html#visualizing-the-decision-boundary",
    "title": "3  Modeling and logistic regression",
    "section": "3.3 Visualizing the decision boundary",
    "text": "3.3 Visualizing the decision boundary\nThis is just one dimension of the data, but it can illustrate how this works. You can almost see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. You can read it like this: If this team shoots this well and the opponent shoots this well, most of the time this team wins. Or loses. It just depends on where the dot ends up.\nThat neatly captures the probabilities we’re looking at here.\n\nggplot() + \n  geom_point(\n    data=games, aes(x=team_rolling_ppp, y=opponent_rolling_ppp, color=team_result))"
  },
  {
    "objectID": "logistic-regression.html#the-logistic-regression",
    "href": "logistic-regression.html#the-logistic-regression",
    "title": "3  Modeling and logistic regression",
    "section": "3.4 The logistic regression",
    "text": "3.4 The logistic regression\nTo create a model, we have to go through a process. That process starts with splitting data where we know the outomes into two groups – training and testing. The training data is what we will use to create our model. The testing data is how we will determine how good it is. Then, going forward, our model can predict games we haven’t seen yet.\nTo do this, we’re going to first split our modelgames data into two groups – with 80 percent of it in one, 20 percent in the other. We do that by feeding our simplified dataframe into the initial_split function. Then we’ll explicitly name those into new dataframes called train and test.\n\n3.4.1 Exercise 6: What are we splitting?\n\nlog_split <- initial_split(?????????, prop = .8)\nlog_train <- training(log_split)\nlog_test <- testing(log_split)\n\n\n\n\nNow we have two dataframes – log_train and log_test – that we can now use for modeling.\nFirst step to making a model is to set what type of model this will be. We’re going to name our model object – log_mod works because this is a logistic regression model. We’ll use the logistic_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “glm”. The mode in our case is “classification” because we’re trying to classify something as a W or L. Later, we’ll use “regression” to predict numbers.\n\nlog_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nThe next step is to create a recipe. This is a series of steps we’ll use to put our data into our model. For example – what is predicting what? And what aren’t predictors and what are? And do we have to do any pre-processing of the data?\nThe first part of the recipe is the formula. In this case, we’re saying – in real words – team_result is approximately modeled by our predictors, which we represent as . which means all the stuff. Then, importantly, we say what isn’t a predictor next with update_role. So the team name, the game date and things like that are not predictors. So we need to tell it that. The last step is normalizing our numbers. With logistic regression, scale differences in numbers can skew things, so we’re going to turn everything into Z-scores.\n\nlog_recipe <- \n  recipe(team_result ~ ., data = log_train) %>% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %>%\n  step_normalize(all_predictors())\n\nsummary(log_recipe)\n\n# A tibble: 8 × 4\n  variable                    type    role      source  \n  <chr>                       <chr>   <chr>     <chr>   \n1 game_id                     numeric ID        original\n2 game_date                   date    ID        original\n3 team_short_display_name     nominal ID        original\n4 opponent_short_display_name nominal ID        original\n5 season                      numeric ID        original\n6 team_rolling_ppp            numeric predictor original\n7 opponent_rolling_ppp        numeric predictor original\n8 team_result                 nominal outcome   original\n\n\nNow we have enough for a workflow. A workflow is what we use to put it all together. In it, we add our model definition and our recipe.\n\n\n3.4.2 Exercise 7: Making a workflow\n\nlog_workflow <- \n  workflow() %>% \n  add_model(log_???) %>% \n  add_recipe(log_??????)\n\n\n\n\nAnd now we fit our model (this can take a few minutes).\n\nlog_fit <- \n  log_workflow %>% \n  fit(data = log_train)"
  },
  {
    "objectID": "logistic-regression.html#evaluating-the-fit",
    "href": "logistic-regression.html#evaluating-the-fit",
    "title": "3  Modeling and logistic regression",
    "section": "3.5 Evaluating the fit",
    "text": "3.5 Evaluating the fit\nWith logistic regression, there’s two things we’re looking at: The prediction and the probabilities. We can get those with two different fits and combine them together.\nFirst, you can see the predictions like this:\n\ntrainpredict <- log_fit %>% predict(new_data = log_train) %>%\n  bind_cols(log_train)\n\ntrainpredict\n\n# A tibble: 61,020 × 9\n   .pred_class   game_id game_date  team_short_display_… opponent_short_… season\n   <fct>           <int> <date>     <chr>                <chr>             <int>\n 1 W           401083976 2019-02-03 UNC Wilmington       James Madison      2019\n 2 L           400839389 2016-02-11 Nebraska             Wisconsin          2016\n 3 W           400988107 2018-02-10 Wichita State        UConn              2018\n 4 L           401373737 2022-02-25 UC Davis             UCSB               2022\n 5 W           401309755 2021-03-05 Cincinnati           Vanderbilt         2021\n 6 L           401377777 2022-02-27 Temple               Tulane             2022\n 7 L           400868397 2016-03-04 Manhattan            Marist             2016\n 8 L           400988599 2018-02-04 Seton Hall           Villanova          2018\n 9 W           401172379 2020-01-18 Towson               James Madison      2020\n10 W           400847285 2016-02-27 South Dakota St      Oral Roberts       2016\n# … with 61,010 more rows, and 3 more variables: team_rolling_ppp <dbl>,\n#   opponent_rolling_ppp <dbl>, team_result <fct>\n\n\nThen, we can just add it to trainpredict using bind_cols, which means we’re going to bind the columns of this new fit to the old trainpredict.\n\ntrainpredict <- log_fit %>% predict(new_data = log_train, type=\"prob\") %>%\n  bind_cols(trainpredict)\n\ntrainpredict\n\n# A tibble: 61,020 × 11\n   .pred_W .pred_L .pred_class   game_id game_date  team_short_display_name\n     <dbl>   <dbl> <fct>           <int> <date>     <chr>                  \n 1   0.688   0.312 W           401083976 2019-02-03 UNC Wilmington         \n 2   0.430   0.570 L           400839389 2016-02-11 Nebraska               \n 3   0.618   0.382 W           400988107 2018-02-10 Wichita State          \n 4   0.346   0.654 L           401373737 2022-02-25 UC Davis               \n 5   0.506   0.494 W           401309755 2021-03-05 Cincinnati             \n 6   0.289   0.711 L           401377777 2022-02-27 Temple                 \n 7   0.262   0.738 L           400868397 2016-03-04 Manhattan              \n 8   0.176   0.824 L           400988599 2018-02-04 Seton Hall             \n 9   0.715   0.285 W           401172379 2020-01-18 Towson                 \n10   0.622   0.378 W           400847285 2016-02-27 South Dakota St        \n# … with 61,010 more rows, and 5 more variables:\n#   opponent_short_display_name <chr>, season <int>, team_rolling_ppp <dbl>,\n#   opponent_rolling_ppp <dbl>, team_result <fct>\n\n\nThere’s several metrics to look at to evaluate the model on our training data, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. The inputs here are the data, the column that has the actual result, and the column with the prediction, called .pred_class.\n\n3.5.1 Exercise 8: Metrics\n\nmetrics(trainpredict, ????_??????, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.617\n2 kap      binary         0.233\n\n\nSo how accurate is our model? If we’re looking for perfection, we’re far from it. But if we’re looking to make straight up win loss bets … we’re doing okay!\nAnother way to look at the results is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy.\n\n\n3.5.2 Exercise 9: Confusion matrix\n\ntrainpredict %>%\n  conf_mat(????_result, .pred_?????)\n\n\n\n          Truth\nPrediction     W     L\n         W 18923 11744\n         L 11647 18706"
  },
  {
    "objectID": "logistic-regression.html#comparing-it-to-test-data",
    "href": "logistic-regression.html#comparing-it-to-test-data",
    "title": "3  Modeling and logistic regression",
    "section": "3.6 Comparing it to test data",
    "text": "3.6 Comparing it to test data\nNow we can apply our fit to the test data to see how robust it is. If the metrics are similar, that’s good – it means our model is robust. If the metrics change a lot, that’s bad. It means our model is guessing.\n\ntestpredict <- log_fit %>% predict(new_data = log_test) %>%\n  bind_cols(log_test)\n\ntestpredict <- log_fit %>% predict(new_data = log_test, type=\"prob\") %>%\n  bind_cols(testpredict)\n\nAnd now some metrics on the test data.\n\n3.6.1 Exercise 10: Testing\n\nmetrics(????predict, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.614\n2 kap      binary         0.229\n\n\nHow does that compare to our training data? Is it lower? Higher? Are the changes large – like are we talking about single digit changes or double digit changes? The less it changes, the better.\nAnd now the confusion matrix.\n\ntestpredict %>%\n  conf_mat(team_result, .pred_class)\n\n          Truth\nPrediction    W    L\n         W 4639 2954\n         L 2929 4734\n\n\nHow does that compare to the training data?"
  },
  {
    "objectID": "logistic-regression.html#how-well-did-it-do-with-nebraska",
    "href": "logistic-regression.html#how-well-did-it-do-with-nebraska",
    "title": "3  Modeling and logistic regression",
    "section": "3.7 How well did it do with Nebraska?",
    "text": "3.7 How well did it do with Nebraska?\nLet’s grab predictions for Nebraska from both our test and train data and take a look.\n\nnutrain <- trainpredict %>% filter(team_short_display_name == \"Nebraska\" &  season == 2023)\n\nnutest <- testpredict %>% filter(team_short_display_name == \"Nebraska\" & season == 2023)\n\nbind_rows(nutrain, nutest) %>% \n  arrange(game_date) %>% \n  select(.pred_W, .pred_class, team_result, team_short_display_name, opponent_short_display_name) %>% \n  gt()\n\n\n\n\n\n  \n  \n    \n      .pred_W\n      .pred_class\n      team_result\n      team_short_display_name\n      opponent_short_display_name\n    \n  \n  \n    0.5497281\nW\nW\nNebraska\nFlorida St\n    0.5973002\nW\nW\nNebraska\nBoston College\n    0.5175988\nW\nW\nNebraska\nCreighton\n    0.4341151\nL\nL\nNebraska\nIndiana\n    0.2667989\nL\nL\nNebraska\nPurdue\n    0.4810879\nL\nL\nNebraska\nKansas St\n    0.3176467\nL\nW\nNebraska\nQueens\n    0.3050453\nL\nW\nNebraska\nIowa\n    0.3458684\nL\nL\nNebraska\nMichigan St\n    0.4699218\nL\nW\nNebraska\nMinnesota\n    0.4310008\nL\nL\nNebraska\nIllinois\n    0.2072669\nL\nL\nNebraska\nPurdue\n    0.3605986\nL\nW\nNebraska\nOhio State\n    0.2536178\nL\nL\nNebraska\nPenn State\n    0.3102129\nL\nL\nNebraska\nNorthwestern\n    0.2234788\nL\nL\nNebraska\nMaryland\n    0.3468551\nL\nL\nNebraska\nIllinois\n  \n  \n  \n\n\n\n\nBy our rolling metrics, are there any surprises? Should we have beaten Creighton or Iowa?\nHow could you improve this?"
  },
  {
    "objectID": "random-forest.html#the-basics",
    "href": "random-forest.html#the-basics",
    "title": "4  Decision trees and random forests",
    "section": "4.1 The basics",
    "text": "4.1 The basics\nTree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less than x seasons of experience? Do they have more or less then y minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have on average.\nThe upside of decision trees is that if the model is small, you can explain it to anyone. They’re very easy to understand. The trouble with decision trees is that if the model is small, they’re a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree.\nThe most common is the random forest.\nLet’s implement one. We start with libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\nlibrary(zoo)\n\nset.seed(1234)\n\nLet’s use what we had from the last tutorial – a rolling window of points per possession for team and opponent. I’ve gone ahead and run it all in the background. You can see modelgames by using head in the block.\n\nteamgames <- load_mbb_team_box(seasons = 2015:2023)\n\nteamstats <- teamgames %>% \n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    ppp = team_score/possessions\n  )\n\nrollingteamstats <- teamstats %>% \n  group_by(team_short_display_name, season) %>%\n  arrange(game_date) %>%\n  mutate(\n    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup()\n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_team_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    )\n\nopponent_side <- team_side %>%\n  select(-opponent_team_id) %>% \n  rename(\n    opponent_team_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_team_id)\n)\n\ngames <- team_side %>% inner_join(opponent_side)\n\nJoining with `by = join_by(game_id, opponent_team_id, game_date, season)`\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n)))\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>%\n  na.omit()\n\nFor this tutorial, we’re going to create two models from two workflows so that we can compare a logistic regression to a random forest."
  },
  {
    "objectID": "random-forest.html#setup",
    "href": "random-forest.html#setup",
    "title": "4  Decision trees and random forests",
    "section": "4.2 Setup",
    "text": "4.2 Setup\nA random forest is, as the name implies, a large number of decision trees, and they use a random set of inputs. The algorithm creates a large number of randomly selected training inputs, and randomly chooses the feature input for each branch, creating predictions. The goal is to create uncorrelated forests of trees. The trees all make predictions, and the wisdom of the crowds takes over. In the case of classification algorithm, the most common prediction is the one that gets chosen. In a regression model, the predictions get averaged together.\nThe random part of random forest is in how the number of tree splits get created and how the samples from the data are taken to generate the splits. They’re randomized, which has the effect of limiting the influence of a particular feature and prevents overfitting – where your predictions are so tailored to your training data that they miss badly on the test data.\nFor random forests, we change the model type to rand_forest and set the engine to “ranger”. There’s multiple implementations of the random forest algorithm, and the differences between them are beyond the scope of what we’re doing here.\nWe’re going to go through the steps of modeling again, starting with splitting our modelgames data.\n\n4.2.1 Exercise 1: setting up your data\n\ngame_split <- initial_split(??????????, prop = .8)\ngame_train <- training(game_split)\ngame_test <- testing(game_split)\n\n\n\n\nFor this walkthrough, we’re going to do both a logistic regression and a random forest side by side to show the value of workflows.\nThe recipe we’ll create is the same for both, so we’ll use it twice.\n\n\n4.2.2 Exercise 2: setting up the receipe\nSo what data are we feeding into our recipe?\n\ngame_recipe <- \n  recipe(team_result ~ ., data = game_?????) %>% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %>%\n  step_normalize(all_predictors())\n\nsummary(game_recipe)\n\n\n\n# A tibble: 8 × 4\n  variable                    type    role      source  \n  <chr>                       <chr>   <chr>     <chr>   \n1 game_id                     numeric ID        original\n2 game_date                   date    ID        original\n3 team_short_display_name     nominal ID        original\n4 opponent_short_display_name nominal ID        original\n5 season                      numeric ID        original\n6 team_rolling_ppp            numeric predictor original\n7 opponent_rolling_ppp        numeric predictor original\n8 team_result                 nominal outcome   original\n\n\nNow, we’re going to create two different model specifications. The first will be the logistic regression model definintion and the second will be the random forest.\n\nlog_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nrf_mod <- \n  rand_forest() %>% \n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nNow we have enough for our workflows. We have two models and one recipe.\n\n\n4.2.3 Exercise 3: making workflows\n\nlog_workflow <- \n  workflow() %>% \n  add_model(???_mod) %>% \n  add_recipe(????_recipe)\n\nrf_workflow <- \n  workflow() %>% \n  add_model(??_mod) %>% \n  add_recipe(????_recipe)\n\n\n\n\nNow we can fit our models to the data.\n\n\n4.2.4 Exercise 4: fitting our models\n\nlog_fit <- \n  log_workflow %>% \n  fit(data = ????_?????)\n\nrf_fit <- \n  rf_workflow %>% \n  fit(data = ????_?????)\n\n\n\n\nNow we can bind our predictions to the training data and see how we did.\n\nlogpredict <- log_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nlogpredict <- log_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(logpredict)\n\nrfpredict <- rf_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nrfpredict <- rf_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(rfpredict)\n\nNow, how did we do? First, let’s look at the logistic regression.\n\n\n4.2.5 Exercise 5: The first metrics\nWhat prediction dataset do we feed into our metrics?\n\nmetrics(??????????, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.614\n2 kap      binary         0.228\n\n\nSame as last time, the logistic regression model comes in at 62 percent accuracy, and when we expose it to testing data, it remains pretty stable. This is a gigantic hint about what is to come.\nHow about the random forest?\n\n\n4.2.6 Exercise 6: Random forest metrics\n\nmetrics(??predict, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.961\n2 kap      binary         0.921\n\n\nHoly buckets! We made a model that’s 96 percent accurate? GET ME TO VEGAS.\nRemember: Where a model makes its money is in data that it has never seen before.\nFirst, we look at logistic regression.\n\nlogtestpredict <- log_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nlogtestpredict <- log_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(logtestpredict)\n\nmetrics(logtestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.617\n2 kap      binary         0.234\n\n\nJust about the same. That’s a robust model.\nNow, the inevitable crash with random forests.\n\nrftestpredict <- rf_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nrftestpredict <- rf_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(rftestpredict)\n\nmetrics(rftestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.573\n2 kap      binary         0.146\n\n\nRight at 57 percent. A little bit lower than logistic regression. But did they come to the same answers to get those numbers? No.\n\nlogtestpredict %>%\n  conf_mat(team_result, .pred_class)\n\n          Truth\nPrediction    L    W\n         L 5063 3128\n         W 3057 4902\n\n\n\nrftestpredict %>%\n  conf_mat(team_result, .pred_class)\n\n          Truth\nPrediction    L    W\n         L 4680 3453\n         W 3440 4577\n\n\nOur two models, based on our very basic feature engineering, are only slightly better than flipping a coin. If we want to get better, we’ve got work to do."
  },
  {
    "objectID": "xgboost.html#the-basics",
    "href": "xgboost.html#the-basics",
    "title": "5  XGBoost",
    "section": "5.1 The basics",
    "text": "5.1 The basics\nAs we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular.\nBoosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on.\nThe path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost and lightgbm, are very popular for solving a wide variety of problems.\nLet’s re-implement our predictions in an XGBoost algorithm. First, we’ll load libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(zoo)\nlibrary(hoopR)\n\nset.seed(1234)\n\nWe’ll load our game data and do a spot of feature engineering that we used with our other models.\n\nteamgames <- load_mbb_team_box(seasons = 2015:2023) %>%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %>%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %>%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %>%\n  mutate_at(12:34, as.numeric)\n\nteamstats <- teamgames %>% \n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    ppp = team_score/possessions\n  )\n\nrollingteamstats <- teamstats %>% \n  group_by(team_short_display_name, season) %>%\n  arrange(game_date) %>%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup() \n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    ) \n\nopponent_side <- team_side %>%\n  select(-opponent_id) %>% \n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_id)\n)\n\ngames <- team_side %>% inner_join(opponent_side)\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n))) %>% na.omit()\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>% \n  na.omit()\n\nPer usual, we split our data into training and testing.\n\ngame_split <- initial_split(modelgames, prop = .8)\ngame_train <- training(game_split)\ngame_test <- testing(game_split)\n\nAnd our recipe.\n\ngame_recipe <- \n  recipe(team_result ~ ., data = game_train) %>% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\")\n\nsummary(game_recipe)\n\n# A tibble: 8 × 4\n  variable                    type    role      source  \n  <chr>                       <chr>   <chr>     <chr>   \n1 game_id                     numeric ID        original\n2 game_date                   date    ID        original\n3 team_short_display_name     nominal ID        original\n4 opponent_short_display_name nominal ID        original\n5 season                      numeric ID        original\n6 team_rolling_ppp            numeric predictor original\n7 opponent_rolling_ppp        numeric predictor original\n8 team_result                 nominal outcome   original\n\n\nTo this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to."
  },
  {
    "objectID": "xgboost.html#hyperparameters",
    "href": "xgboost.html#hyperparameters",
    "title": "5  XGBoost",
    "section": "5.2 Hyperparameters",
    "text": "5.2 Hyperparameters\nThe hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters we are going to tune (there are others we can, but every addition to the number of hyperparameters means more computation):\n\nmtry – the number of predictors that will be randomly sampled at each split when making trees.\nLearn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point.\nTree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting.\nMinimum number of observations in the terminal node (min_n) – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting).\nLoss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative.\n\nOthers you can tune, but have sensible defaults:\n\nNumber of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem. You can tune this, but I’ll warn you – this is the most computationally expensive tuning. For our example, we’re going to set the number of trees at 30. The default is 15. Tuning it can take a good computer more than an hour to complete, and you’ll have gained .1 percent of accuracy. If we’re Amazon and billions of dollars are on the line, it’s worth it. For predicting NCAA tournament games, it is not.\nSample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfitting.\n\nAll of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning.\n\n5.2.1 Exercise 1: tuning\nFirst, we make a model and label each parameter as tune()\n\nxg_mod <- boost_tree(\n  trees = 30,\n  mtry = tune(),\n  learn_rate = ????(),\n  tree_depth = ????(), \n  min_n = ????(),\n  loss_reduction = ????()\n  ) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\")\n\n\n\n\n\n\n5.2.2 Exercise 2: making a workflow\nLet’s make a workflow now that we have our recipe and our model.\n\ngame_wflow <- \n  workflow() %>% \n  add_model(??_???) %>% \n  add_recipe(????_recipe)\n\n\n\n\nNow, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. Why is that important? Because this hypercube is how your tuning is going to find the optimal outputs.\nWhat follows is what parameters the hypercube will tune.\n\nxgb_grid <- grid_latin_hypercube(\n  finalize(mtry(), game_train),\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  learn_rate(),\n  size = 30\n)\n\nxgb_grid\n\n# A tibble: 30 × 5\n    mtry tree_depth min_n loss_reduction learn_rate\n   <int>      <int> <int>          <dbl>      <dbl>\n 1     5         11    12       9.68e+ 0   7.33e- 3\n 2     2          6    33       5.23e- 4   2.73e- 3\n 3     3         11     9       3.49e- 9   6.81e-10\n 4     5          3    29       2.37e- 4   6.51e- 8\n 5     4         14    26       1.26e+ 0   1.03e- 7\n 6     5          1    25       2.94e+ 0   9.73e- 5\n 7     4          8    30       1.90e-10   6.59e- 6\n 8     4         12    40       8.71e- 7   5.07e- 4\n 9     1          2    32       6.77e- 6   1.46e- 5\n10     6         11     5       1.21e- 2   2.86e- 6\n# … with 20 more rows\n\n\nHow do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. It’ll take a random square in the hypercube, try the combinations in there, and see what happens. It’ll then keep doing that, over and over and over and over. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result.\n\n\n5.2.3 Exercise 3: creating our cross-fold validation set\nThis will create the folds, which are just 10 random subsets of the training data.\n\ngame_folds <- vfold_cv(game_?????)\n\ngame_folds\n\n\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [55431/6159]> Fold01\n 2 <split [55431/6159]> Fold02\n 3 <split [55431/6159]> Fold03\n 4 <split [55431/6159]> Fold04\n 5 <split [55431/6159]> Fold05\n 6 <split [55431/6159]> Fold06\n 7 <split [55431/6159]> Fold07\n 8 <split [55431/6159]> Fold08\n 9 <split [55431/6159]> Fold09\n10 <split [55431/6159]> Fold10\n\n\nNow we come to the part that is going to take some time on your computer. How long? It depends. On my old 2018 Intel Mac, this part took about 25-30 minutes on my machine and made it sounds like it was attempting liftoff. My 2022 M1 Mac? Right around 10 minutes. Depending on how new and how powerful you computer is, it could take minutes, or it could take hours. The point being, start it up, walk away, and let it burn.\n\nxgb_res <- tune_grid(\n  game_wflow,\n  resamples = game_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nOur grid has run on all of our validation samples, and what do we see?\n\ncollect_metrics(xgb_res)\n\n# A tibble: 60 × 11\n    mtry min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean\n   <int> <int>      <int>      <dbl>          <dbl> <chr>    <chr>      <dbl>\n 1     5    12         11   7.33e- 3  9.68          accuracy binary     0.611\n 2     5    12         11   7.33e- 3  9.68          roc_auc  binary     0.658\n 3     2    33          6   2.73e- 3  0.000523      accuracy binary     0.611\n 4     2    33          6   2.73e- 3  0.000523      roc_auc  binary     0.658\n 5     3     9         11   6.81e-10  0.00000000349 accuracy binary     0.500\n 6     3     9         11   6.81e-10  0.00000000349 roc_auc  binary     0.5  \n 7     5    29          3   6.51e- 8  0.000237      accuracy binary     0.607\n 8     5    29          3   6.51e- 8  0.000237      roc_auc  binary     0.639\n 9     4    26         14   1.03e- 7  1.26          accuracy binary     0.603\n10     4    26         14   1.03e- 7  1.26          roc_auc  binary     0.644\n# … with 50 more rows, and 3 more variables: n <int>, std_err <dbl>,\n#   .config <chr>\n\n\nWell we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination. There’s a function to just show us the best one called … wait for it … show_best.\n\nshow_best(xgb_res, \"accuracy\")\n\n# A tibble: 5 × 11\n   mtry min_n tree_depth learn_rate loss_reduction .metric  .estimator  mean\n  <int> <int>      <int>      <dbl>          <dbl> <chr>    <chr>      <dbl>\n1     8     2          2  0.0478     0.00000000913 accuracy binary     0.612\n2     5    12         11  0.00733    9.68          accuracy binary     0.611\n3     3    13          6  0.0000306  0.120         accuracy binary     0.611\n4     7    23          8  0.0215     0.0000224     accuracy binary     0.611\n5     2    33          6  0.00273    0.000523      accuracy binary     0.611\n# … with 3 more variables: n <int>, std_err <dbl>, .config <chr>\n\n\nThe best combination as of this data update comes up with an accuracy of about 61.5 percent."
  },
  {
    "objectID": "xgboost.html#finalizing-our-model",
    "href": "xgboost.html#finalizing-our-model",
    "title": "5  XGBoost",
    "section": "5.3 Finalizing our model",
    "text": "5.3 Finalizing our model\nLet’s capture our best set of hyperparameters so we can use them in our model.\n\nbest_acc <- select_best(xgb_res, \"accuracy\")\n\nAnd now we put that into a final workflow. Pay attention to the main arguments in the output below.\n\nfinal_xgb <- finalize_workflow(\n  game_wflow,\n  best_acc\n)\n\nfinal_xgb\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 8\n  trees = 30\n  min_n = 2\n  tree_depth = 2\n  learn_rate = 0.0478471146705376\n  loss_reduction = 9.13322009844852e-09\n\nComputational engine: xgboost \n\n\nThere’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along.\n\n5.3.1 Exercise 4: making our final workflow\nWe create a fit using our finalized workflow.\n\nxg_fit <- \n  ?????_xgb %>% \n  fit(data = game_train)\n\n\n\n\nWe can see something things about that fit, including all the iterations of our XGBoost model. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The error goes down with each iteration as we go down the gradient descent.\n\nxg_fit %>% \n  extract_fit_parsnip() \n\nparsnip model object\n\n##### xgb.Booster\nraw: 30.2 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.0478471146705376, max_depth = 2L, \n    gamma = 9.13322009844852e-09, colsample_bytree = 1, colsample_bynode = 1, \n    min_child_weight = 2L, subsample = 1), data = x$data, nrounds = 30, \n    watchlist = x$watchlist, verbose = 0, nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.0478471146705376\", max_depth = \"2\", gamma = \"9.13322009844852e-09\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"2\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 2 \nniter: 30\nnfeatures : 2 \nevaluation_log:\n    iter training_logloss\n       1        0.6906536\n       2        0.6883864\n---                      \n      29        0.6609857\n      30        0.6605502\n\n\n\n\n5.3.2 Exercise 5: prediction time\nNow, like before, we can bind our predictions using our xg_fit to the game_train data.\n\ntrainresults <- game_train %>%\n  bind_cols(predict(??_???, game_train))\n\n\n\n\n\n\n5.3.3 Exercise 6: metrics\nAnd now see how we did.\n\nmetrics(????????????, truth = team_result, estimate = .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.616\n2 kap      binary         0.233\n\n\nHow about the test data?\n\ntestresults <- game_test %>%\n  bind_cols(predict(xg_fit, game_test))\n\nmetrics(testresults, truth = team_result, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.612\n2 kap      binary         0.225\n\n\nUnlike the random forest, not nearly the drop in metrics between train and test."
  },
  {
    "objectID": "lightgbm.html#the-basics",
    "href": "lightgbm.html#the-basics",
    "title": "6  LightGBM",
    "section": "6.1 The basics",
    "text": "6.1 The basics\nLightGBM is another tree-based method that is similar to XGBoost but differs in ways that make it computationally more efficient. Where XGBoost and Random Forests are based on branches, LightGBM grows leaf-wise. Think of it like this – XGBoost uses lots of short trees with branches – it comes to a fork, makes a decision that reduces the amount of error the last tree got, and makes a new branch. LightGBM on the other hand, makes new leaves of each branch, which can mean lots of little splits, instead of big ones. That can lead to over-fitting on small datasets, but it also means it’s much faster than XGBoost.\nLightGBM also uses histograms of the data to make choices, where XGBoost is comptutationally optimizing those choices. Roughly translated – LightGBM is looking at the fat part of a normal distribution to make choices, where XGBoost is tuning parameters to find the optimal path forward. It’s another reason why LightGBM is faster, but also not reliable with small datasets.\nLet’s implement a LightGBM model. We start with libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\nlibrary(zoo)\nlibrary(bonsai)\n\nset.seed(1234)\n\nWe’ll continue to use what we’ve done for feature engineering – a rolling window of points per possession for team and opponent. You should be quite familiar with this by now.\n\nteamgames <- load_mbb_team_box(seasons = 2015:2023) %>%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %>%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %>%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %>%\n  mutate_at(12:34, as.numeric)\n\nteamstats <- teamgames %>% \n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    ppp = team_score/possessions\n  )\n\nrollingteamstats <- teamstats %>% \n  group_by(team_short_display_name, season) %>%\n  arrange(game_date) %>%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup() \n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    ) \n\nopponent_side <- team_side %>%\n  select(-opponent_id) %>% \n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_id)\n)\n\ngames <- team_side %>% inner_join(opponent_side)\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n))) %>% na.omit()\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>% \n  na.omit()\n\nFor this tutorial, we’re going to create three models from three workflows so that we can compare a logistic regression to a random forest to a lightbgm model."
  },
  {
    "objectID": "lightgbm.html#setup",
    "href": "lightgbm.html#setup",
    "title": "6  LightGBM",
    "section": "6.2 Setup",
    "text": "6.2 Setup\nWe’re going to go through the steps of modeling again, starting with splitting our modelgames data.\n\n6.2.1 Exercise 1: setting up your data\n\ngame_split <- initial_split(??????????, prop = .8)\ngame_train <- training(game_split)\ngame_test <- testing(game_split)\n\n\n\n\nThe recipe we’ll create is the same for both, so we’ll use it three times.\n\n\n6.2.2 Exercise 2: setting up the receipe\nSo what data are we feeding into our recipe?\n\ngame_recipe <- \n  recipe(team_result ~ ., data = game_?????) %>% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %>%\n  step_normalize(all_predictors())\n\nsummary(game_recipe)\n\n\n\n# A tibble: 8 × 4\n  variable                    type    role      source  \n  <chr>                       <chr>   <chr>     <chr>   \n1 game_id                     numeric ID        original\n2 game_date                   date    ID        original\n3 team_short_display_name     nominal ID        original\n4 opponent_short_display_name nominal ID        original\n5 season                      numeric ID        original\n6 team_rolling_ppp            numeric predictor original\n7 opponent_rolling_ppp        numeric predictor original\n8 team_result                 nominal outcome   original\n\n\nNow, we’re going to create three different model specifications. The first will be the logistic regression model definition, the second will be the random forest, the third is the lightgbm.\n\nlog_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nrf_mod <- \n  rand_forest() %>% \n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nlightgbm_mod <- \n  boost_tree() %>%\n  set_engine(\"lightgbm\") %>%\n  set_mode(mode = \"classification\")\n\nNow we have enough for our workflows. We have three models and one recipe.\n\n\n6.2.3 Exercise 3: making workflows\n\nlog_workflow <- \n  workflow() %>% \n  add_model(???_mod) %>% \n  add_recipe(????_recipe)\n\nrf_workflow <- \n  workflow() %>% \n  add_model(??_mod) %>% \n  add_recipe(????_recipe)\n\nlightgbm_workflow <- \n  workflow() %>% \n  add_model(light???_mod) %>% \n  add_recipe(game_recipe)\n\n\n\n\nNow we can fit our models to the data.\n\n\n6.2.4 Exercise 4: fitting our models\n\nlog_fit <- \n  log_workflow %>% \n  fit(data = ????_?????)\n\nrf_fit <- \n  rf_workflow %>% \n  fit(data = ????_?????)\n\nlightgbm_fit <- \n  lightgbm_workflow %>% \n  fit(data = ????_?????)"
  },
  {
    "objectID": "lightgbm.html#prediction-time",
    "href": "lightgbm.html#prediction-time",
    "title": "6  LightGBM",
    "section": "6.3 Prediction time",
    "text": "6.3 Prediction time\nNow we can bind our predictions to the training data and see how we did.\n\nlogpredict <- log_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nlogpredict <- log_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(logpredict)\n\nrfpredict <- rf_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nrfpredict <- rf_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(rfpredict)\n\nlightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nlightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(lightgbmpredict)\n\nNow, how did we do?\n\n6.3.1 Exercise 5: The first metrics\nWhat prediction dataset do we feed into our metrics? Let’s look first at the random forest, because it’s a tree-based method just like lightgbm.\n\nmetrics(?????????, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.960\n2 kap      binary         0.919\n\n\nSame as last time, the random forest produces bonkers training numbers. Can you say overfit?\nHow about the lightgbm?\n\n\n6.3.2 Exercise 6: LightGBM metrics\n\nmetrics(????????predict, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.627\n2 kap      binary         0.254\n\n\nAbout 63 percent accuracy. Which, if you’ll recall, is a few percentage points better than logistic regression, and worse than random forest WITH A HUGE ASTERISK.\nRemember: Where a model makes its money is in data that it has never seen before.\nFirst, we look at random forest. The inevitable crash with random forests.\n\nrftestpredict <- rf_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nrftestpredict <- rf_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(rftestpredict)\n\nmetrics(rftestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.576\n2 kap      binary         0.153\n\n\nRight at 57 percent. A little bit lower than logistic regression. But did they come to the same answers to get those numbers? No.\nAnd now lightGBM.\n\nlightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nlightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(lightgbmtestpredict)\n\nmetrics(lightgbmtestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.612\n2 kap      binary         0.225\n\n\nOur three models, based on our very basic feature engineering, are still only slightly better than flipping a coin. If we want to get better, we’ve still got work to do."
  },
  {
    "objectID": "svm.html#prediction-time",
    "href": "svm.html#prediction-time",
    "title": "7  Support Vector Machines",
    "section": "7.1 Prediction time",
    "text": "7.1 Prediction time\nNow we can bind our predictions to the training data and see how we did.\n\nlogpredict <- log_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nlogpredict <- log_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(logpredict)\n\nlightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nlightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(lightgbmpredict)\n\nsvmpredict <- svm_fit %>% predict(new_data = game_train) %>%\n  bind_cols(game_train) \n\nsvmpredict <- svm_fit %>% predict(new_data = game_train, type=\"prob\") %>%\n  bind_cols(svmpredict)\n\nNow, how did we do?\n\n7.1.1 Exercise 5: The first metrics\nWhat prediction dataset do we feed into our metrics? Let’s look first at the lightGBM.\n\nmetrics(?????????, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.627\n2 kap      binary         0.254\n\n\nAnd now the SVM.\n\n\n7.1.2 Exercise 6: SVM metrics\n\nmetrics(???predict, team_result, .pred_class)\n\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.616\n2 kap      binary         0.233\n\n\nLooks like the LightGBM did a little better than the SVM on training. But remember: Where a model makes its money is in data that it has never seen before.\nFirst, we look at lightGBM.\n\nlightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nlightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(lightgbmtestpredict)\n\nmetrics(lightgbmtestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.612\n2 kap      binary         0.225\n\n\nRight at 61 percent. And now SVM.\n\nsvmtestpredict <- svm_fit %>% predict(new_data = game_test) %>%\n  bind_cols(game_test)\n\nsvmtestpredict <- svm_fit %>% predict(new_data = game_test, type=\"prob\") %>%\n  bind_cols(svmtestpredict)\n\nmetrics(svmtestpredict, team_result, .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.615\n2 kap      binary         0.230\n\n\nSlightly better – very slightly. But it shows that SVM is a bit more robust to new data than the lightGBM."
  },
  {
    "objectID": "predictions.html#redoing-the-feature-engineering",
    "href": "predictions.html#redoing-the-feature-engineering",
    "title": "8  Making predictions with new games",
    "section": "8.1 Redoing the feature engineering",
    "text": "8.1 Redoing the feature engineering\nNow that we have our fit based on known data, we need to redo our feature engineering so that we now have up to the moment data. You get that by just simply removing the lags. Make sure you remove the lag( parts AND the n=1) parts both.\n\nrollingteamstats <- teamstats %>% \n  group_by(team_short_display_name, season) %>%\n  arrange(game_date) %>%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    team_rolling_ppp = rollmean(ppp, k=5, align=\"right\", fill=NA)\n    ) %>% \n  ungroup() \n\nteam_side <- rollingteamstats %>%\n  select(\n    game_id,\n    team_id, \n    team_short_display_name, \n    opponent_id, \n    game_date, \n    season, \n    team_score, \n    team_rolling_ppp\n    ) \n\nopponent_side <- team_side %>%\n  select(-opponent_id) %>% \n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_ppp = team_rolling_ppp\n  ) %>%\n  mutate(opponent_id = as.numeric(opponent_id)\n)\n\ngames <- team_side %>% inner_join(opponent_side)\n\nJoining with `by = join_by(game_id, opponent_id, game_date, season)`\n\ngames <- games %>% mutate(\n  team_result = as.factor(case_when(\n    team_score > opponent_score ~ \"W\",\n    opponent_score > team_score ~ \"L\"\n))) %>% na.omit()\n\nmodelgames <- games %>% \n  select(\n    game_id, \n    game_date, \n    team_short_display_name, \n    opponent_short_display_name, \n    season, \n    team_rolling_ppp, \n    opponent_rolling_ppp, \n    team_result\n    ) %>% \n  na.omit()\n\nNow we need to get the first games. Let’s start with the first round of the Big Ten Tournament. To do this, we’re first going to make a tibble with the teams in the team_short_display_name and opponnent_short_display_name.\n\nround1games <- tibble(\n  team_short_display_name=\"Ohio State\",\n  opponent_short_display_name=\"Wisconsin\"\n) %>% add_row(\n  team_short_display_name=\"Minnesota\",\n  opponent_short_display_name=\"Nebraska\"\n)\n\nNow with that, we need to get all the team data for our game and join it to our round 1 games. This will get the latest information, drop the team_result and all the opponent information and then add it to our round1games dataframe. Then it will do it again, but this time for the opponent side of the game.\n\nround1games <- modelgames %>% group_by(team_short_display_name) %>% filter(game_date == max(game_date) & season == 2023) %>% slice(1) %>% select(-team_result, -starts_with(\"opponent\")) %>% right_join(round1games)\n\nJoining with `by = join_by(team_short_display_name)`\n\nround1games <- modelgames %>% group_by(opponent_short_display_name) %>% filter(game_date == max(game_date) & season == 2023) %>% slice(1) %>% ungroup() %>% select(-team_result, -starts_with(\"team\"), -game_id, -game_date, -season) %>% right_join(round1games) \n\nJoining with `by = join_by(opponent_short_display_name)`\n\n\nNow, just like before, we apply our fits. The select at the end is just to move the right stuff up to the front of the table and make it easy for us to see what we need to see.\nAnd who does our model think will win the first round?\n\nround1 <- svm_fit %>% predict(new_data = round1games) %>%\n  bind_cols(round1games) %>% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nround1 <- svm_fit %>% predict(new_data = round1games, type=\"prob\") %>%\n  bind_cols(round1) %>% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nround1\n\n# A tibble: 2 × 10\n  .pred_class .pred_W .pred_L team_s…¹ oppon…² oppon…³ game_id game_date  season\n  <fct>         <dbl>   <dbl> <chr>    <chr>     <dbl>   <int> <date>      <int>\n1 L             0.444   0.556 Minneso… Nebras…    1.10  4.01e8 2023-03-06   2023\n2 W             0.585   0.415 Ohio St… Wiscon…    1.05  4.01e8 2023-03-04   2023\n# … with 1 more variable: team_rolling_ppp <dbl>, and abbreviated variable\n#   names ¹​team_short_display_name, ²​opponent_short_display_name,\n#   ³​opponent_rolling_ppp\n\n\nGo Big Red!"
  },
  {
    "objectID": "regression-basics.html#the-basics",
    "href": "regression-basics.html#the-basics",
    "title": "9  Using linear regression to predict a number",
    "section": "9.1 The basics",
    "text": "9.1 The basics\nLinear models are something you’ve understood since you took middle school math and learned the equation of a line. Remember y = mx + b? It’s back. And, unlike what you complained bitterly in middle school, it’s very, very useful.\nWhat a linear model says, in words is that we can predict y if we multiply a value – a coefficient – by our x value offset with b, which is really the y-intercept, but think of it like where the line starts. Or, expressed as y = mx + b: points = true_shooting_percentage * ? + some starting point. Think of some starting point as what the score should be if the true_shooting_percentage is zero. Should be zero, right? Intuitively, yes, but it won’t always work out so easily.\nWhat we’re trying to do here is predict how many fantasy points a player should score given their draft position (and later other stats). For this, we’ll look at wide receivers, and we’re going to build a model based on the past 10 draft classes."
  },
  {
    "objectID": "regression-basics.html#feature-engineering",
    "href": "regression-basics.html#feature-engineering",
    "title": "9  Using linear regression to predict a number",
    "section": "9.2 Feature engineering",
    "text": "9.2 Feature engineering\nFirst we’ll need libraries. You might need to install corrr with install.packages(\"corrr\") run in your console.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\n\nAnd we’ll need some data. In this case, we’ve got draft data from cfbfastR and fantasy data from Pro-Football Reference.\n\nfantasy <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/fantasyfootball20132022.csv\")\n\nwr <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/wr20132022.csv\")\n\nwrdrafted <- wr %>% \n  inner_join(fantasy, by=c(\"name\"=\"Player\", \"year\"=\"Season\"))\n\nThat leaves us with a dataframe of 263 observations – drafted wide receivers with their fantasy stats attached to them.\nLet’s thin the herd here a bit and just get our selected stats for modeling. We’re really just going to have a handful of things: name, year, their college team and the team that drafted them, their overall draft number, their pre-draft grade from ESPN and the number of fantasy points they scored in their first year in the league.\n\nwrselected <- wrdrafted %>%\n  select(\n    name,\n    year,\n    college_team,\n    nfl_team,\n    overall,\n    pre_draft_grade,\n    FantPt\n  )"
  },
  {
    "objectID": "regression-basics.html#setting-up-the-modeling-process",
    "href": "regression-basics.html#setting-up-the-modeling-process",
    "title": "9  Using linear regression to predict a number",
    "section": "9.3 Setting up the modeling process",
    "text": "9.3 Setting up the modeling process\nWith most modeling tasks we need to start with setting a random number seed to aid our random splitting of data into training and testing.\n\nset.seed(1234)\n\nRandom numbers play a large role in a lot of data science algorithms, so setting one helps our reproducibility.\nAfter that, we split our data. There’s a number of ways to do this – R has a bunch and you’ll find all kinds of examples online – but Tidymodels has made this easy.\n\nplayer_split <- initial_split(wrselected, prop = .8)\n\nplayer_train <- training(player_split)\nplayer_test <- testing(player_split)\n\nLet’s start with a simple linear regression with one variable. We’re just going to use the overall draft position to predict fantasy points. How well does the draft pick do that? are top picks big point getters and low picks low point scorers? Is there a pattern?\nA lot of what comes next is familiar to you. We’re going to make a model, make a fit, then add the results to our training data and see where that gets us. The fit is made up of the FantPt and overall divided by a ~, which can be verbalized as “is approximately modeled by.” So FantPt is approximately modeled by overall.\nOur metrics, though will say new things.\n\n9.3.1 Exercise 1: Your first fit\n\nlm_model <- linear_reg() %>%\n    set_engine(\"lm\")\n\nfit_lm <- lm_model %>%\n  fit(?????? ~ ???????, data = player_train)\n\n\n\n\nWe can look now at the pieces of the equation of a line here.\n\ntidy(fit_lm, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   89.8      5.48       16.4  4.10e-38   79.0     101.   \n2 overall       -0.365    0.0426     -8.57 3.58e-15   -0.449    -0.281\n\n\nThe two most important things to see here are the terms and the estimates. Start with overall. What that says is for every pick in the draft, a player should score about a third of a fantasy point less than the previous pick. So the first pick scores -.3, the second pick scores -.6 and so on. So the higher the pick, the lower the number. Thus our slope.\nHOWEVER, the intercept has something to say about this. What the intercept says is that players start with about 87 fantasy points. The slope then adjusts them downwards each pick they go.\nThink again about y = mx + b. We have our terms here: y is fantasy points, m is -.3 x the pick number and b is 87. Let’s pretend for a minute that you were drafted with the 10th pick. That would mean, on the slope, you’re down 3 points, so -3 + 87 is 84. Our model would predict the 10th pick of the draft, if they were a wide receiver, would score 84 fantasy points in their rookie season.\n\n\n9.3.2 Exercise 2: What is the truth?\nThat sounds good, but how good is our model?\n\ntrainresults <- player_train %>%\n    bind_cols(predict(fit_lm, player_train))\n\nmetrics(trainresults, truth = ??????, estimate = .pred)\n\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      41.8  \n2 rsq     standard       0.280\n3 mae     standard      32.9  \n\n\nOur first step in evaluating a linear model is to get the r-squared value. The yardstick library (part of Tidymodels) does this nicely. We tell it to produce metrics on a dataset, and we have to tell it what the real world result is (the truth column) and what the estimate column is (.pred).\nWe have two numbers we’re going to focus on – rsq or r squared, and rmse or root mean squared error. R squared is the amount that changes in overall predict changes in fantasy points. You can read it as a percentage. So changes in overall draft position account for about 28 percent of the change in fantasy points. Not great, but we’re just starting.\nThe rmse is how off your predictions are on average. In this case, our fantasy point prediction is off by 40 (plus or minus) on average. Given that we started with 87, that’s also not great.\n\n\n9.3.3 Exercise 3: How does it fare?\nWe need to make those numbers smaller. But first, we should see how it does with test data.\n\ntestresults <- player_???? %>%\n    bind_cols(predict(fit_lm, player_????))\n\nmetrics(testresults, truth = FantPt, estimate = .pred)\n\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      41.4  \n2 rsq     standard       0.291\n3 mae     standard      31.4  \n\n\nOur r squared is up a bit, but so is our rmse. So we didn’t change all much, which is good. That means our model is robust to new data."
  },
  {
    "objectID": "regression-basics.html#multiple-regression",
    "href": "regression-basics.html#multiple-regression",
    "title": "9  Using linear regression to predict a number",
    "section": "9.4 Multiple regression",
    "text": "9.4 Multiple regression\nThe problem with simple regressions? They’re simple. Anyone who has watched a sport knows there’s a lot more to the outcome than just one number.\nEnter the multiple regression.\nMultiple regressions are a step toward reality – where more than one thing influences the outcome. However, the more variance we attempt to explain, the more error and uncertainty we introduce into our model.\nTo add a variable to your regression model to make a multiple regression model, you simply use + and add it in. Let’s add pre_draft_grade.\n\n9.4.1 Exercise 4: Adding another variable\n\nlm_model <- linear_reg() %>%\n    set_engine(\"lm\")\n\nfit_lm <- lm_model %>%\n  fit(FantPt ~ overall + ???_?????_?????, data = player_train)\n\n\n\n\nLet’s look at the pieces of the equation of a line again.\n\ntidy(fit_lm, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term            estimate std.error statistic   p.value conf.low conf.high\n  <chr>              <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)       56.8     27.4         2.07 0.0398       2.68    111.   \n2 overall           -0.302    0.0756     -3.99 0.0000953   -0.451    -0.153\n3 pre_draft_grade    0.369    0.294       1.25 0.212       -0.212     0.950\n\n\nSo our intercept is five points lower. Our overall estimate is about the same – each pick lowers the fantasy point expectation – but the pre_draft_grade now adds five tenths of a point for each point of pre-draft grade.\nHow does that impact our draft model?\n\ntrainresults <- player_train %>%\n    bind_cols(predict(fit_lm, player_train))\n\nmetrics(trainresults, truth = FantPt, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      41.9  \n2 rsq     standard       0.287\n3 mae     standard      32.7  \n\n\nHuh. Our r squared is almost unchanged and our rmse is up. What gives?\nThere are multiple ways to find the right combination of inputs to your models. With multiple regressions, the most common is the correlation matrix. We’re looking to maximize r-squared by choosing inputs that are highly correlated to our target value, but not correlated with other things. Example: We can assume that overall draft pick and pre-draft grade are highly correlated to fantasy points, but the problem lies in if they are highly correlated to each other. If so, we’re just adding error and not getting any new predictive value.\nUsing corrr, we can create a correlation matrix in a dataframe to find columns that are highly correlated with our target – FantPt. To do this, we need to select the columns we’re working with – overall and pre_draft_grade.\n\nwrselected %>% \n  select(FantPt, overall, pre_draft_grade) %>% \n  correlate()\n\n# A tibble: 3 × 4\n  term            FantPt overall pre_draft_grade\n  <chr>            <dbl>   <dbl>           <dbl>\n1 FantPt          NA      -0.532           0.451\n2 overall         -0.532  NA              -0.814\n3 pre_draft_grade  0.451  -0.814          NA    \n\n\nReading this when you have a lot of numbers can be a lot, and it helps to take some notes as you go.\nYou read up and down and left and right – it’s a matrix. Follow the FantPt row across to the overall column and you’ll see they’re about 50 percent negatively correlated – -1 is a perfect negative correlation. Now look to the right to pre_draft_grade. They’re almost the same – but this time it’s about 45 percent positively correlated.\nNow look at the overall column, and go down to the pre_draft_grade. The correlation: -0.8141840. Remember that a -1 is a perfect negative corelation. For every 1 one goes up, the other goes down 1. That’s really close to -1.\nWhat does that mean? It means including both is going to just add error without adding much value. They’re so similar. You pick the one that is more highly correlated with fantasy points – overall pick."
  },
  {
    "objectID": "random-forest-for-regression.html#the-basics",
    "href": "random-forest-for-regression.html#the-basics",
    "title": "10  Random forests to predict a number",
    "section": "10.1 The basics",
    "text": "10.1 The basics\nAnd now we return to decision trees and random forests. Recall that tree-based algorithms are based on decision trees, which are very easy to understand. A random forest is, as the name implies, a large number of decision trees, and they use a random choice of inputs at each fork in the tree. The algorithm creates a large number of randomly selected training inputs, and randomly chooses the feature input for each branch, creating predictions. The goal is to create uncorrelated forests of trees. The trees all make predictions, and the wisdom of the crowds takes over.\nThis time, we’re going to clean up our code a bit and make it more like we are accustomed to with our previous work, where we’ll make recipes and workflows.\nAs always, we start with libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nset.seed(1234)\n\nAnd we’ll need some data. We’ll use our draft data from cfbfastR and fantasy data from Pro-Football Reference.\n\nfantasy <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/fantasyfootball20132022.csv\")\n\nwr <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/wr20132022.csv\")\n\nwrdrafted <- wr %>% \n  inner_join(fantasy, by=c(\"name\"=\"Player\", \"year\"=\"Season\"))\n\nAnd again we have a dataframe of 263 observations – drafted wide receivers with their fantasy stats attached to them.\nLet’s narrow that down to just our columns we need:\n\nwrselected <- wrdrafted %>%\n  select(\n    name,\n    year,\n    college_team,\n    nfl_team,\n    overall,\n    pre_draft_grade,\n    FantPt\n  ) %>% na.omit()\n\nBefore we get to the recipe, let’s split our data.\n\nplayer_split <- initial_split(wrselected, prop = .8)\n\nplayer_train <- training(player_split)\nplayer_test <- testing(player_split)\n\nNow we’re ready to start.\n\n10.1.1 Exercise 1: What data are we feeding the recipe?\n\nplayer_recipe <- \n  recipe(FantPt ~ ., data = player_?????) %>% \n  update_role(name, year, college_team, nfl_team, new_role = \"ID\")\n\nsummary(player_recipe)\n\n\n\n# A tibble: 7 × 4\n  variable        type    role      source  \n  <chr>           <chr>   <chr>     <chr>   \n1 name            nominal ID        original\n2 year            numeric ID        original\n3 college_team    nominal ID        original\n4 nfl_team        nominal ID        original\n5 overall         numeric predictor original\n6 pre_draft_grade numeric predictor original\n7 FantPt          numeric outcome   original\n\n\nNow, we’re going to create two different model specifications. The first will be the linear regression model definition and the second will be the random forest.\n\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nrf_mod <- \n  rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nNow we have enough for our workflows. We have two models and one recipe.\n\n\n10.1.2 Exercise 2: making workflows\n\nlinear_workflow <- \n  workflow() %>% \n  add_model(??????_mod) %>% \n  add_recipe(???????_recipe)\n\nrf_workflow <- \n  workflow() %>% \n  add_model(??_mod) %>% \n  add_recipe(??????_recipe)\n\n\n\n\nNow we can fit our models to the data.\n\n\n10.1.3 Exercise 3: fitting our models\n\nlinear_fit <- \n  linear_workflow %>% \n  fit(data = ????_?????)\n\nrf_fit <- \n  rf_workflow %>% \n  fit(data = ????_?????)\n\n\n\n\nNow we can bind our predictions to the training data and see how we did.\n\nlinearpredict <- \n  linear_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nrfpredict <- \n  rf_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nNow, how did we do? First, let’s look at the linear regression.\n\n\n10.1.4 Exercise 4: The first metrics\nWhat prediction dataset do we feed into our metrics?\n\nmetrics(?????????????, FantPt, .pred)\n\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      40.9  \n2 rsq     standard       0.316\n3 mae     standard      31.5  \n\n\nSame as last time. An r squared in the high 20s, an rmse in the 40s. Nothing to write home about. How did the random forest do?\n\n\n10.1.5 Exercise 5: Random forest metrics\n\nmetrics(??predict, FantPt, .pred)\n\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      22.6  \n2 rsq     standard       0.812\n3 mae     standard      17.2  \n\n\nHopefully you learned your lesson the last time we did random forests – they have a habit of bringing up your hopes on training only to dash them on testing, especially when you have two highly correlated values.\nIs that what happened here?\n\nrftestpredict <- \n  rf_fit %>% \n  predict(new_data = player_test) %>%\n  bind_cols(player_test) \n\nmetrics(rftestpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      49.3  \n2 rsq     standard       0.157\n3 mae     standard      38.2  \n\n\nIndeed.\nSafe to say we’ve reached the limits of overall draft pick and pre-draft grades for predictive value. Time to add more."
  },
  {
    "objectID": "xgboost-for-regression.html#the-basics",
    "href": "xgboost-for-regression.html#the-basics",
    "title": "11  XGBoost for regression",
    "section": "11.1 The basics",
    "text": "11.1 The basics\nAnd now we return to XGBoost, but now for regression. Recall that boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on.\nSo far, our linear regression and random forest methods aren’t that great. Does XGBoost, with it’s method of optimizing for reduced error, fare any better? Let’s try, but this time we’re going to add more information. We’re going to add college receiving stats.\nAs always, we start with libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nset.seed(1234)\n\nWe’re going to load a new data file this time. It’s called wrdraftedstats and it now has wide receivers who were drafted, their fantasy stats, and their college career stats.\n\nwrdraftedstats <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/wrdraftedstats20132022.csv\")\n\nLet’s narrow that down to just our columns we need. We’re going to add total_yards and total_touchdowns to our data to see what happens to our predictions.\n\nwrselected <- wrdraftedstats %>%\n  select(\n    name,\n    year,\n    college_team,\n    nfl_team,\n    overall,\n    total_yards,\n    total_touchdowns,\n    FantPt\n  ) %>% na.omit()\n\nBefore we get to the recipe, let’s split our data.\n\nplayer_split <- initial_split(wrselected, prop = .8)\n\nplayer_train <- training(player_split)\nplayer_test <- testing(player_split)"
  },
  {
    "objectID": "xgboost-for-regression.html#implementing-xgboost",
    "href": "xgboost-for-regression.html#implementing-xgboost",
    "title": "11  XGBoost for regression",
    "section": "11.2 Implementing XGBoost",
    "text": "11.2 Implementing XGBoost\nOur player recipe will remain unchanged because we’re using the . notation to mean “everything that isn’t an ID or a predictor.”\n\nplayer_recipe <- \n  recipe(FantPt ~ ., data = player_train) %>%\n  update_role(name, year, college_team, nfl_team, new_role = \"ID\")\n\nsummary(player_recipe)\n\n# A tibble: 8 × 4\n  variable         type    role      source  \n  <chr>            <chr>   <chr>     <chr>   \n1 name             nominal ID        original\n2 year             numeric ID        original\n3 college_team     nominal ID        original\n4 nfl_team         nominal ID        original\n5 overall          numeric predictor original\n6 total_yards      numeric predictor original\n7 total_touchdowns numeric predictor original\n8 FantPt           numeric outcome   original\n\n\nOur prediction will use the overall draft pick, their total yards in college and their total touchdowns in college. Does that predict Fantasy points better? Let’s implement multiple models side by side.\n\nlinear_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nrf_mod <- \n  rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nxg_mod <- boost_tree(\n  trees = tune(), \n  learn_rate = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  loss_reduction = tune(), \n  sample_size = tune(), \n  mtry = tune(), \n  ) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\")\n\nNow to create workflows.\n\nlinear_workflow <- \n  workflow() %>% \n  add_model(linear_mod) %>% \n  add_recipe(player_recipe)\n\nrf_workflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(player_recipe)\n\nxg_workflow <- \n  workflow() %>% \n  add_model(xg_mod) %>% \n  add_recipe(player_recipe)\n\nNow to tune the XGBoost model.\n\nxgb_grid <- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), player_train),\n  learn_rate()\n)\n\nplayer_folds <- vfold_cv(player_train)\n\nxgb_res <- tune_grid(\n  xg_workflow,\n  resamples = player_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nbest_rmse <- select_best(xgb_res, \"rmse\")\n\nfinal_xgb <- finalize_workflow(\n  xg_workflow,\n  best_rmse\n)\n\nBecause there’s not a ton of data here, this goes relatively quickly. Now to create fits.\n\nlinear_fit <- \n  linear_workflow %>% \n  fit(data = player_train)\n\nrf_fit <- \n  rf_workflow %>% \n  fit(data = player_train)\n\nxg_fit <- \n  final_xgb %>% \n  fit(data = player_train)\n\nAnd now to make predictions.\n\nlinearpredict <- \n  linear_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nrfpredict <- \n  rf_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nxgpredict <- \n  xg_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nFor your assignment: Interpret the metrics output of each. Compare them. How does each model do relative to each other?\n\nmetrics(linearpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      43.6  \n2 rsq     standard       0.279\n3 mae     standard      34.2  \n\n\n\nmetrics(rfpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      24.6  \n2 rsq     standard       0.839\n3 mae     standard      18.5  \n\n\n\nmetrics(xgpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      38.1  \n2 rsq     standard       0.455\n3 mae     standard      28.6  \n\n\nFor your assignment: Implement metrics for test. What happens?"
  },
  {
    "objectID": "lightgbm-for-regression.html#the-basics",
    "href": "lightgbm-for-regression.html#the-basics",
    "title": "12  LightGBM for regression",
    "section": "12.1 The basics",
    "text": "12.1 The basics\nReminder from before: LightGBM is another tree-based method that is similar to XGBoost but differs in ways that make it computationally more efficient. Where XGBoost and Random Forests are based on branches, LightGBM grows leaf-wise. Think of it like this – XGBoost uses lots of short trees with branches – it comes to a fork, makes a decision that reduces the amount of error the last tree got, and makes a new branch. LightGBM on the other hand, makes new leaves of each branch, which can mean lots of little splits, instead of big ones. That can lead to over-fitting on small datasets, but it also means it’s much faster than XGBoost.\nDid you catch that bold part? LightGBM is prone to overfitting on small datasets. Our dataset is small.\nLightGBM also uses histograms of the data to make choices, where XGBoost is comptutationally optimizing those choices. Roughly translated – LightGBM is looking at the fat part of a normal distribution to make choices, where XGBoost is tuning parameters to find the optimal path forward. It’s another reason why LightGBM is faster, but also not reliable with small datasets.\nWhat changes from before to now? Very little. Just the output – we’re predicting a number this time, not a category.\nLet’s implement a LightGBM model. We start with libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bonsai)\n\nset.seed(1234)\n\nWe’ll use the same data – wide receivers with college stats, draft information and fantasy points.\n\nwrdraftedstats <- read_csv(\"https://mattwaite.github.io/sportsdatafiles/wrdraftedstats20132022.csv\")\n\nWe thin up the inputs.\n\nwrselected <- wrdraftedstats %>%\n  select(\n    name,\n    year,\n    college_team,\n    nfl_team,\n    overall,\n    total_yards,\n    total_touchdowns,\n    FantPt\n  ) %>% na.omit()\n\nAnd split our data.\n\nplayer_split <- initial_split(wrselected, prop = .8)\n\nplayer_train <- training(player_split)\nplayer_test <- testing(player_split)\n\nNow a recipe.\n\nplayer_recipe <- \n  recipe(FantPt ~ ., data = player_train) %>%\n  update_role(name, year, college_team, nfl_team, new_role = \"ID\")\n\nsummary(player_recipe)\n\n# A tibble: 8 × 4\n  variable         type    role      source  \n  <chr>            <chr>   <chr>     <chr>   \n1 name             nominal ID        original\n2 year             numeric ID        original\n3 college_team     nominal ID        original\n4 nfl_team         nominal ID        original\n5 overall          numeric predictor original\n6 total_yards      numeric predictor original\n7 total_touchdowns numeric predictor original\n8 FantPt           numeric outcome   original"
  },
  {
    "objectID": "lightgbm-for-regression.html#implementing-lightgbm",
    "href": "lightgbm-for-regression.html#implementing-lightgbm",
    "title": "12  LightGBM for regression",
    "section": "12.2 Implementing LightGBM",
    "text": "12.2 Implementing LightGBM\nWe’re going to implement XGBoost and LightGBM side by side. That will give us the chance to compare.\nWe start with model definition.\n\nxg_mod <- boost_tree(\n  trees = tune(), \n  learn_rate = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  loss_reduction = tune(), \n  sample_size = tune(), \n  mtry = tune(), \n  ) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\")\n\nlightgbm_mod <- \n  boost_tree() %>%\n  set_engine(\"lightgbm\") %>%\n  set_mode(mode = \"regression\")\n\nNow we create workflows.\n\nxg_workflow <- \n  workflow() %>% \n  add_model(xg_mod) %>% \n  add_recipe(player_recipe)\n\nlightgbm_workflow <- \n  workflow() %>% \n  add_model(lightgbm_mod) %>% \n  add_recipe(player_recipe)\n\nWe’ll tune the XGBoost model.\n\nxgb_grid <- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), player_train),\n  learn_rate()\n)\n\nplayer_folds <- vfold_cv(player_train)\n\nxgb_res <- tune_grid(\n  xg_workflow,\n  resamples = player_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nbest_rmse <- select_best(xgb_res, \"rmse\")\n\nfinal_xgb <- finalize_workflow(\n  xg_workflow,\n  best_rmse\n)\n\nNow we make fits.\n\nxg_fit <- \n  final_xgb %>% \n  fit(data = player_train)\n\nlightgbm_fit <- \n  lightgbm_workflow %>% \n  fit(data = player_train)\n\nWith the fits in hand, we can bind the predictions to the data.\n\nxgpredict <- \n  xg_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nlightgbmpredict <- \n  lightgbm_fit %>% \n  predict(new_data = player_train) %>%\n  bind_cols(player_train) \n\nFor your assignment: How do these two compare?\n\nmetrics(xgpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      38.3  \n2 rsq     standard       0.449\n3 mae     standard      29.2  \n\n\n\nmetrics(lightgbmpredict, FantPt, .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      31.8  \n2 rsq     standard       0.638\n3 mae     standard      24.4  \n\n\nFor your assignment: How do these models fare in testing?"
  }
]