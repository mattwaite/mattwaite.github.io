<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 XGBoost | Advanced Sports Data Analysis</title>
  <meta name="description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 XGBoost | Advanced Sports Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 XGBoost | Advanced Sports Data Analysis" />
  
  <meta name="twitter:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

<meta name="author" content="By Matt Waite" />


<meta name="date" content="2021-02-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees-and-random-forests.html"/>
<link rel="next" href="support-vector-machines.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Sports Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#requirements-and-conventions"><i class="fa fa-check"></i><b>1.1</b> Requirements and Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-modeling-process.html"><a href="the-modeling-process.html"><i class="fa fa-check"></i><b>2</b> The modeling process</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-modeling-process.html"><a href="the-modeling-process.html#setting-up-the-modeling-process"><i class="fa fa-check"></i><b>2.1</b> Setting up the modeling process</a></li>
<li class="chapter" data-level="2.2" data-path="the-modeling-process.html"><a href="the-modeling-process.html#predicting-based-on-the-model"><i class="fa fa-check"></i><b>2.2</b> Predicting based on the model</a></li>
<li class="chapter" data-level="2.3" data-path="the-modeling-process.html"><a href="the-modeling-process.html#predicting-data-we-havent-seen-before"><i class="fa fa-check"></i><b>2.3</b> Predicting data we haven’t seen before</a></li>
<li class="chapter" data-level="2.4" data-path="the-modeling-process.html"><a href="the-modeling-process.html#looking-locally"><i class="fa fa-check"></i><b>2.4</b> Looking locally</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html"><i class="fa fa-check"></i><b>3</b> Multiple regression and feature engineering</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#a-multiple-regression-speed-run"><i class="fa fa-check"></i><b>3.1</b> A multiple regression speed run</a></li>
<li class="chapter" data-level="3.2" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#picking-what-moves-the-needle"><i class="fa fa-check"></i><b>3.2</b> Picking what moves the needle</a></li>
<li class="chapter" data-level="3.3" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#feature-engineering"><i class="fa fa-check"></i><b>3.3</b> Feature engineering</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html"><i class="fa fa-check"></i><b>4</b> Decision trees and random forests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#an-intro-to-pre-processing"><i class="fa fa-check"></i><b>4.1</b> An intro to pre-processing</a></li>
<li class="chapter" data-level="4.2" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision trees</a></li>
<li class="chapter" data-level="4.3" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#random-forest"><i class="fa fa-check"></i><b>4.3</b> Random forest</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>5</b> XGBoost</a>
<ul>
<li class="chapter" data-level="5.1" data-path="xgboost.html"><a href="xgboost.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Sports Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="xgboost" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> XGBoost</h1>
<p>As we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular.</p>
<p>Boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on.</p>
<p>The path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost, are very popular for solving a wide variety of problems.</p>
<p>Let’s re-implement our predictions for possessions in an XGBoost algorithm. First, we’ll load libraries and we’re going to introduce a new one here – doParallel – which handles using more of your computer’s processor cores to accomplish tasks in parallel instead of one core at a time. In other words, instead of one task, it can do X at a time in parallel and put the answers together after.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="xgboost.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb99-2"><a href="xgboost.html#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb99-3"><a href="xgboost.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(zoo)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="xgboost.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb102-2"><a href="xgboost.html#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="xgboost.html#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(doParallel)</span></code></pre></div>
<pre><code>## Loading required package: doParallel</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="xgboost.html#cb109-1" aria-hidden="true" tabindex="-1"></a>cores <span class="ot">&lt;-</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>(<span class="at">logical =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>We’ll load our game data.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="xgboost.html#cb110-1" aria-hidden="true" tabindex="-1"></a>games <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;~/Documents/Books/AdvancedSportsDataAnalysis/data/cbblogs1521.csv&quot;</span>)</span></code></pre></div>
<pre><code>## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   .default = col_double(),
##   Season = col_character(),
##   Date = col_date(format = &quot;&quot;),
##   TeamFull = col_character(),
##   Opponent = col_character(),
##   HomeAway = col_character(),
##   W_L = col_character(),
##   URL = col_character(),
##   Conference = col_character(),
##   Team = col_character()
## )
## ℹ Use `spec()` for the full column specifications.</code></pre>
<p>Now we’ll do a spot of feature engineering. I’m going to combine a rolling mean and a cumulative mean with the opponent’s rating to try and predict possessions.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="xgboost.html#cb112-1" aria-hidden="true" tabindex="-1"></a>rolling <span class="ot">&lt;-</span> games <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(</span>
<span id="cb112-2"><a href="xgboost.html#cb112-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">TeamPossessions =</span> TeamFGA <span class="sc">-</span> TeamOffRebounds <span class="sc">+</span> TeamTurnovers <span class="sc">+</span> (.<span class="dv">475</span> <span class="sc">*</span> TeamFTA),</span>
<span id="cb112-3"><a href="xgboost.html#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">TeamPPP =</span> TeamScore<span class="sc">/</span>TeamPossessions) <span class="sc">%&gt;%</span></span>
<span id="cb112-4"><a href="xgboost.html#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Team, Season) <span class="sc">%&gt;%</span></span>
<span id="cb112-5"><a href="xgboost.html#cb112-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb112-6"><a href="xgboost.html#cb112-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Possessions =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(TeamPossessions,<span class="at">n=</span><span class="dv">1</span>), <span class="at">k =</span> <span class="dv">2</span>, <span class="at">fill=</span>TeamPossessions),</span>
<span id="cb112-7"><a href="xgboost.html#cb112-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Turnovers =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(TeamTurnovers,<span class="at">n=</span><span class="dv">1</span>), <span class="at">k =</span> <span class="dv">2</span>, <span class="at">fill=</span>TeamTurnovers),</span>
<span id="cb112-8"><a href="xgboost.html#cb112-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_FGA =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(TeamFGA,<span class="at">n=</span><span class="dv">1</span>), <span class="at">k =</span> <span class="dv">2</span>, <span class="at">fill=</span>TeamFGA),</span>
<span id="cb112-9"><a href="xgboost.html#cb112-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">Cumulative_Mean_Possessions =</span> <span class="fu">cummean</span>(TeamPossessions),</span>
<span id="cb112-10"><a href="xgboost.html#cb112-10" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span> <span class="fu">select</span>(Team, Opponent, Date, Season, TeamPossessions, Rolling_Mean_Possessions, Cumulative_Mean_Possessions, Rolling_Mean_Turnovers, Rolling_Mean_FGA, OpponentSRS) <span class="sc">%&gt;%</span> <span class="fu">na.omit</span>()</span></code></pre></div>
<p>Per usual, we split our data into training and testing.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="xgboost.html#cb113-1" aria-hidden="true" tabindex="-1"></a>rolling_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(rolling, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb113-2"><a href="xgboost.html#cb113-2" aria-hidden="true" tabindex="-1"></a>rolling_train <span class="ot">&lt;-</span> <span class="fu">training</span>(rolling_split)</span>
<span id="cb113-3"><a href="xgboost.html#cb113-3" aria-hidden="true" tabindex="-1"></a>rolling_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(rolling_split)</span></code></pre></div>
<p>And our recipe.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="xgboost.html#cb114-1" aria-hidden="true" tabindex="-1"></a>rolling_rec <span class="ot">&lt;-</span> </span>
<span id="cb114-2"><a href="xgboost.html#cb114-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(TeamPossessions <span class="sc">~</span> ., <span class="at">data =</span> rolling_train) <span class="sc">%&gt;%</span></span>
<span id="cb114-3"><a href="xgboost.html#cb114-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_role</span>(Team, Opponent, Date, Season, <span class="at">new_role =</span> <span class="st">&quot;ID&quot;</span>)</span>
<span id="cb114-4"><a href="xgboost.html#cb114-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-5"><a href="xgboost.html#cb114-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(rolling_rec)</span></code></pre></div>
<pre><code>## # A tibble: 10 x 4
##    variable                    type    role      source  
##    &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 Team                        nominal ID        original
##  2 Opponent                    nominal ID        original
##  3 Date                        date    ID        original
##  4 Season                      nominal ID        original
##  5 Rolling_Mean_Possessions    numeric predictor original
##  6 Cumulative_Mean_Possessions numeric predictor original
##  7 Rolling_Mean_Turnovers      numeric predictor original
##  8 Rolling_Mean_FGA            numeric predictor original
##  9 OpponentSRS                 numeric predictor original
## 10 TeamPossessions             numeric outcome   original</code></pre>
<p>To this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to.</p>
<div id="hyperparameters" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Hyperparameters</h2>
<p>The hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters:</p>
<ol style="list-style-type: decimal">
<li>Number of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem.</li>
<li>Learn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point.</li>
<li>Tree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting.</li>
<li>Minimum number of observations in the terminal node – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting).</li>
</ol>
<p>Other settings:</p>
<p>Loss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative.
Sample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfittting.
mtry – the number of predictors that will be randomly sampled at each split when making trees.</p>
<p>All of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning.</p>
<p>First, we make a mode and label each parameter as tune()</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="xgboost.html#cb116-1" aria-hidden="true" tabindex="-1"></a>xg_mod <span class="ot">&lt;-</span>   <span class="fu">boost_tree</span>(</span>
<span id="cb116-2"><a href="xgboost.html#cb116-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">tune</span>(), </span>
<span id="cb116-3"><a href="xgboost.html#cb116-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb116-4"><a href="xgboost.html#cb116-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">tune</span>(), </span>
<span id="cb116-5"><a href="xgboost.html#cb116-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> <span class="fu">tune</span>(),</span>
<span id="cb116-6"><a href="xgboost.html#cb116-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fu">tune</span>(), </span>
<span id="cb116-7"><a href="xgboost.html#cb116-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="fu">tune</span>(), </span>
<span id="cb116-8"><a href="xgboost.html#cb116-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fu">tune</span>(), </span>
<span id="cb116-9"><a href="xgboost.html#cb116-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb116-10"><a href="xgboost.html#cb116-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb116-11"><a href="xgboost.html#cb116-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>, <span class="at">nthread =</span> cores)</span></code></pre></div>
<p>Let’s make a workflow now that we have our recipe and our model.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="xgboost.html#cb117-1" aria-hidden="true" tabindex="-1"></a>rolling_wflow <span class="ot">&lt;-</span> </span>
<span id="cb117-2"><a href="xgboost.html#cb117-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb117-3"><a href="xgboost.html#cb117-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(xg_mod) <span class="sc">%&gt;%</span> </span>
<span id="cb117-4"><a href="xgboost.html#cb117-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(rolling_rec)</span></code></pre></div>
<p>Now, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. What follows is what parameters the hypercube will tune.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="xgboost.html#cb118-1" aria-hidden="true" tabindex="-1"></a>xgb_grid <span class="ot">&lt;-</span> <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb118-2"><a href="xgboost.html#cb118-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">trees</span>(),</span>
<span id="cb118-3"><a href="xgboost.html#cb118-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tree_depth</span>(),</span>
<span id="cb118-4"><a href="xgboost.html#cb118-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min_n</span>(),</span>
<span id="cb118-5"><a href="xgboost.html#cb118-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">loss_reduction</span>(),</span>
<span id="cb118-6"><a href="xgboost.html#cb118-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="fu">sample_prop</span>(),</span>
<span id="cb118-7"><a href="xgboost.html#cb118-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(<span class="fu">mtry</span>(), rolling_train),</span>
<span id="cb118-8"><a href="xgboost.html#cb118-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">learn_rate</span>(),</span>
<span id="cb118-9"><a href="xgboost.html#cb118-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> <span class="dv">30</span></span>
<span id="cb118-10"><a href="xgboost.html#cb118-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-11"><a href="xgboost.html#cb118-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-12"><a href="xgboost.html#cb118-12" aria-hidden="true" tabindex="-1"></a>xgb_grid</span></code></pre></div>
<pre><code>## # A tibble: 30 x 7
##    trees tree_depth min_n loss_reduction sample_size  mtry learn_rate
##    &lt;int&gt;      &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;
##  1  1257         14    16       3.49e- 8       0.688     2   2.41e-10
##  2   569          9    27       6.65e- 9       0.445     1   1.16e- 3
##  3  1990          7    25       1.11e- 3       0.185     6   1.92e- 7
##  4  1594         10    12       2.17e-10       0.619     5   3.19e- 6
##  5   475          1    14       2.78e- 7       0.731     4   2.18e- 9
##  6   696          9    23       3.92e- 7       0.133     9   1.59e- 8
##  7  1741         11    36       2.49e- 5       0.194     9   6.11e- 3
##  8   664         11    35       2.28e+ 1       0.298     5   4.25e- 2
##  9  1401         15    33       2.68e+ 0       0.348     7   3.52e- 9
## 10    56          8     7       1.69e- 8       0.477     5   1.50e- 6
## # … with 20 more rows</code></pre>
<p>How do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="xgboost.html#cb120-1" aria-hidden="true" tabindex="-1"></a>rolling_folds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(rolling_train)</span>
<span id="cb120-2"><a href="xgboost.html#cb120-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-3"><a href="xgboost.html#cb120-3" aria-hidden="true" tabindex="-1"></a>rolling_folds</span></code></pre></div>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits               id    
##    &lt;list&gt;               &lt;chr&gt; 
##  1 &lt;split [32.7K/3.6K]&gt; Fold01
##  2 &lt;split [32.7K/3.6K]&gt; Fold02
##  3 &lt;split [32.7K/3.6K]&gt; Fold03
##  4 &lt;split [32.7K/3.6K]&gt; Fold04
##  5 &lt;split [32.7K/3.6K]&gt; Fold05
##  6 &lt;split [32.7K/3.6K]&gt; Fold06
##  7 &lt;split [32.7K/3.6K]&gt; Fold07
##  8 &lt;split [32.7K/3.6K]&gt; Fold08
##  9 &lt;split [32.7K/3.6K]&gt; Fold09
## 10 &lt;split [32.7K/3.6K]&gt; Fold10</code></pre>
<p>This part takes about 25-30 minutes on my machine and it will saturate all of your processors, so your computer just needs to sit there. No texting, no YouTube, nothing. Let it burn.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="xgboost.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">234</span>)</span>
<span id="cb122-2"><a href="xgboost.html#cb122-2" aria-hidden="true" tabindex="-1"></a>doParallel<span class="sc">::</span><span class="fu">registerDoParallel</span>(<span class="at">cores =</span> cores)</span>
<span id="cb122-3"><a href="xgboost.html#cb122-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-4"><a href="xgboost.html#cb122-4" aria-hidden="true" tabindex="-1"></a>xgb_res <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb122-5"><a href="xgboost.html#cb122-5" aria-hidden="true" tabindex="-1"></a>  rolling_wflow,</span>
<span id="cb122-6"><a href="xgboost.html#cb122-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamples =</span> rolling_folds,</span>
<span id="cb122-7"><a href="xgboost.html#cb122-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">grid =</span> xgb_grid,</span>
<span id="cb122-8"><a href="xgboost.html#cb122-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>)</span>
<span id="cb122-9"><a href="xgboost.html#cb122-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;rlang&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,
##     flatten_lgl, flatten_raw, invoke, list_along, modify, prepend,
##     splice</code></pre>
<pre><code>## 
## Attaching package: &#39;vctrs&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     data_frame</code></pre>
<pre><code>## The following object is masked from &#39;package:tibble&#39;:
## 
##     data_frame</code></pre>
<pre><code>## 
## Attaching package: &#39;xgboost&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="xgboost.html#cb130-1" aria-hidden="true" tabindex="-1"></a>doParallel<span class="sc">::</span><span class="fu">stopImplicitCluster</span>()</span>
<span id="cb130-2"><a href="xgboost.html#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="xgboost.html#cb130-3" aria-hidden="true" tabindex="-1"></a>xgb_res</span></code></pre></div>
<pre><code>## # Tuning results
## # 10-fold cross-validation 
## # A tibble: 10 x 5
##    splits            id     .metrics         .notes         .predictions        
##    &lt;list&gt;            &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;         &lt;list&gt;              
##  1 &lt;split [32.7K/3.… Fold01 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  2 &lt;split [32.7K/3.… Fold02 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  3 &lt;split [32.7K/3.… Fold03 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  4 &lt;split [32.7K/3.… Fold04 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  5 &lt;split [32.7K/3.… Fold05 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  6 &lt;split [32.7K/3.… Fold06 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  7 &lt;split [32.7K/3.… Fold07 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  8 &lt;split [32.7K/3.… Fold08 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
##  9 &lt;split [32.7K/3.… Fold09 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …
## 10 &lt;split [32.7K/3.… Fold10 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [108,930 × …</code></pre>
<p>So our grid has run on all of our validation samples, and what do we see?</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="xgboost.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(xgb_res)</span></code></pre></div>
<pre><code>## # A tibble: 60 x 13
##     mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
##  1     4   475    14          1    2.18e-9    0.000000278       0.731 rmse   
##  2     4   475    14          1    2.18e-9    0.000000278       0.731 rsq    
##  3     6  1114    17          2    9.26e-6    1.61              0.240 rmse   
##  4     6  1114    17          2    9.26e-6    1.61              0.240 rsq    
##  5     3  1177    31          2    6.04e-5   12.8               0.964 rmse   
##  6     3  1177    31          2    6.04e-5   12.8               0.964 rsq    
##  7     7   163    28          3    7.06e-9    0.000592          0.494 rmse   
##  8     7   163    28          3    7.06e-9    0.000592          0.494 rsq    
##  9     4  1475    32          3    1.78e-3    0.00000127        0.534 rmse   
## 10     4  1475    32          3    1.78e-3    0.00000127        0.534 rsq    
## # … with 50 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;,
## #   n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;</code></pre>
<p>Well we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="xgboost.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(xgb_res, <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 13
##    mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
## 1    10  1665    19         12    0.00876       3.44e- 1       0.990 rmse   
## 2     9  1741    36         11    0.00611       2.49e- 5       0.194 rmse   
## 3     8   916     3         13    0.0172        2.81e-10       0.867 rmse   
## 4     3  1382    24          6    0.0732        2.72e- 9       0.701 rmse   
## 5     5   664    35         11    0.0425        2.28e+ 1       0.298 rmse   
## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;,
## #   std_err &lt;dbl&gt;, .config &lt;chr&gt;</code></pre>
<p>The best combination comes up with an RMSE of 4.1708. Second is 4.1769, so very, very close.</p>
<p>Let’s capture our best set of hyperparameters.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="xgboost.html#cb136-1" aria-hidden="true" tabindex="-1"></a>best_rmse <span class="ot">&lt;-</span> <span class="fu">select_best</span>(xgb_res, <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>And now put that into a final workflow. Pay attention to the main arguments in the output below.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="xgboost.html#cb137-1" aria-hidden="true" tabindex="-1"></a>final_xgb <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(</span>
<span id="cb137-2"><a href="xgboost.html#cb137-2" aria-hidden="true" tabindex="-1"></a>  rolling_wflow,</span>
<span id="cb137-3"><a href="xgboost.html#cb137-3" aria-hidden="true" tabindex="-1"></a>  best_rmse</span>
<span id="cb137-4"><a href="xgboost.html#cb137-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb137-5"><a href="xgboost.html#cb137-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-6"><a href="xgboost.html#cb137-6" aria-hidden="true" tabindex="-1"></a>final_xgb</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: boost_tree()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 0 Recipe Steps
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = 10
##   trees = 1665
##   min_n = 19
##   tree_depth = 12
##   learn_rate = 0.00876331042129029
##   loss_reduction = 0.343906082245162
##   sample_size = 0.990407944282051
## 
## Engine-Specific Arguments:
##   nthread = cores
## 
## Computational engine: xgboost</code></pre>
<p>There’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along.</p>
<p>We create a fit.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="xgboost.html#cb139-1" aria-hidden="true" tabindex="-1"></a>xg_fit <span class="ot">&lt;-</span> </span>
<span id="cb139-2"><a href="xgboost.html#cb139-2" aria-hidden="true" tabindex="-1"></a>  final_xgb <span class="sc">%&gt;%</span> </span>
<span id="cb139-3"><a href="xgboost.html#cb139-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> rolling_train)</span></code></pre></div>
<p>We can see something things about that fit, including all the iterations of our XGBoost model. Note: our tuned number of trees is 1,665 – and in the workflow fit, you can see 1,665 iterations. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The RMSE goes down with each iteration as we go down the gradient desent.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="xgboost.html#cb140-1" aria-hidden="true" tabindex="-1"></a>xg_fit <span class="sc">%&gt;%</span> </span>
<span id="cb140-2"><a href="xgboost.html#cb140-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_workflow_fit</span>() </span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  1m 28s 
## ##### xgb.Booster
## raw: 10.6 Mb 
## call:
##   xgboost::xgb.train(params = list(eta = 0.00876331042129029, max_depth = 12L, 
##     gamma = 0.343906082245162, colsample_bytree = 1, min_child_weight = 19L, 
##     subsample = 0.990407944282051), data = x$data, nrounds = 1665L, 
##     watchlist = x$watchlist, verbose = 0, objective = &quot;reg:squarederror&quot;, 
##     nthread = 6L)
## params (as set within xgb.train):
##   eta = &quot;0.00876331042129029&quot;, max_depth = &quot;12&quot;, gamma = &quot;0.343906082245162&quot;, colsample_bytree = &quot;1&quot;, min_child_weight = &quot;19&quot;, subsample = &quot;0.990407944282051&quot;, objective = &quot;reg:squarederror&quot;, nthread = &quot;6&quot;, validate_parameters = &quot;TRUE&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.evaluation.log()
## # of features: 5 
## niter: 1665
## nfeatures : 5 
## evaluation_log:
##     iter training_rmse
##        1     69.588165
##        2     68.981148
## ---                   
##     1664      3.521024
##     1665      3.520814</code></pre>
<p>Now, like before, we can bind our predictions using our xg_fit to the rolling_train data.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="xgboost.html#cb142-1" aria-hidden="true" tabindex="-1"></a>trainresults <span class="ot">&lt;-</span> rolling_train <span class="sc">%&gt;%</span></span>
<span id="cb142-2"><a href="xgboost.html#cb142-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(xg_fit, rolling_train))</span></code></pre></div>
<p>And now see how we did.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="xgboost.html#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(trainresults, <span class="at">truth =</span> TeamPossessions, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       3.52 
## 2 rsq     standard       0.713
## 3 mae     standard       2.72</code></pre>
<p>How about the test data?</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="xgboost.html#cb145-1" aria-hidden="true" tabindex="-1"></a>testresults <span class="ot">&lt;-</span> rolling_test <span class="sc">%&gt;%</span></span>
<span id="cb145-2"><a href="xgboost.html#cb145-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(xg_fit, rolling_test))</span></code></pre></div>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="xgboost.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(testresults, <span class="at">truth =</span> TeamPossessions, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       4.19 
## 2 rsq     standard       0.598
## 3 mae     standard       3.22</code></pre>
<p>More robust than the random forest, but still a fair drop in RMSE and R squared.</p>
<p>Let’s try one where we know the outcome. In Febuary, Nebraska played Maryland back to back and lost both. Here’s the inputs for the second game:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="xgboost.html#cb148-1" aria-hidden="true" tabindex="-1"></a>poss_prediction <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb148-2"><a href="xgboost.html#cb148-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Team=</span><span class="st">&quot;Nebraska&quot;</span>,</span>
<span id="cb148-3"><a href="xgboost.html#cb148-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Opponent=</span><span class="st">&quot;Maryland&quot;</span>,</span>
<span id="cb148-4"><a href="xgboost.html#cb148-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Date =</span> <span class="fu">as.Date</span>(<span class="st">&quot;2021-02-16&quot;</span>),</span>
<span id="cb148-5"><a href="xgboost.html#cb148-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">Season =</span> <span class="st">&quot;2020-2021&quot;</span>,</span>
<span id="cb148-6"><a href="xgboost.html#cb148-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Possessions =</span> <span class="fl">72.5000</span>,</span>
<span id="cb148-7"><a href="xgboost.html#cb148-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Cumulative_Mean_Possessions =</span> <span class="fl">74.54211</span>,</span>
<span id="cb148-8"><a href="xgboost.html#cb148-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Turnovers =</span> <span class="dv">10</span>,</span>
<span id="cb148-9"><a href="xgboost.html#cb148-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_FGA =</span> <span class="fl">63.0</span>,</span>
<span id="cb148-10"><a href="xgboost.html#cb148-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">OpponentSRS =</span> <span class="fl">15.41</span></span>
<span id="cb148-11"><a href="xgboost.html#cb148-11" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> <span class="fu">add_row</span>(</span>
<span id="cb148-12"><a href="xgboost.html#cb148-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">Team=</span><span class="st">&quot;Maryland&quot;</span>,</span>
<span id="cb148-13"><a href="xgboost.html#cb148-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">Opponent=</span><span class="st">&quot;Nebraska&quot;</span>,</span>
<span id="cb148-14"><a href="xgboost.html#cb148-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">Date =</span> <span class="fu">as.Date</span>(<span class="st">&quot;2021-02-16&quot;</span>),</span>
<span id="cb148-15"><a href="xgboost.html#cb148-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">Season =</span> <span class="st">&quot;2020-2021&quot;</span>,</span>
<span id="cb148-16"><a href="xgboost.html#cb148-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Possessions =</span> <span class="fl">61.7500</span>,</span>
<span id="cb148-17"><a href="xgboost.html#cb148-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">Cumulative_Mean_Possessions =</span> <span class="fl">67.14239</span>,</span>
<span id="cb148-18"><a href="xgboost.html#cb148-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_Turnovers =</span> <span class="dv">10</span>,</span>
<span id="cb148-19"><a href="xgboost.html#cb148-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rolling_Mean_FGA =</span> <span class="fl">52.0</span>,</span>
<span id="cb148-20"><a href="xgboost.html#cb148-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">OpponentSRS =</span> <span class="fl">5.91</span></span>
<span id="cb148-21"><a href="xgboost.html#cb148-21" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>If I take that tibble and bind my predictions to it, I come up with the following:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="xgboost.html#cb149-1" aria-hidden="true" tabindex="-1"></a>poss_prediction <span class="sc">%&gt;%</span></span>
<span id="cb149-2"><a href="xgboost.html#cb149-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(xg_fit, poss_prediction))</span></code></pre></div>
<pre><code>## # A tibble: 2 x 10
##   Team  Opponent Date       Season Rolling_Mean_Po… Cumulative_Mean…
##   &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt;            &lt;dbl&gt;
## 1 Nebr… Maryland 2021-02-16 2020-…             72.5             74.5
## 2 Mary… Nebraska 2021-02-16 2020-…             61.8             67.1
## # … with 4 more variables: Rolling_Mean_Turnovers &lt;dbl&gt;,
## #   Rolling_Mean_FGA &lt;dbl&gt;, OpponentSRS &lt;dbl&gt;, .pred &lt;dbl&gt;</code></pre>
<p>Prediction: Nebraska would have 71.2 possessions, Maryland would have 62.8.</p>
<p>Reality: Nebraska 68, Maryland 63.7.</p>
<p>Not bad at all.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees-and-random-forests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machines.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-xgboost.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["advancedsportsdataanalysis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
