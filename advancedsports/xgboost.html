<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 XGBoost | Advanced Sports Data Analysis</title>
  <meta name="description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 XGBoost | Advanced Sports Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 XGBoost | Advanced Sports Data Analysis" />
  
  <meta name="twitter:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

<meta name="author" content="By Matt Waite" />


<meta name="date" content="2022-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees-and-random-forests.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Sports Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#requirements-and-conventions"><i class="fa fa-check"></i><b>1.1</b> Requirements and Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html"><i class="fa fa-check"></i><b>2</b> The modeling process and linear regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html#feature-engineering"><i class="fa fa-check"></i><b>2.1</b> Feature engineering</a></li>
<li class="chapter" data-level="2.2" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html#setting-up-the-modeling-process"><i class="fa fa-check"></i><b>2.2</b> Setting up the modeling process</a></li>
<li class="chapter" data-level="2.3" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html#predicting-based-on-the-model"><i class="fa fa-check"></i><b>2.3</b> Predicting based on the model</a></li>
<li class="chapter" data-level="2.4" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html#predicting-data-we-havent-seen-before"><i class="fa fa-check"></i><b>2.4</b> Predicting data we haven’t seen before</a></li>
<li class="chapter" data-level="2.5" data-path="the-modeling-process-and-linear-regression.html"><a href="the-modeling-process-and-linear-regression.html#looking-locally"><i class="fa fa-check"></i><b>2.5</b> Looking locally</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#a-multiple-regression-speed-run"><i class="fa fa-check"></i><b>3.1</b> A multiple regression speed run</a></li>
<li class="chapter" data-level="3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#picking-what-moves-the-needle"><i class="fa fa-check"></i><b>3.2</b> Picking what moves the needle</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html"><i class="fa fa-check"></i><b>4</b> Decision trees and random forests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#an-intro-to-pre-processing"><i class="fa fa-check"></i><b>4.1</b> An intro to pre-processing</a></li>
<li class="chapter" data-level="4.2" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision trees</a></li>
<li class="chapter" data-level="4.3" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#random-forest"><i class="fa fa-check"></i><b>4.3</b> Random forest</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>5</b> XGBoost</a>
<ul>
<li class="chapter" data-level="5.1" data-path="xgboost.html"><a href="xgboost.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Sports Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="xgboost" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> XGBoost</h1>
<p>As we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular.</p>
<p>Boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on.</p>
<p>The path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost, are very popular for solving a wide variety of problems.</p>
<p>Let’s re-implement our predictions in an XGBoost algorithm. First, we’ll load libraries and we’re going to introduce a new one here – doParallel – which handles using more of your computer’s processor cores to accomplish tasks in parallel instead of one core at a time. In other words, instead of one task, it can do X at a time in parallel and put the answers together after.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="xgboost.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb92-2"><a href="xgboost.html#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb92-3"><a href="xgboost.html#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(zoo)</span>
<span id="cb92-4"><a href="xgboost.html#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hoopR)</span>
<span id="cb92-5"><a href="xgboost.html#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="xgboost.html#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb92-7"><a href="xgboost.html#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="xgboost.html#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb92-9"><a href="xgboost.html#cb92-9" aria-hidden="true" tabindex="-1"></a>cores <span class="ot">&lt;-</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>(<span class="at">logical =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>We’ll load our game data and do a spot of feature engineering that we used with random forests.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="xgboost.html#cb93-1" aria-hidden="true" tabindex="-1"></a>teamgames <span class="ot">&lt;-</span> <span class="fu">load_mbb_team_box</span>(<span class="at">seasons =</span> <span class="dv">2015</span><span class="sc">:</span><span class="dv">2022</span>) <span class="sc">%&gt;%</span></span>
<span id="cb93-2"><a href="xgboost.html#cb93-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(field_goals_made_field_goals_attempted, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;field_goals_made&quot;</span>,<span class="st">&quot;field_goals_attempted&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb93-3"><a href="xgboost.html#cb93-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(three_point_field_goals_made_three_point_field_goals_attempted, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;three_point_field_goals_made&quot;</span>,<span class="st">&quot;three_point_field_goals_attempted&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb93-4"><a href="xgboost.html#cb93-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(free_throws_made_free_throws_attempted, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;free_throws_made&quot;</span>,<span class="st">&quot;free_throws_attempted&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb93-5"><a href="xgboost.html#cb93-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_at</span>(<span class="dv">12</span><span class="sc">:</span><span class="dv">35</span>, as.numeric)</span></code></pre></div>
<pre><code>## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="xgboost.html#cb95-1" aria-hidden="true" tabindex="-1"></a>teamstats <span class="ot">&lt;-</span> teamgames <span class="sc">%&gt;%</span> </span>
<span id="cb95-2"><a href="xgboost.html#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(team_short_display_name) <span class="sc">%&gt;%</span></span>
<span id="cb95-3"><a href="xgboost.html#cb95-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb95-4"><a href="xgboost.html#cb95-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">team_score =</span> ((field_goals_made<span class="sc">-</span>three_point_field_goals_made) <span class="sc">*</span> <span class="dv">2</span>) <span class="sc">+</span> (three_point_field_goals_made<span class="sc">*</span><span class="dv">3</span>) <span class="sc">+</span> free_throws_made,</span>
<span id="cb95-5"><a href="xgboost.html#cb95-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">possessions =</span> field_goals_attempted <span class="sc">-</span> offensive_rebounds <span class="sc">+</span> turnovers <span class="sc">+</span> (.<span class="dv">475</span> <span class="sc">*</span> free_throws_attempted),</span>
<span id="cb95-6"><a href="xgboost.html#cb95-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">ppp =</span> team_score<span class="sc">/</span>possessions,</span>
<span id="cb95-7"><a href="xgboost.html#cb95-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_shooting_percentage =</span> (team_score <span class="sc">/</span> (<span class="dv">2</span><span class="sc">*</span>(field_goals_attempted <span class="sc">+</span> (.<span class="dv">44</span> <span class="sc">*</span> free_throws_attempted)))) <span class="sc">*</span> <span class="dv">100</span>,</span>
<span id="cb95-8"><a href="xgboost.html#cb95-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">turnover_pct =</span> turnovers<span class="sc">/</span>(field_goals_attempted <span class="sc">+</span> <span class="fl">0.44</span> <span class="sc">*</span> free_throws_attempted <span class="sc">+</span> turnovers),</span>
<span id="cb95-9"><a href="xgboost.html#cb95-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">free_throw_factor =</span> free_throws_made<span class="sc">/</span>field_goals_attempted,</span>
<span id="cb95-10"><a href="xgboost.html#cb95-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_shooting_percentage =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(field_goal_pct, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>field_goal_pct),</span>
<span id="cb95-11"><a href="xgboost.html#cb95-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_ppp =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(ppp, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>ppp),</span>
<span id="cb95-12"><a href="xgboost.html#cb95-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_true_shooting_percentage =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(true_shooting_percentage, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>true_shooting_percentage),</span>
<span id="cb95-13"><a href="xgboost.html#cb95-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_turnover_percentage =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(turnover_pct, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>turnover_pct),</span>
<span id="cb95-14"><a href="xgboost.html#cb95-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_free_throw_factor =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(free_throw_factor, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>free_throw_factor),    </span>
<span id="cb95-15"><a href="xgboost.html#cb95-15" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> <span class="fu">ungroup</span>()</span>
<span id="cb95-16"><a href="xgboost.html#cb95-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-17"><a href="xgboost.html#cb95-17" aria-hidden="true" tabindex="-1"></a>opponent <span class="ot">&lt;-</span> teamstats <span class="sc">%&gt;%</span> <span class="fu">select</span>(game_id, team_id, offensive_rebounds, defensive_rebounds) <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">opponent_id=</span>team_id, <span class="at">opponent_offensive_rebounds =</span> offensive_rebounds, <span class="at">opponent_defensive_rebounds=</span>defensive_rebounds) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">opponent_id =</span> <span class="fu">as.numeric</span>(opponent_id))</span>
<span id="cb95-18"><a href="xgboost.html#cb95-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-19"><a href="xgboost.html#cb95-19" aria-hidden="true" tabindex="-1"></a>newteamstats <span class="ot">&lt;-</span> teamstats <span class="sc">%&gt;%</span> </span>
<span id="cb95-20"><a href="xgboost.html#cb95-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(opponent) <span class="sc">%&gt;%</span> </span>
<span id="cb95-21"><a href="xgboost.html#cb95-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb95-22"><a href="xgboost.html#cb95-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">orb =</span> offensive_rebounds <span class="sc">/</span> (offensive_rebounds <span class="sc">+</span> opponent_defensive_rebounds),</span>
<span id="cb95-23"><a href="xgboost.html#cb95-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">drb =</span> defensive_rebounds <span class="sc">/</span> (opponent_offensive_rebounds <span class="sc">+</span> defensive_rebounds),</span>
<span id="cb95-24"><a href="xgboost.html#cb95-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_orb =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(orb, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>orb),</span>
<span id="cb95-25"><a href="xgboost.html#cb95-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">rolling_drb =</span> <span class="fu">rollmean</span>(<span class="fu">lag</span>(drb, <span class="at">n=</span><span class="dv">1</span>), <span class="at">k=</span><span class="dv">2</span>, <span class="at">fill=</span>drb)</span>
<span id="cb95-26"><a href="xgboost.html#cb95-26" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<pre><code>## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;)</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="xgboost.html#cb97-1" aria-hidden="true" tabindex="-1"></a>modelgames <span class="ot">&lt;-</span> newteamstats <span class="sc">%&gt;%</span></span>
<span id="cb97-2"><a href="xgboost.html#cb97-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(team_short_display_name, opponent_name, game_date, season, team_score, rolling_true_shooting_percentage, rolling_free_throw_factor, rolling_turnover_percentage, rolling_orb, rolling_drb) <span class="sc">%&gt;%</span> <span class="fu">na.omit</span>()</span></code></pre></div>
<p>Per usual, we split our data into training and testing.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="xgboost.html#cb98-1" aria-hidden="true" tabindex="-1"></a>game_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(modelgames, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb98-2"><a href="xgboost.html#cb98-2" aria-hidden="true" tabindex="-1"></a>game_train <span class="ot">&lt;-</span> <span class="fu">training</span>(game_split)</span>
<span id="cb98-3"><a href="xgboost.html#cb98-3" aria-hidden="true" tabindex="-1"></a>game_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(game_split)</span></code></pre></div>
<p>And our recipe.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="xgboost.html#cb99-1" aria-hidden="true" tabindex="-1"></a>game_rec <span class="ot">&lt;-</span> </span>
<span id="cb99-2"><a href="xgboost.html#cb99-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(team_score <span class="sc">~</span> ., <span class="at">data =</span> game_train) <span class="sc">%&gt;%</span></span>
<span id="cb99-3"><a href="xgboost.html#cb99-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_role</span>(team_short_display_name, opponent_name, game_date, season, <span class="at">new_role =</span> <span class="st">&quot;ID&quot;</span>)</span>
<span id="cb99-4"><a href="xgboost.html#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="xgboost.html#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(game_rec)</span></code></pre></div>
<pre><code>## # A tibble: 10 × 4
##    variable                         type    role      source  
##    &lt;chr&gt;                            &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 team_short_display_name          nominal ID        original
##  2 opponent_name                    nominal ID        original
##  3 game_date                        date    ID        original
##  4 season                           numeric ID        original
##  5 rolling_true_shooting_percentage numeric predictor original
##  6 rolling_free_throw_factor        numeric predictor original
##  7 rolling_turnover_percentage      numeric predictor original
##  8 rolling_orb                      numeric predictor original
##  9 rolling_drb                      numeric predictor original
## 10 team_score                       numeric outcome   original</code></pre>
<p>To this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to.</p>
<div id="hyperparameters" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Hyperparameters</h2>
<p>The hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters:</p>
<ol style="list-style-type: decimal">
<li>Number of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem.</li>
<li>Learn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point.</li>
<li>Tree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting.</li>
<li>Minimum number of observations in the terminal node – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting).</li>
</ol>
<p>Other settings:</p>
<p>Loss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative.
Sample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfitting.
mtry – the number of predictors that will be randomly sampled at each split when making trees.</p>
<p>All of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning.</p>
<p>First, we make a mode and label each parameter as tune()</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="xgboost.html#cb101-1" aria-hidden="true" tabindex="-1"></a>xg_mod <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(</span>
<span id="cb101-2"><a href="xgboost.html#cb101-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">tune</span>(), </span>
<span id="cb101-3"><a href="xgboost.html#cb101-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb101-4"><a href="xgboost.html#cb101-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">tune</span>(), </span>
<span id="cb101-5"><a href="xgboost.html#cb101-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> <span class="fu">tune</span>(),</span>
<span id="cb101-6"><a href="xgboost.html#cb101-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fu">tune</span>(), </span>
<span id="cb101-7"><a href="xgboost.html#cb101-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="fu">tune</span>(), </span>
<span id="cb101-8"><a href="xgboost.html#cb101-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fu">tune</span>(), </span>
<span id="cb101-9"><a href="xgboost.html#cb101-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb101-10"><a href="xgboost.html#cb101-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb101-11"><a href="xgboost.html#cb101-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>, <span class="at">nthread =</span> cores)</span></code></pre></div>
<p>Let’s make a workflow now that we have our recipe and our model.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="xgboost.html#cb102-1" aria-hidden="true" tabindex="-1"></a>game_wflow <span class="ot">&lt;-</span> </span>
<span id="cb102-2"><a href="xgboost.html#cb102-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb102-3"><a href="xgboost.html#cb102-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(xg_mod) <span class="sc">%&gt;%</span> </span>
<span id="cb102-4"><a href="xgboost.html#cb102-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(game_rec)</span></code></pre></div>
<p>Now, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. What follows is what parameters the hypercube will tune.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="xgboost.html#cb103-1" aria-hidden="true" tabindex="-1"></a>xgb_grid <span class="ot">&lt;-</span> <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb103-2"><a href="xgboost.html#cb103-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">trees</span>(),</span>
<span id="cb103-3"><a href="xgboost.html#cb103-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tree_depth</span>(),</span>
<span id="cb103-4"><a href="xgboost.html#cb103-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min_n</span>(),</span>
<span id="cb103-5"><a href="xgboost.html#cb103-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">loss_reduction</span>(),</span>
<span id="cb103-6"><a href="xgboost.html#cb103-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="fu">sample_prop</span>(),</span>
<span id="cb103-7"><a href="xgboost.html#cb103-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(<span class="fu">mtry</span>(), game_train),</span>
<span id="cb103-8"><a href="xgboost.html#cb103-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">learn_rate</span>(),</span>
<span id="cb103-9"><a href="xgboost.html#cb103-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> <span class="dv">30</span></span>
<span id="cb103-10"><a href="xgboost.html#cb103-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-11"><a href="xgboost.html#cb103-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-12"><a href="xgboost.html#cb103-12" aria-hidden="true" tabindex="-1"></a>xgb_grid</span></code></pre></div>
<pre><code>## # A tibble: 30 × 7
##    trees tree_depth min_n loss_reduction sample_size  mtry learn_rate
##    &lt;int&gt;      &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;
##  1    24         11     2        2.35e-6       0.531     2   6.76e- 6
##  2  1693          5    26        5.94e-8       0.628     7   1.38e- 9
##  3  1013          7     8        5.28e-9       0.132     1   6.44e- 2
##  4   952         11    23        1.54e+1       0.811     4   1.22e- 3
##  5   608          2    11        1.57e-7       0.588     6   1.08e- 2
##  6  1646          9    33        1.09e-9       0.759     5   2.60e- 3
##  7   140         14    16        1.36e-1       0.313     4   6.52e-10
##  8  1321          6    37        1.43e-8       0.403     2   8.17e- 9
##  9   222          3    29        2.48e-5       0.488     5   1.28e- 7
## 10  1227         11    12        2.23e-8       0.501     8   8.74e- 8
## # … with 20 more rows</code></pre>
<p>How do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="xgboost.html#cb105-1" aria-hidden="true" tabindex="-1"></a>game_folds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(game_train)</span>
<span id="cb105-2"><a href="xgboost.html#cb105-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-3"><a href="xgboost.html#cb105-3" aria-hidden="true" tabindex="-1"></a>game_folds</span></code></pre></div>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 × 2
##    splits               id    
##    &lt;list&gt;               &lt;chr&gt; 
##  1 &lt;split [62413/6935]&gt; Fold01
##  2 &lt;split [62413/6935]&gt; Fold02
##  3 &lt;split [62413/6935]&gt; Fold03
##  4 &lt;split [62413/6935]&gt; Fold04
##  5 &lt;split [62413/6935]&gt; Fold05
##  6 &lt;split [62413/6935]&gt; Fold06
##  7 &lt;split [62413/6935]&gt; Fold07
##  8 &lt;split [62413/6935]&gt; Fold08
##  9 &lt;split [62414/6934]&gt; Fold09
## 10 &lt;split [62414/6934]&gt; Fold10</code></pre>
<p>This part takes about 25-30 minutes on my machine and it will saturate all of your processors, so your computer just needs to sit there. No texting, no YouTube, nothing. Let it burn.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="xgboost.html#cb107-1" aria-hidden="true" tabindex="-1"></a>doParallel<span class="sc">::</span><span class="fu">registerDoParallel</span>(<span class="at">cores =</span> cores)</span>
<span id="cb107-2"><a href="xgboost.html#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="xgboost.html#cb107-3" aria-hidden="true" tabindex="-1"></a>xgb_res <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb107-4"><a href="xgboost.html#cb107-4" aria-hidden="true" tabindex="-1"></a>  game_wflow,</span>
<span id="cb107-5"><a href="xgboost.html#cb107-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamples =</span> game_folds,</span>
<span id="cb107-6"><a href="xgboost.html#cb107-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">grid =</span> xgb_grid,</span>
<span id="cb107-7"><a href="xgboost.html#cb107-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>)</span>
<span id="cb107-8"><a href="xgboost.html#cb107-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb107-9"><a href="xgboost.html#cb107-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-10"><a href="xgboost.html#cb107-10" aria-hidden="true" tabindex="-1"></a>doParallel<span class="sc">::</span><span class="fu">stopImplicitCluster</span>()</span>
<span id="cb107-11"><a href="xgboost.html#cb107-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-12"><a href="xgboost.html#cb107-12" aria-hidden="true" tabindex="-1"></a>xgb_res</span></code></pre></div>
<pre><code>## Warning: This tuning result has notes. Example notes on model fitting include:
## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned.
## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned.
## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned.</code></pre>
<pre><code>## # Tuning results
## # 10-fold cross-validation 
## # A tibble: 10 × 5
##    splits               id     .metrics           .notes           .predictions 
##    &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;           &lt;list&gt;       
##  1 &lt;split [62413/6935]&gt; Fold01 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  2 &lt;split [62413/6935]&gt; Fold02 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  3 &lt;split [62413/6935]&gt; Fold03 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  4 &lt;split [62413/6935]&gt; Fold04 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  5 &lt;split [62413/6935]&gt; Fold05 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  6 &lt;split [62413/6935]&gt; Fold06 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  7 &lt;split [62413/6935]&gt; Fold07 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  8 &lt;split [62413/6935]&gt; Fold08 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
##  9 &lt;split [62414/6934]&gt; Fold09 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…
## 10 &lt;split [62414/6934]&gt; Fold10 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [208…</code></pre>
<p>So our grid has run on all of our validation samples, and what do we see?</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="xgboost.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(xgb_res)</span></code></pre></div>
<pre><code>## # A tibble: 60 × 13
##     mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
##  1     9  1855    38          1   1.29e- 5    1.50              0.936 rmse   
##  2     9  1855    38          1   1.29e- 5    1.50              0.936 rsq    
##  3     6   608    11          2   1.08e- 2    0.000000157       0.588 rmse   
##  4     6   608    11          2   1.08e- 2    0.000000157       0.588 rsq    
##  5     7  1166    28          2   3.29e-10    0.00000426        0.265 rmse   
##  6     7  1166    28          2   3.29e-10    0.00000426        0.265 rsq    
##  7     8  1397    22          3   3.72e- 6    0.0204            0.114 rmse   
##  8     8  1397    22          3   3.72e- 6    0.0204            0.114 rsq    
##  9     5   222    29          3   1.28e- 7    0.0000248         0.488 rmse   
## 10     5   222    29          3   1.28e- 7    0.0000248         0.488 rsq    
## # … with 50 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;,
## #   n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;</code></pre>
<p>Well we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="xgboost.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(xgb_res, <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 13
##    mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
## 1     5  1924     4          7    0.0158   0.000000896         0.975 rmse   
## 2     5  1646    33          9    0.00260  0.00000000109       0.759 rmse   
## 3     6   608    11          2    0.0108   0.000000157         0.588 rmse   
## 4     4  1102    32         14    0.0291   0.000748            0.832 rmse   
## 5     1  1013     8          7    0.0644   0.00000000528       0.132 rmse   
## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;,
## #   std_err &lt;dbl&gt;, .config &lt;chr&gt;</code></pre>
<p>The best combination as of this data update comes up with an RMSE of 9.93. Second is 9.97.</p>
<p>Let’s capture our best set of hyperparameters.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="xgboost.html#cb114-1" aria-hidden="true" tabindex="-1"></a>best_rmse <span class="ot">&lt;-</span> <span class="fu">select_best</span>(xgb_res, <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>And now put that into a final workflow. Pay attention to the main arguments in the output below.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="xgboost.html#cb115-1" aria-hidden="true" tabindex="-1"></a>final_xgb <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(</span>
<span id="cb115-2"><a href="xgboost.html#cb115-2" aria-hidden="true" tabindex="-1"></a>  game_wflow,</span>
<span id="cb115-3"><a href="xgboost.html#cb115-3" aria-hidden="true" tabindex="-1"></a>  best_rmse</span>
<span id="cb115-4"><a href="xgboost.html#cb115-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb115-5"><a href="xgboost.html#cb115-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-6"><a href="xgboost.html#cb115-6" aria-hidden="true" tabindex="-1"></a>final_xgb</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: boost_tree()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 0 Recipe Steps
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = 5
##   trees = 1924
##   min_n = 4
##   tree_depth = 7
##   learn_rate = 0.0158059709032973
##   loss_reduction = 8.96336082569855e-07
##   sample_size = 0.975113640134223
## 
## Engine-Specific Arguments:
##   nthread = cores
## 
## Computational engine: xgboost</code></pre>
<p>There’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along.</p>
<p>We create a fit.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="xgboost.html#cb117-1" aria-hidden="true" tabindex="-1"></a>xg_fit <span class="ot">&lt;-</span> </span>
<span id="cb117-2"><a href="xgboost.html#cb117-2" aria-hidden="true" tabindex="-1"></a>  final_xgb <span class="sc">%&gt;%</span> </span>
<span id="cb117-3"><a href="xgboost.html#cb117-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> game_train)</span></code></pre></div>
<p>We can see something things about that fit, including all the iterations of our XGBoost model. Note: our tuned number of trees is 1,665 – and in the workflow fit, you can see 1,665 iterations. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The RMSE goes down with each iteration as we go down the gradient desent.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="xgboost.html#cb118-1" aria-hidden="true" tabindex="-1"></a>xg_fit <span class="sc">%&gt;%</span> </span>
<span id="cb118-2"><a href="xgboost.html#cb118-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_workflow_fit</span>() </span></code></pre></div>
<pre><code>## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.
## Please use `extract_fit_parsnip()` instead.</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  2m 17.8s 
## ##### xgb.Booster
## raw: 13.4 Mb 
## call:
##   xgboost::xgb.train(params = list(eta = 0.0158059709032973, max_depth = 7L, 
##     gamma = 8.96336082569855e-07, colsample_bytree = 1, colsample_bynode = 1, 
##     min_child_weight = 4L, subsample = 0.975113640134223, objective = &quot;reg:squarederror&quot;), 
##     data = x$data, nrounds = 1924L, watchlist = x$watchlist, 
##     verbose = 0, nthread = 6L)
## params (as set within xgb.train):
##   eta = &quot;0.0158059709032973&quot;, max_depth = &quot;7&quot;, gamma = &quot;8.96336082569855e-07&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;1&quot;, min_child_weight = &quot;4&quot;, subsample = &quot;0.975113640134223&quot;, objective = &quot;reg:squarederror&quot;, nthread = &quot;6&quot;, validate_parameters = &quot;TRUE&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.evaluation.log()
## # of features: 5 
## niter: 1924
## nfeatures : 5 
## evaluation_log:
##     iter training_rmse
##        1     71.055603
##        2     69.955750
## ---                   
##     1923      8.955710
##     1924      8.955146</code></pre>
<p>Now, like before, we can bind our predictions using our xg_fit to the game_train data.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="xgboost.html#cb121-1" aria-hidden="true" tabindex="-1"></a>trainresults <span class="ot">&lt;-</span> game_train <span class="sc">%&gt;%</span></span>
<span id="cb121-2"><a href="xgboost.html#cb121-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(xg_fit, game_train))</span></code></pre></div>
<p>And now see how we did.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="xgboost.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(trainresults, <span class="at">truth =</span> team_score, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       8.96 
## 2 rsq     standard       0.553
## 3 mae     standard       6.95</code></pre>
<p>How about the test data?</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="xgboost.html#cb124-1" aria-hidden="true" tabindex="-1"></a>testresults <span class="ot">&lt;-</span> game_test <span class="sc">%&gt;%</span></span>
<span id="cb124-2"><a href="xgboost.html#cb124-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(xg_fit, game_test))</span></code></pre></div>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="xgboost.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(testresults, <span class="at">truth =</span> team_score, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      10.0  
## 2 rsq     standard       0.438
## 3 mae     standard       7.82</code></pre>
<p>Unlike the random forest, not nearly the drop in metrics between train and test.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees-and-random-forests.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-xgboost.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["advancedsportsdataanalysis.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
