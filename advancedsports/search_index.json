[["index.html", "Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R Chapter 1 Introduction 1.1 Requirements and Conventions 1.2 About this book", " Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R By Matt Waite 2022-02-03 Chapter 1 Introduction The 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic. For the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner. How close? This is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved. The specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here. First, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good. Second, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good. Third, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away. Using logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t. So how much of a corner would the team need to turn? With modeling, we can figure this out. What would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent? As played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa). So how close are they to turning the corner? That close. 1.1 Requirements and Conventions This book is all in the R statistical language. To follow along, you’ll do the following: Install the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer. Install R Studio Desktop. The free version is great. Going forward, you’ll see passages like this: install.packages(&quot;tidyverse&quot;) Don’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do. 1.2 About this book This book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it: It is free for students. The topics will remain the same but the text is going to be constantly tinkered with. What is the work of the author is copyright Matt Waite 2021. The text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either. As such, the whole book – authored in Bookdown – is open sourced on Github. Pull requests welcomed! "],["the-modeling-process-and-linear-regression.html", "Chapter 2 The modeling process and linear regression 2.1 Feature engineering 2.2 Setting up the modeling process 2.3 Predicting based on the model 2.4 Predicting data we haven’t seen before 2.5 Looking locally", " Chapter 2 The modeling process and linear regression One of the most common – and seemingly least rigorous – parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method – looking at how a team is playing, looking at the players, consulting their gut – but rarely ever do you hear of a sports pundit using a model. We’re going to change that. Throughout this book, you’ll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not). Let’s start by looking at predicting how many points the team should score given how well they are shooting. And we’ll use this as a chance to look at linear regression modeling. If you don’t have them already installed, we’ll need the tidyverse and tidymodels for this book. As well as zoo to make rolling means and hoopR for data. install.packages(c(&quot;tidyverse&quot;, &quot;tidymodels&quot;, &quot;zoo&quot;, &quot;hoopR&quot;) After they’ve installed – and if you haven’t this will take a bit – load them. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) For this walkthrough, we’re going to use a dataset of college basketball games from the 14-15 season through current games from hoopR. You can pull those from games from the library like this: Let’s load this data and do a little work on it. The first function pulls the team box scores, then I use dplyr’s separate and mutate_at functions to reformat hoopR’s ways of recording shots made and shots attempted. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) 2.1 Feature engineering Feature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good. Let’s look at basketball games again. A number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score? One problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this: Possessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts) Since we’re trying to predict how many points a team will score, we need to know that. If you look at the data, however, you’ll see that’s not actually in the data. Which is unfortunate. But we can calculate it pretty easily. Then we’ll use the possessions estimate formula to get that, so we can then calculate points per possession. While we’re here, we’ll add true shooting percentage as well – to try and incorporate some free throw shooting into our metrics. We’ll save that to a new dataframe called teamstats. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100 ) %&gt;% ungroup() Now we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar. Split your data into training and testing data sets. A common split is 80/20. Train the model on the training dataset. Evaluate the model on the training data. Apply the model to the testing data. Evaluate the model on the test data. From there, it’s how you want to use the model. We’ll walk through a simple example here, using the simplest model – a linear model. Linear models are something you’ve understood since you took middle school math and learned the equation of a line. Remember y = mx + b? It’s back. And, unlike what you complained bitterly in middle school, it’s very, very useful. What a linear model says, in words is that we can predict y if we multiply a value – a coefficient – by our x value offset with b, which is really the y-intercept, but think of it like where the line starts. Or, expressed as y = mx + b: points = true_shooting_percentage * ? + some starting point. Think of some starting point as what the score should be if the true_shooting_percentage is zero. Should be zero, right? Intuitively, yes, but it won’t always work out so easily. What we’re trying to do here is predict how many points a team should score given their shooting prowess as a team or their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats BEFORE the game – what we knew about the team going into the game in question. We can do that using zoo and rolling means. We’ll add three new columns – the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage. teamstats &lt;- teamstats %&gt;% group_by(team_short_display_name) %&gt;% mutate( rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=4, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=4, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=4, fill=true_shooting_percentage) ) %&gt;% ungroup() 2.2 Setting up the modeling process With most modeling tasks we need to start with setting a random number seed to aid our random splitting of data into training and testing. set.seed(1234) Random numbers play a large role in a lot of data science algorithms, so setting one helps our reproducibility. After that, we split our data. There’s a number of ways to do this – R has a bunch and you’ll find all kinds of examples online – but Tidymodels has made this easy. game_split &lt;- initial_split(teamstats, prop = .8) game_split ## &lt;Analysis/Assess/Total&gt; ## &lt;69068/17268/86336&gt; What does this mean? It says that initial_split divided the data into 68,000+ games in analysis (or training), 17,000+ into assess (or test), of the 86,000+ total records in the dataset. But the split object isn’t useful to us. We need to assign them to dataframes. We do so like this: game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Now we have two dataframes – game_train and game_test – that we can now use for modeling. First step to making a model is to set what type of model this will be. We’re going to name our model object – lm_model works because this is a linear model. We’ll use the linear_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “lm.” lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) We can get a peek at lm_model and make sure we did everything right by just typing it and executing. lm_model ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Now, let’s fit a linear model to our data. We’ll name the fitted model fit_lm and we’ll take our model object that we just created and fit it using the fit function. What goes in the fit function can be read like this: team_score is approximately modeled by the rolling mean of shooting percentage The only thing left is to specify the dataset. fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) Let’s take a look at what the fitted model object tells us about our data. tidy(fit_lm, conf.int = TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13.1 0.421 31.2 2.31e-212 12.3 14.0 ## 2 rolling_shooting_pe… 1.33 0.00953 139. 0 1.31 1.35 The two most important things to see here are the terms and the estimates. Start with rolling_shooting_percentage. What that says is for every 10 percentage points of shooting percentage, a team should score 13 points. HOWEVER, the intercept has something to say about this. What the intercept says is that a team with a big fat zero for shooting percentage is going to score 13 points. Wait … how? Well, are field goals the only way to score in basketball? No. So there’s some of your non-zero intercept. Think again about y = mx + b. We have our terms here: y is team score, m is 1.3 x is the team shooting percentage and b is 13.1. Let’s pretend for a minute that you coached a team that shot 40 percent in college basketball. Our model predicts you would score about 65 points. 2.3 Predicting based on the model Now, we can take the model predictions and bind them to our dataset. This will be a common step throughout this book so we can see what the model predicted vs what the real world produced. trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) Walking through this, we’re creating a dataframe called trainresults, which is game_train with the results of the predict function bound to it. The predict function takes two arguments – the fitted model and the dataset it is being applied to, which in this case is the same dataset. What will result is our game_train dataset with a new column: .pred Our first step in evaluating a linear model is to get the r-squared value. The yardstick library (part of Tidymodels) does this nicely. We tell it to produce metrics on a dataset, and we have to tell it what the real world result is (the truth column) and what the estimate column is (.pred). metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.8 ## 2 rsq standard 0.221 ## 3 mae standard 9.25 We’ll get more into RMSE and MAE later. For now, focus on rsq or r-squared. What that says is that changes in a four game rolling shooting percentage account for 22 percent of the variation in team score. That’s pretty good. Not great, but for one stat, it’s not bad. A way to look at this is with a scatterplot. The geom_smooth creates its own linear model and puts the line of best fit through our dots. ggplot() + geom_point(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score)) + geom_smooth(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 696 rows containing non-finite values (stat_smooth). ## Warning: Removed 696 rows containing missing values (geom_point). As you can see, there’s a lot of dots above the line and below the line. That gap is a called a residual. The residual is the actual thing minus the predicted thing. The truth minus our guess. A positive residual – in this case – is good. It means that player is scoring more than we’d predict they would. A negative residual means they’re not scoring as much as we’d expect. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% mutate(label = case_when( residual &gt; 0 ~ &quot;Positive&quot;, residual &lt; 0 ~ &quot;Negative&quot;) ) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=team_score, color=label)) + geom_smooth(aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 569 rows containing non-finite values (stat_smooth). ## Warning: Removed 569 rows containing missing values (geom_point). Residuals, aside from telling us who is and isn’t playing well, can tell us if a linear model is appropriate for this data. We can use a scatterplot to reveal this. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=residual)) ## Warning: Removed 569 rows containing missing values (geom_point). What we’re looking for is for the dots to be randomly spaced around the plot. It should look like someone spilled Skittles on the floor. This … does. It means a linear model is appropriate here. More on that in the coming chapters. 2.4 Predicting data we haven’t seen before Now we can do the same thing, but with the test data. testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) What do these metrics look like? metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.8 ## 2 rsq standard 0.218 ## 3 mae standard 9.22 If you look at the r-squared value, you’ll note that when we apply the same model to our test data, the amount of variance that we can explain goes down a little. It’s not much, so the model does a decent job of predicting data we haven’t seen before, which is the whole point of creating a model. 2.5 Looking locally We can get clearer picture of what these predictions look like if we look at something we know – like this season’s Nebraska team. What does the model say about how they are doing? First, we can get Nebraska’s games with a filter. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) Now apply the model to the games. nupreds &lt;- nu %&gt;% bind_cols(predict(fit_lm, nu)) To really see this clearly, we’ll calculate the residual, then sort by the residual. Where did the model miss the most, for good or bad? nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 22 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 74.8 25.2 ## 2 2021-12-23 Nebraska Kennesaw State 88 71.0 17.0 ## 3 2021-11-27 Nebraska South Dakota 83 68.1 14.9 ## 4 2022-01-03 Nebraska Ohio State 79 64.9 14.1 ## 5 2022-02-02 Nebraska Michigan 79 70.4 8.55 ## 6 2021-12-08 Nebraska Michigan 67 61.2 5.78 ## 7 2021-11-20 Nebraska Idaho State 78 74.0 4.00 ## 8 2021-11-10 Nebraska Western Illinois 74 70.6 3.42 ## 9 2021-11-17 Nebraska Creighton 69 66.0 2.97 ## 10 2021-11-21 Nebraska Southern 82 79.2 2.79 ## # … with 12 more rows What does this mean? It says the model predicted the team would score 75 against NC State and they put up 100, for a 25 point miss (residual). Might have had something to do with that game going to three overtimes, but alas, our model can’t get everything right. For most games, the prediction is within a few points, with some odd games with larger misses. Linear models are incredibly important to understand — they underpin many of the more advanced methods we’ll talk about going forward — so understanding them now is critical. "],["multiple-regression.html", "Chapter 3 Multiple regression 3.1 A multiple regression speed run 3.2 Picking what moves the needle", " Chapter 3 Multiple regression As we saw in the previous chapter, we can measure how much something can be predicted by another thing. We looked at how many points a team can score based on their shooting percentage. The theory being how well you shoot the ball probably has a lot to say about how many points you score. And what did we find? It’s a part of the story, but not the whole story. But that raises the problem with simple regressions – they’re simple. Anyone who has watched a basketball game knows there’s a lot more to the outcome than just shooting prowess. Enter the multiple regression. Multiple regressions are a step toward reality – where more than one thing influences the outcome. However, the more variance we attempt to explain, the more error and uncertainty we introduce into our model. Let’s begin by loading some libraries and installing a new one: corrr library(tidyverse) library(tidymodels) library(zoo) library(hoopR) library(corrr) For this, we’ll work with our college basketball game data and we’ll continue down the road we started in the last chapter. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion 3.1 A multiple regression speed run First, let’s restore what we did in last chapter with the feature engineering we did, making the different metrics and the rolling numbers. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=2, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, fill=true_shooting_percentage) ) %&gt;% ungroup() Now we’ll split our data into training and testing, creating a linear model predicting score from the shooting percentage and producing the metrics for the results. set.seed(1234) game_split &lt;- initial_split(teamstats, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.0 ## 2 rsq standard 0.320 ## 3 mae standard 8.64 Bottom line: We can predict about 32 percent of the difference in team scores by the shooting percentage. But we know, because we’ve shot hoops in the driveway before, or went through a basketball unit in PE in the third grade, that sure, being a good shooter is important, but how many times you shoot the ball is also important. If you’re a 100 percent shooter, that’s insane, but it probably means you took one shot. Congrats, you scored two points (three if you’re gutsy). One shot is not going to win a game. In our feature engineering, we created another metric – points per posesssion. It’s a measure of efficiency – did you score when you had the ball? We created a rolling metric for this too. To add it to our model, it’s as simple as just adding + rolling_ppp fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage + rolling_ppp, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.99 ## 2 rsq standard 0.443 ## 3 mae standard 7.78 And just like that, by acknowledging reality, we’ve jumped to 44 percent of variance explained and our root mean squared error – the average amount we’re off by – has dropped from 11 to 10. Your temptation now is to start adding things until we get to 1 on the r-squared and 0 on the rmse. The problem with that is called “overfitting” Overfitting is where you produce a model that is too close to your training data, which makes it prone to fail with data you’ve never seen before – the model becomes unreliable when it’s not the training data anymore. A secondary problem you encounter is this: the point of this is to predict future events. In this class, we’re attempting to predict the outcome of things that have not yet happened. That means we are going to be estimating the inputs to these models, inputs that will no doubt have error. So our inputs have a range of possible outcomes, our model is not perfect, so the outcome is going to combine the two. The more elements of your model that you use as inputs, the more error – uncertainty – you are introducing. The point is you want to pick the things that really matter and ignore the rest in some vain quest to get to 100 percent. You won’t get there. 3.2 Picking what moves the needle There are multiple ways to find the right combination of inputs to your models. With multiple regressions, the most common is the correlation matrix. We’re looking to maximize r-squared by choosing inputs that are highly correlated to our target value, but not correlated with other things. Example: We can assume that field_goals_made and field_goal_pct are highly correlated to team_score, but the number of Field Goals made is also highly correlated with the field goal percentage. Using corrr, we can create a correlation matrix in a dataframe to find columns that are highly correlated with our target – team_score. To do this, we need to select the columns we’re working with – our three rolling metrics. teamstats %&gt;% select(team_score, rolling_shooting_percentage, rolling_ppp, rolling_true_shooting_percentage) %&gt;% correlate() ## # A tibble: 4 × 5 ## term team_score rolling_shooting_pe… rolling_ppp rolling_true_shooti… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 team_score NA 0.566 0.665 0.597 ## 2 rolling_shoo… 0.566 NA 0.817 0.930 ## 3 rolling_ppp 0.665 0.817 NA 0.864 ## 4 rolling_true… 0.597 0.930 0.864 NA Reading this can be a lot, and it helps to take some notes as you go. You read up and down and left and right – it’s a matrix. Follow the rolling_true_shooting_percentage row across to the rolling_shooting_percentage column and you’ll see they’re almost perfectly correlated with each other – 1 is a perfect correlation. What does that mean? It means including both is going to just add error without adding much value. They’re so similar. You pick the one that is more highly correlated with team_score – true shooting. So how does this look in a model? Since we already have the features – the columns – in our data and we have it split into training and testing, we just need to create a new fit. new_fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_true_shooting_percentage + rolling_ppp, data = game_train) Let’s take a peek at our model coefficients. tidy(new_fit_lm, conf.int = TRUE) ## # A tibble: 3 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.355 0.327 -1.09 2.76e- 1 -0.995 0.285 ## 2 rolling_true_shootin… 0.179 0.0118 15.2 5.00e-52 0.156 0.202 ## 3 rolling_ppp 60.5 0.582 104. 0 59.4 61.7 What this says is if we can manage one point per possession in a basketball game, we’ll score 60 points. The model says for each 10 points of true shooting percentage, we’ll add another 1.8 points. Now we play the games. newtrainresults &lt;- game_train %&gt;% bind_cols(predict(new_fit_lm, game_train)) metrics(newtrainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.99 ## 2 rsq standard 0.443 ## 3 mae standard 7.78 So … we’re up to an r-squared of .44 and an rmse under 10. The goal here is to add to r-squared and reduce our error metrics. How well does the model do with data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.90 ## 2 rsq standard 0.447 ## 3 mae standard 7.73 It’s remarkably stable. Our rmse is all but unchanged as is our r-squared. Until we find other metrics or other methods, this is solid. Let’s put this against a set of games we’re familiar with. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(new_fit_lm, nu)) nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 22 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 71.9 28.1 ## 2 2022-02-02 Nebraska Michigan 79 66.6 12.4 ## 3 2022-01-03 Nebraska Ohio State 79 67.2 11.8 ## 4 2021-12-23 Nebraska Kennesaw State 88 76.6 11.4 ## 5 2021-12-08 Nebraska Michigan 67 56.6 10.4 ## 6 2021-11-10 Nebraska Western Illinois 74 65.7 8.26 ## 7 2021-11-13 Nebraska Sam Houston 74 68.9 5.11 ## 8 2021-11-27 Nebraska South Dakota 83 78.3 4.73 ## 9 2021-11-17 Nebraska Creighton 69 65.8 3.16 ## 10 2021-11-20 Nebraska Idaho State 78 75.1 2.93 ## # … with 12 more rows The question to start thinking about is this – what else could we include? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
