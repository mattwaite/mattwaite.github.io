[["index.html", "Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R Chapter 1 Introduction 1.1 Requirements and Conventions 1.2 About this book", " Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R By Matt Waite 2021-03-04 Chapter 1 Introduction The 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic. For the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner. How close? This is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved. The specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here. First, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good. Second, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good. Third, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away. Using logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t. So how much of a corner would the team need to turn? With modeling, we can figure this out. What would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent? As played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa). So how close are they to turning the corner? That close. 1.1 Requirements and Conventions This book is all in the R statistical language. To follow along, you’ll do the following: Install the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer. Install R Studio Desktop. The free version is great. Going forward, you’ll see passages like this: install.packages(&quot;tidyverse&quot;) Don’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do. 1.2 About this book This book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it: It is free for students. The topics will remain the same but the text is going to be constantly tinkered with. What is the work of the author is copyright Matt Waite 2021. The text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either. As such, the whole book – authored in Bookdown – is open sourced on Github. Pull requests welcomed! "],["the-modeling-process.html", "Chapter 2 The modeling process 2.1 Setting up the modeling process 2.2 Predicting based on the model 2.3 Predicting data we haven’t seen before 2.4 Looking locally", " Chapter 2 The modeling process The Nebraska men’s basketball team in 2019-2020 was … not good. The first season of Fred Hoiberg brought excitement and a lot of new faces to Lincoln, but the product on the floor didn’t go as hoped. The team finished 7-25. In the off-season, The Mayor turned over the roster (again), bringing on players who couldn’t play the previous season because of transfer rules and some new transfers who could play. As of this writing, the team has won 4 games and lost 7, and they’ve lost every Big Ten game they’ve played. But watch the team, and it’s obvious they are better. But – and this is a question we’ll explore over multiple chapters – how much better? Let’s start by looking at predicting how many points the team should score given how well they are shooting. And we’ll use this as a chance to look at linear regression modeling. If you don’t have them already installed, we’ll need the tidyverse and tidymodels for this book. install.packages(&quot;tidyverse&quot;) install.packages(&quot;tidymodels&quot;) After they’ve installed – and if you haven’t this will take a bit – load them. library(tidyverse) library(tidymodels) For this walkthrough, we’re going to use a dataset of college basketball games from the 14-15 season through the 20-21 season as of Jan. 8. For this walkthrough: Download csv file Let’s load this data and take a look at it. games &lt;- read_csv(&quot;data/cbblogs1521.csv&quot;) Any time we’re building models, we need to explore the data a bit. We can learn about our data before we try anything, which helps us learn more. A common first step? Histograms of the columns you’re looking at. A histogram groups data into bins and counts up the number of rows in that fall in the bin. Let’s first look at the histogram of points a team has scored (TeamScore). ggplot() + geom_histogram(data=games, aes(x=TeamScore)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. First, look at the highest bar. It’s in the upper 60s, with a line near the top around 70. So a lot of college basketball teams scored that area in a game. We’re also looking at shooting perentage, so let’s make a histogram for that. ggplot() + geom_histogram(data=games, aes(x=TeamFGPCT)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 4 rows containing non-finite values (stat_bin). First, note that 2 rows were removed for containing “non-finite values.” What does that mean? Two games don’t have shooting percentages. They got a score, but the data is missing. It’s not going to matter for modeling, so we should drop those games so they don’t cause us problems later. Second, most teams are shooting in the mid-40s per game for all shots from the floor (that’s both two point and three point shots combined). We can’t model nothing, so we need to drop the games with no stats. We can do that with a simple filter. games &lt;- games %&gt;% filter(TeamFGPCT&gt;0) Now we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar. Split your data into training and testing data sets. A common split is 80/20. Train the model on the training dataset. Evaluate the model on the training data. Apply the model to the testing data. Evaluate the model on the test data. From there, it’s how you want to use the model. We’ll walk through a simple example here, using the simplest model – a linear model. Linear models are something you’ve understood since you took middle school math and learned the equation of a line. Remember y = mx + b? It’s back. And, unlike what you complained bitterly in middle school, it’s very, very useful. What a linear model says, in words is that we can predict y if we multiply a value – a coefficient – by our x value offset with b, which is really the y-intercept, but think of it like where the line starts. What we’re trying to do here is predict how many points a team should score given their shooting prowess as a team. Or, expressed as y = mx + b: points = TeamFGPCT * ? + some starting point. Think of some starting point as what the score should be if the TeamFGPCT is zero. Should be zero, right? Intuitively, yes, but it won’t always work out so easily. 2.1 Setting up the modeling process With most modeling tasks we need to start with setting a random number seed to aid our random splitting of data into training and testing. set.seed(1234) Random numbers play a large role in a lot of data science algorithms, so setting one helps our reproducibility. After that, we split our data. There’s a number of ways to do this – R has a bunch and you’ll find all kinds of examples online – but Tidymodels has made this easy. game_split &lt;- initial_split(games, prop = .8) game_split ## &lt;Analysis/Assess/Total&gt; ## &lt;51254/12813/64067&gt; What does this mean? It says that initial_split divided the data into 48,108 games in analysis (or training), 12,026 into assess (or test), of the 60,134 total records in the dataset. But the split object isn’t useful to us. We need to assign them to dataframes. We do so like this: game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Now we have two dataframes – game_train and game_test – that we can now use for modeling. First step to making a model is to set what type of model this will be. We’re going to name our model object – lm_model works because this is a linear model. We’ll use the linear_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “lm.” lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) We can get a peek at lm_model and make sure we did everything right by just typing it and executing. lm_model ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Now, let’s fit a linear model to our data. We’ll name the fitted model fit_lm and we’ll take our model object that we just created and fit it using the fit function. What goes in the fit function can be read like this: TeamScore is approximately modeled by TeamFGPCT The only thing left is to specify the dataset. fit_lm &lt;- lm_model %&gt;% fit(TeamScore ~ TeamFGPCT, data = game_train) Let’s take a look at what the fitted model object tells us about our data. tidy(fit_lm, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 16.3 0.245 66.3 0 15.8 16.7 ## 2 TeamFGPCT 125. 0.550 228. 0 124. 127. The two most important things to see here are the terms and the estimates. Start with TeamFGPCT. What that says is for every 10 percentage points of shooting percentage, a team should score 12.6 points. HOWEVER, the intercept has something to say about this. What the intercept says is that a team with a big fat zero for shooting percentage is going to score just a hair short of 16 points. Wait … how? Well, are field goals the only way to score in basketball? No. So there’s some of your non-zero intercept. Think again about y = mx + b. We have our terms here: y is team score, m is 126.26, x is the team shooting percentage and b is 15.98997. Let’s pretend for a minute that you coached a team that shot 40 percent in college basketball. Our model predicts you would score about 66 points. But look at the confidence intervals. So our model says you’d score 66, but we’re 95 percent sure the real number is going to be between 65 and 67. 2.2 Predicting based on the model Now, we can take the model predictions and bind them to our dataset. This will be a common step throughout this book so we can see what the model predicted vs what the real world produced. trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) Walking through this, we’re creating a dataframe called trainresults, which is game_train with the results of the predict function bound to it. The predict function takes two arguments – the fitted model and the dataset it is being applied to, which in this case is the same dataset. What will result is our game_train dataset with a new column: .pred Our first step in evaluating a linear model is to get the r-squared value. The yardstick library (part of Tidymodels) does this nicely. We tell it to produce metrics on a dataset, and we have to tell it what the real world result is (the truth column) and what the estimate column is (.pred). metrics(trainresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.28 ## 2 rsq standard 0.503 ## 3 mae standard 7.23 We’ll get more into RMSE and MAE later. For now, focus on rsq or r-squared. What that says is that changes in shooting percentage account for 50.48 percent of the variation in team score. That’s pretty good. Not great, but for one stat, it’s not bad. A way to look at this is with a scatterplot. The geom_smooth creates its own linear model and puts the line of best fit through our dots. ggplot() + geom_point(data=games, aes(x=TeamFGPCT, y=TeamScore)) + geom_smooth(data=games, aes(x=TeamFGPCT, y=TeamScore), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; As you can see, there’s a lot of dots above the line and below the line. That gap is a called a residual. The residual is the actual thing minus the predicted thing. The truth minus our guess. A positive residual – in this case – is good. It means that player is scoring more than we’d predict they would. A negative residual means they’re not scoring as much as we’d expect. trainresults %&gt;% mutate(Residual = TeamScore - .pred) %&gt;% mutate(Label = case_when( Residual &gt; 0 ~ &quot;Positive&quot;, Residual &lt; 0 ~ &quot;Negative&quot;) ) %&gt;% ggplot() + geom_point(aes(x=TeamFGPCT, y=TeamScore, color=Label)) + geom_smooth(aes(x=TeamFGPCT, y=TeamScore), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; Residuals, aside from telling us who is and isn’t playing well, can tell us if a linear model is appropriate for this data. We can use a scatterplot to reveal this. trainresults %&gt;% mutate(Residual = TeamScore - .pred) %&gt;% ggplot() + geom_point(aes(x=TeamFGPCT, y=Residual)) What we’re looking for is for the dots to be randomly spaced around the plot. It should look like someone spilled Skittles on the floor. This … does. It means a linear model is appropriate here. More on that in the coming chapters. 2.3 Predicting data we haven’t seen before Now we can do the same thing, but with the test data. testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) What do these metrics look like? metrics(testresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.23 ## 2 rsq standard 0.500 ## 3 mae standard 7.20 If you look at the r-squared value, you’ll note that when we apply the same model to our test data, the amount of variance that we can explain goes down a little. It’s not much, so the model does a decent job of predicting data we haven’t seen before, which is the whole point of creating a model. 2.4 Looking locally We can get clearer picture of what these predictions look like if we look at something we know – like this season’s Nebraska team. What does the model say about how they are doing? First, we can get Nebraska’s games with a filter. nu &lt;- games %&gt;% filter(Season == &quot;2020-2021&quot;, Team == &quot;Nebraska&quot;) nu ## # A tibble: 24 x 48 ## Season Game Date TeamFull Opponent HomeAway W_L TeamScore ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-… 1 2020-11-25 Nebrask… McNeese… &lt;NA&gt; W 102 ## 2 2020-… 2 2020-11-26 Nebrask… Nevada &lt;NA&gt; L 66 ## 3 2020-… 3 2020-11-28 Nebrask… North D… &lt;NA&gt; W 79 ## 4 2020-… 4 2020-12-01 Nebrask… South D… &lt;NA&gt; W 76 ## 5 2020-… 5 2020-12-09 Nebrask… Georgia… &lt;NA&gt; L 64 ## 6 2020-… 6 2020-12-11 Nebrask… Creight… @ L 74 ## 7 2020-… 7 2020-12-17 Nebrask… Doane C… &lt;NA&gt; W 110 ## 8 2020-… 8 2020-12-22 Nebrask… Wiscons… @ L 53 ## 9 2020-… 9 2020-12-25 Nebrask… Michigan &lt;NA&gt; L 69 ## 10 2020-… 10 2020-12-30 Nebrask… Ohio St… @ L 54 ## # … with 14 more rows, and 40 more variables: OpponentScore &lt;dbl&gt;, ## # TeamFG &lt;dbl&gt;, TeamFGA &lt;dbl&gt;, TeamFGPCT &lt;dbl&gt;, Team3P &lt;dbl&gt;, Team3PA &lt;dbl&gt;, ## # Team3PPCT &lt;dbl&gt;, TeamFT &lt;dbl&gt;, TeamFTA &lt;dbl&gt;, TeamFTPCT &lt;dbl&gt;, ## # TeamOffRebounds &lt;dbl&gt;, TeamTotalRebounds &lt;dbl&gt;, TeamAssists &lt;dbl&gt;, ## # TeamSteals &lt;dbl&gt;, TeamBlocks &lt;dbl&gt;, TeamTurnovers &lt;dbl&gt;, ## # TeamPersonalFouls &lt;dbl&gt;, OpponentFG &lt;dbl&gt;, OpponentFGA &lt;dbl&gt;, ## # OpponentFGPCT &lt;dbl&gt;, Opponent3P &lt;dbl&gt;, Opponent3PA &lt;dbl&gt;, ## # Opponent3PPCT &lt;dbl&gt;, OpponentFT &lt;dbl&gt;, OpponentFTA &lt;dbl&gt;, ## # OpponentFTPCT &lt;dbl&gt;, OpponentOffRebounds &lt;dbl&gt;, ## # OpponentTotalRebounds &lt;dbl&gt;, OpponentAssists &lt;dbl&gt;, OpponentSteals &lt;dbl&gt;, ## # OpponentBlocks &lt;dbl&gt;, OpponentTurnovers &lt;dbl&gt;, OpponentPersonalFouls &lt;dbl&gt;, ## # URL &lt;chr&gt;, Conference &lt;chr&gt;, Team &lt;chr&gt;, TeamSRS &lt;dbl&gt;, TeamSOS &lt;dbl&gt;, ## # OpponentSRS &lt;dbl&gt;, OpponentSOS &lt;dbl&gt; Now apply the model to the games. nupreds &lt;- nu %&gt;% bind_cols(predict(fit_lm, nu)) To really see this clearly, we’ll calculate the residual, then sort by the residual. Where did the model miss the most, for good or bad? nupreds %&gt;% mutate(Residual = TeamScore - .pred) %&gt;% arrange(desc(Residual)) %&gt;% select(Team, Opponent, TeamScore, .pred, Residual) ## # A tibble: 24 x 5 ## Team Opponent TeamScore .pred Residual ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebraska Doane College 110 85.8 24.2 ## 2 Nebraska McNeese State 102 79.0 23.0 ## 3 Nebraska Nevada 66 52.6 13.4 ## 4 Nebraska South Dakota 76 66.4 9.55 ## 5 Nebraska Illinois 72 62.6 9.44 ## 6 Nebraska North Dakota State 79 74.0 5.02 ## 7 Nebraska Michigan 69 65.3 3.68 ## 8 Nebraska Creighton 74 71.3 2.66 ## 9 Nebraska Indiana 76 73.5 2.53 ## 10 Nebraska Ohio State 54 51.8 2.23 ## # … with 14 more rows What does this mean? It says the model predicted the team would score 86 against Doane and they put up 110, for a 24 point miss (residual). For most games, the prediction is within a few points – it nailed the Georgia Tech game - and we did worse against Wisconsin than the model would have guessed. We underperformed, in a manner of speaking. Linear models are incredibly important to understand — they underpin many of the more advanced methods we’ll talk about going forward — so understanding them now is critical. "],["multiple-regression-and-feature-engineering.html", "Chapter 3 Multiple regression and feature engineering 3.1 A multiple regression speed run 3.2 Picking what moves the needle 3.3 Feature engineering", " Chapter 3 Multiple regression and feature engineering As we saw in the previous chapter, we can measure how much something can be predicted by another thing. We looked at how many points a team can score based on their shooting percentage. The theory being how well you shoot the ball probably has a lot to say about how many points you score. And what did we find? It’s about half the story. But that raises the problem with simple regressions – they’re simple. Anyone who has watched a basketball game knows there’s a lot more to the outcome than just shooting prowess. Enter the multiple regression. Multiple regressions are a step toward reality – where more than one thing influences the outcome. However, the more variance we attempt to explain, the more error and uncertainty we introduce into our model. Let’s begin by loading some libraries and installing a new one: corrr We install it by going to the console and typing install.packages(\"corrr\") library(tidyverse) library(tidymodels) library(corrr) For this, we’ll work with our college basketball game data and we’ll continue down the road we started in the last chapter. For this walkthrough: Download csv file Let’s import the data. games &lt;- read_csv(&quot;data/cbblogs1521.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Season = col_character(), ## Date = col_date(format = &quot;&quot;), ## TeamFull = col_character(), ## Opponent = col_character(), ## HomeAway = col_character(), ## W_L = col_character(), ## URL = col_character(), ## Conference = col_character(), ## Team = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. 3.1 A multiple regression speed run First, let’s restore what we did in last chapter, spitting our data into training and testing, creating a linear model predicting score from the shooting percentage and producing the metrics for the results. set.seed(1234) game_split &lt;- initial_split(games, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) fit_lm &lt;- lm_model %&gt;% fit(TeamScore ~ TeamFGPCT, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.30 ## 2 rsq standard 0.504 ## 3 mae standard 7.24 Bottom line: We can predict about 50 percent of the difference in team scores by the shooting percentage. But we know, because we’ve shot hoops in the driveway before, or went through a basketball unit in PE in the third grade, that sure, being a good shooter is important, but how many times you shoot the ball is also important. If you’re a 100 percent shooter, that’s insane, but it probably means you took one shot. Congrats, you scored two points (three if you’re gutsy). One shot is not going to win a game. It would make sense, then, that we should combine the number of shots taken with the shooting percentage to predict the score. Doing that could not be easier. It’s literally adding + TeamFGA to your fit function. Now our model says TeamScore is approximately modeled by how well a team shoots the ball and how many shots they take. fit_lm &lt;- lm_model %&gt;% fit(TeamScore ~ TeamFGPCT + TeamFGA, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 6.44 ## 2 rsq standard 0.762 ## 3 mae standard 5.13 And just like that, by acknowledging reality, we’ve jumped to 76 percent of variance explained and our root mean squared error – the average amount we’re off by – has dropped from 9.3 to 6.5. Your temptation now is to start adding things until we get to 1 on the r-squared and 0 on the rmse. The problem with that is called “overfitting” Overfitting is where you produce a model that is too close to your training data, which makes it prone to fail with data you’ve never seen before – the model becomes unreliable when it’s not the training data anymore. A secondary problem you encounter is this: the point of this is to predict future events. In this class, we’re attempting to predict the outcome of things that have not yet happened. That means we are going to be estimating the inputs to these models, inputs that will no doubt have error. So our inputs have a range of possible outcomes, our model is not perfect, so the outcome is going to combine the two. The more elements of your model that you use as inputs, the more error – uncertainty – you are introducing. The point is you want to pick the things that really matter and ignore the rest in some vain quest to get to 100 percent. You won’t get there. 3.2 Picking what moves the needle There are multiple ways to find the right combination of inputs to your models. With multiple regressions, the most common is the correlation matrix. We’re looking to maximize r-squared by choosing inputs that are highly correlated to our target value, but not correlated with other things. Example: We can assume that TeamFG and TeamFGPCT are highly correlated to TeamScore, but the number of Field Goals made is also highly correlated with the field goal percentage. Using corrr, we can create a correlation matrix in a dataframe and use filter to find columns that are highly correlated with our target – team score. To do this, we need to drop all non-numeric data, then we need to dump one numeric variable that isn’t really a number - the game number of the season. To narrow in a bit, we’ll select just two columns to get a view of things, which we will have to expand upon soon. games %&gt;% select_if(is.numeric) %&gt;% select(-Game) %&gt;% correlate() %&gt;% filter(TeamScore &gt; .4) %&gt;% select(term, TeamScore) ## # A tibble: 7 x 2 ## term TeamScore ## &lt;chr&gt; &lt;dbl&gt; ## 1 TeamFG 0.874 ## 2 TeamFGA 0.435 ## 3 TeamFGPCT 0.709 ## 4 Team3P 0.511 ## 5 Team3PPCT 0.472 ## 6 TeamFT 0.415 ## 7 TeamAssists 0.637 By this, there’s seven things correlated with TeamScore at a greater than .4 level. However, this is half the story. The issue becomes how correlated are those things to each other. To see that, we need to fix our select: games %&gt;% select_if(is.numeric) %&gt;% select(-Game) %&gt;% correlate() %&gt;% filter(TeamScore &gt; .4) %&gt;% select(term, TeamScore, TeamFG, TeamFGA, TeamFGPCT, Team3P, Team3PPCT, TeamFT, TeamAssists) ## # A tibble: 7 x 9 ## term TeamScore TeamFG TeamFGA TeamFGPCT Team3P Team3PPCT TeamFT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Team… 0.874 NA 0.564 0.757 0.406 0.380 -0.0216 ## 2 Team… 0.435 0.564 NA -0.100 0.217 -0.0954 -0.134 ## 3 Team… 0.709 0.757 -0.100 NA 0.320 0.541 0.0806 ## 4 Team… 0.511 0.406 0.217 0.320 NA 0.711 -0.104 ## 5 Team… 0.472 0.380 -0.0954 0.541 0.711 NA 0.00780 ## 6 Team… 0.415 -0.0216 -0.134 0.0806 -0.104 0.00780 NA ## 7 Team… 0.637 0.661 0.290 0.561 0.525 0.433 -0.0193 ## # … with 1 more variable: TeamAssists &lt;dbl&gt; Reading this can be a lot, and it helps to take some notes as you go. TeamFG is the most correlated to TeamScore – which makes sense. Made shots are points. But they’re also highly correlated to attempts and even more to percentage. There’s others that it’s highly correlated with. Attempts are moderately associated with TeamScore, but notice they are NOT correlated with TeamFGPCT. What does that tell you? It says teams shoot the ball, regardless of how good they are at it, and the two aren’t related to each other BUT they are related to the points teams score. Perfect. 3.3 Feature engineering Feature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good. Let’s look at basketball games again. A number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score? One problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this: Possessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts) Here is that formula applied to our data, plus creating some new metrics of Points Per Possession for both Team and Opponent: gameswithpossessions &lt;- games %&gt;% mutate( TeamPossessions = TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA), OpponentPossessions = OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA), TeamPPP = TeamScore/TeamPossessions, OpponentPPP = OpponentScore/OpponentPossessions) Now lets look at the correlation matrix for our newly added data: gameswithpossessions %&gt;% select_if(is.numeric) %&gt;% select(-Game) %&gt;% correlate() %&gt;% filter(TeamScore &gt; .4) %&gt;% select(term, TeamScore) ## # A tibble: 10 x 2 ## term TeamScore ## &lt;chr&gt; &lt;dbl&gt; ## 1 TeamFG 0.874 ## 2 TeamFGA 0.435 ## 3 TeamFGPCT 0.709 ## 4 Team3P 0.511 ## 5 Team3PPCT 0.472 ## 6 TeamFT 0.415 ## 7 TeamAssists 0.637 ## 8 TeamPossessions 0.532 ## 9 OpponentPossessions 0.554 ## 10 TeamPPP 0.849 Note: our new possession metrics now make the grade here. But are they correlated with each other? gameswithpossessions %&gt;% select_if(is.numeric) %&gt;% select(-Game) %&gt;% correlate() %&gt;% filter(TeamScore &gt; .4) %&gt;% select(term, TeamScore, TeamFG, TeamFGA, TeamFGPCT, Team3P, Team3PPCT, TeamFT, TeamAssists, TeamPossessions, OpponentPossessions, TeamPPP) ## # A tibble: 10 x 12 ## term TeamScore TeamFG TeamFGA TeamFGPCT Team3P Team3PPCT TeamFT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Team… 0.874 NA 0.564 0.757 0.406 0.380 -0.0216 ## 2 Team… 0.435 0.564 NA -0.100 0.217 -0.0954 -0.134 ## 3 Team… 0.709 0.757 -0.100 NA 0.320 0.541 0.0806 ## 4 Team… 0.511 0.406 0.217 0.320 NA 0.711 -0.104 ## 5 Team… 0.472 0.380 -0.0954 0.541 0.711 NA 0.00780 ## 6 Team… 0.415 -0.0216 -0.134 0.0806 -0.104 0.00780 NA ## 7 Team… 0.637 0.661 0.290 0.561 0.525 0.433 -0.0193 ## 8 Team… 0.532 0.438 0.613 0.0492 0.142 -0.0243 0.335 ## 9 Oppo… 0.554 0.448 0.536 0.121 0.169 0.0300 0.352 ## 10 Team… 0.849 0.757 0.133 0.810 0.517 0.578 0.285 ## # … with 4 more variables: TeamAssists &lt;dbl&gt;, TeamPossessions &lt;dbl&gt;, ## # OpponentPossessions &lt;dbl&gt;, TeamPPP &lt;dbl&gt; Let’s look specifically at our TeamPPP and TeamPossessions metrics. Both are correlated to TeamScore – TeamPPP is highly correlated – but they AREN’T very correlated to each other. If you think about it, it makes some sense: one is a measure of how good the team is at getting the ball into their hands, the other is a measure of how efficient they are at scoring when they have the ball. Both metrics encompass a lot about a game – steals, rebounds, free throws, etc. – that basic shooting metrics don’t give you. So how does this look in a model? First, there’s a very small number of games with blanks for some of shooting stats, so we need to dump them first or we’ll get errors. gameswithpossessions &lt;- gameswithpossessions %&gt;% filter(TeamFGPCT&gt;0, OpponentFGPCT&gt;0) First we split our data into training and testing. newgame_split &lt;- initial_split(gameswithpossessions, prop = .8) newgame_train &lt;- training(newgame_split) newgame_test &lt;- testing(newgame_split) Create the model shell. lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) Create the fit. fit_lm &lt;- lm_model %&gt;% fit(TeamScore ~ TeamPossessions + TeamPPP, data = newgame_train) Let’s take a peek at our model coefficients. tidy(fit_lm, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -72.1 0.0585 -1232. 0 -72.2 -71.9 ## 2 TeamPossessions 1.02 0.000708 1443. 0 1.02 1.02 ## 3 TeamPPP 70.6 0.0303 2329. 0 70.5 70.6 What this says is if we have zero possessions in a basketball game, we’ll score -72 points. Well, we know neither are possible, so we ignore that. The model says for each possession, we should score about 1.02 points, and if we were to score 1 point per possession, we’d score 70.6 points. Now we play the games. trainresults &lt;- newgame_train %&gt;% bind_cols(predict(fit_lm, newgame_train)) metrics(trainresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.08 ## 2 rsq standard 0.993 ## 3 mae standard 0.655 So … knowing possession metrics makes you VERY good at predicting the total points a team will score. That’s frighteningly high — overfitting high. But we’ll continue to test it out. How well does the model do with data it hasn’t seen before? testresults &lt;- newgame_test %&gt;% bind_cols(predict(fit_lm, newgame_test)) metrics(testresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.08 ## 2 rsq standard 0.993 ## 3 mae standard 0.660 Again … somewhat terrifyingly well. Let’s put this against a set of games we’re familiar with. nu &lt;- gameswithpossessions %&gt;% filter(Season == &quot;2020-2021&quot;, TeamFull == &quot;Nebraska Cornhuskers&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(fit_lm, nu)) nupreds %&gt;% mutate(Residual = TeamScore - .pred) %&gt;% arrange(desc(Residual)) %&gt;% select(Team, Opponent, TeamScore, .pred, Residual) ## # A tibble: 24 x 5 ## Team Opponent TeamScore .pred Residual ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebraska Doane College 110 107. 3.48 ## 2 Nebraska McNeese State 102 99.5 2.46 ## 3 Nebraska Wisconsin 48 46.7 1.31 ## 4 Nebraska Wisconsin 53 52.0 0.972 ## 5 Nebraska Maryland 50 49.5 0.535 ## 6 Nebraska Penn State 83 82.8 0.226 ## 7 Nebraska North Dakota State 79 78.9 0.121 ## 8 Nebraska Minnesota 78 77.9 0.106 ## 9 Nebraska Michigan State 77 76.9 0.0704 ## 10 Nebraska Indiana 76 76.0 0.0411 ## # … with 14 more rows Our multiple regression model does the worst with blowouts, but it’s within a point on most games. The question to start thinking about is this – how well can you predict the number of possessions a team will have going into a game, and how many points they’ll score on each of those possessions? "],["decision-trees-and-random-forests.html", "Chapter 4 Decision trees and random forests 4.1 An intro to pre-processing 4.2 Decision trees 4.3 Random forest", " Chapter 4 Decision trees and random forests Tree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less seasons of experience? Do they have more or less minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have. The upside of decision trees is that if the model is small, you can explain it to anyone. They’re very easy to understand. The trouble with decision trees is that if the model is small, they’re a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree. The most common is the random forest. Let’s implement one. We start with libraries. library(tidyverse) library(tidymodels) We’ll be using college basketball games again. For this walkthrough: Download csv file Let’s load this data and add our possession metrics right away. games &lt;- read_csv(&quot;data/cbblogs1521.csv&quot;) %&gt;% filter(TeamFGPCT &gt; 0) %&gt;% mutate( TeamPossessions = TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA), TeamPPP = TeamScore/TeamPossessions ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Season = col_character(), ## Date = col_date(format = &quot;&quot;), ## TeamFull = col_character(), ## Opponent = col_character(), ## HomeAway = col_character(), ## W_L = col_character(), ## URL = col_character(), ## Conference = col_character(), ## Team = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. More often than not, we need to do more than just use the data we have. Often, with modeling, we need to pre-process our data. Pre-processing can mean a lot of things – fixing dates, creating new features, scaling numbers to be similar – but it’s all about making your models better. 4.1 An intro to pre-processing To simplify things, we’re going to first simplify our data. We want to start with a minimum of columns. We need the columns to help us identify individual records, we need our predictors and we need the outcome we’re trying to predict. modelgames &lt;- games %&gt;% select(Team, Opponent, Date, Season, TeamScore, TeamPossessions, TeamPPP) Now we need to split our data into training and testing sets. set.seed(1234) game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) In simple cases, this might be enough. If the data we’re looking at is on the scale, it probably would be enough. But here we have possessions – a counting stat – and points per possession, a ratio. Going forward, we’re going to make our lives easier by using workflows. Workflows in tidymodels take in a pre-processing recipe and a model definition and executes those things to make our modeling code slimmer and our lives easier. To start, we need to define a pre-processsing recipe. The recipe defines a series of steps that will be performed on your data. We’ll start simple and add our formula from previous work. score_rec &lt;- recipe(TeamScore ~ TeamPossessions + TeamPPP, data = game_train) Another, more flexible way to express this, is using the . to say all predictors. In this case, all predictors is TeamPossessions and TeamPPP. What follows is the same as above, just less typing. But we’re also going to add a role to our recipe. In this case, the role is how we’re going to identify each row – an ID. In this case, to identify a game, we need to know the Team, the Opponent, the Date and the Season. score_rec &lt;- recipe(TeamScore ~ ., data = game_train) %&gt;% update_role(Team, Opponent, Date, Season, new_role = &quot;ID&quot;) Now, we’re going to use steps to transform our data. Some models are affected by data being on different scales, so we might have to normalize them. Here’s how to do that: score_rec &lt;- recipe(TeamScore ~ ., data = game_train) %&gt;% update_role(Team, Opponent, Date, Season, new_role = &quot;ID&quot;) Now that we’ve created our pre-processing recipe, we can create our model definition. 4.2 Decision trees As discussed earlier, decision trees are essentially a series of if/else statements. Visualized, they look like branches on a tree (thus, decision trees). We’ve already defined a recipe for our data, so now we’re ready to define a model definition. First, we’ll use decision trees to prove a point. tree &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;regression&quot;) Now we’ll create the workflow. In its simplest form, the workflow defines itself as a workflow and then adds a recipe and a model definition. tree_wf &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(tree) Now we can fit the data with our model using the workflow. This applies our recipe to the data without us having to do it, then uses the model definition to do the fitting. tree_fit &lt;- tree_wf %&gt;% fit(data = game_train) What does this produce? Here’s what a basic decision tree looks like. tree_fit %&gt;% pull_workflow_fit() ## parsnip model object ## ## Fit time: 147ms ## n= 51254 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 51254 8894038.00 71.32136 ## 2) TeamPPP&lt; 0.9996455 24589 1936661.00 62.09203 ## 4) TeamPPP&lt; 0.8667459 9442 467747.90 55.08494 ## 8) TeamPPP&lt; 0.758596 2750 96392.41 48.67382 * ## 9) TeamPPP&gt;=0.758596 6692 211874.50 57.71952 ## 18) TeamPossessions&lt; 71.1375 3819 50554.05 54.16968 * ## 19) TeamPossessions&gt;=71.1375 2873 49225.28 62.43822 * ## 5) TeamPPP&gt;=0.8667459 15147 716330.50 66.45996 ## 10) TeamPossessions&lt; 72.0375 9076 170270.50 62.41836 * ## 11) TeamPossessions&gt;=72.0375 6071 176173.70 72.50206 * ## 3) TeamPPP&gt;=0.9996455 26665 2931439.00 79.83214 ## 6) TeamPossessions&lt; 72.7125 17175 1027871.00 75.35470 ## 12) TeamPPP&lt; 1.142428 10801 267742.70 71.40117 ## 24) TeamPossessions&lt; 66.1625 4151 62736.49 67.08432 * ## 25) TeamPossessions&gt;=66.1625 6650 79365.98 74.09579 * ## 13) TeamPPP&gt;=1.142428 6374 305224.30 82.05413 ## 26) TeamPossessions&lt; 66.4875 2753 92280.17 77.50527 * ## 27) TeamPossessions&gt;=66.4875 3621 112668.70 85.51257 * ## 7) TeamPossessions&gt;=72.7125 9490 936111.40 87.93541 ## 14) TeamPPP&lt; 1.163417 6629 252017.30 83.51727 ## 28) TeamPossessions&lt; 80.3125 5124 84955.94 81.29801 * ## 29) TeamPossessions&gt;=80.3125 1505 55903.96 91.07309 * ## 15) TeamPPP&gt;=1.163417 2861 254880.00 98.17232 ## 30) TeamPPP&lt; 1.293652 2156 90844.47 94.83952 * ## 31) TeamPPP&gt;=1.293652 705 66851.31 108.36450 * They can be a bit tough to read, but take the bottom three nodes, starting with 15. It says if TeamPPP is greater than or equal to 1.01 – you’re scoring more than a point per possession – your score is around 98 points a game. But the terminal nodes – the actual decisions – within this branch – say if it’s less than 1.97 per possession, your score is 95.83, but if it’s greater or equal to 1.97 – and there’s 511 games where teams were scoring nearly a bucket every trip up the floor – their score is 110.47. Easy to understand, right? The algorithm cuts branches when the splits stop reducing error, and there’s a limit But here’s where the crude instrument comes in. Let’s use our decision tree to predict some scores. treeresults &lt;- game_train %&gt;% bind_cols(predict(tree_fit, game_train)) What are the accuracy metrics we get? metrics(treeresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 4.81 ## 2 rsq standard 0.866 ## 3 mae standard 3.69 Our rsquared is .8668, which would under most circumstances would be great. And our RSME says were off by almost five points on average, which is a little worse than our multiple regression analysis. We can do better. 4.3 Random forest Enter the random forest. A random forest is, as the name implies, a large number of decision trees, and they use a random set of inputs. The trees all make predictions, and the wisdom of the crowds takes over. In the case of classification algorithm, the most common prediction is the one that gets chosen. In a regression model, the predictions get averaged together. The random part of random forest is in how the number of tree splits get created and how the samples from the data are taken to generate the splits. They’re randomized, which has the effect of limiting the influence of a particular feature and prevents overfitting. For random forests, we change the model type to rand_forest and set the engine to “ranger.” There’s multiple implementations of the random forest algorithm, and the differences between them are beyond the scope of what we’re doing here. rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) And now we can create our workflow. We first need to define it as a workflow, then add the model and add the recipe. score_wflow &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(rf_mod) score_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Random Forest Model Specification (regression) ## ## Computational engine: ranger With the workflow in place, we can fit our model. Note: this can make your laptop fan go wheeeeee. score_fit &lt;- score_wflow %&gt;% fit(data = game_train) Now we can use a use a new function – pull_workflow_fit, which pulls the fit stats we want to see to evaluate it. score_fit %&gt;% pull_workflow_fit() ## parsnip model object ## ## Fit time: 16s ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 51254 ## Number of independent variables: 2 ## Mtry: 1 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 0.1402611 ## R squared (OOB): 0.9991917 Similar to previous work, we can bind the prediction to our training data and evaluate the model. trainresults &lt;- game_train %&gt;% bind_cols(predict(score_fit, game_train)) metrics(trainresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.192 ## 2 rsq standard 1.00 ## 3 mae standard 0.0518 Note: The RMSE for this model is down to .2. The R-squared is absurdly high. But how does this model handle data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(score_fit, game_test)) metrics(testresults, truth = TeamScore, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.343 ## 2 rsq standard 0.999 ## 3 mae standard 0.0786 How well does the random forest algorithm do with Nebraska’s schedule in 2020-2021 prior to a month-long COVID break? nu &lt;- games %&gt;% filter(Season == &quot;2020-2021&quot;, Team == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(score_fit, nu)) nupreds %&gt;% mutate(Residual = TeamScore - .pred) %&gt;% arrange(desc(Residual)) %&gt;% select(Team, Opponent, TeamScore, .pred, Residual) ## # A tibble: 24 x 5 ## Team Opponent TeamScore .pred Residual ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebraska Doane College 110 110. 0.210 ## 2 Nebraska Illinois 70 69.9 0.119 ## 3 Nebraska Ohio State 54 53.9 0.107 ## 4 Nebraska Maryland 50 49.9 0.0671 ## 5 Nebraska Penn State 83 82.9 0.0558 ## 6 Nebraska Minnesota 78 78.0 0.0133 ## 7 Nebraska Michigan 69 69.0 0.00554 ## 8 Nebraska Creighton 74 74.0 0.00550 ## 9 Nebraska Wisconsin 53 53.0 0.00496 ## 10 Nebraska Georgia Tech 64 64.0 0.00410 ## # … with 14 more rows Compare this to the multiple regression of the previous chapter. Take just the Doane game as an example. The multiple regression model was off by three points – which is really good when you think about it. But the random forest managed to be a tenth of that off. It’s off by just a third of a point. It can’t get much better than that. "],["xgboost.html", "Chapter 5 XGBoost 5.1 Hyperparameters", " Chapter 5 XGBoost As we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular. Boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on. The path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost, are very popular for solving a wide variety of problems. Let’s re-implement our predictions for possessions in an XGBoost algorithm. First, we’ll load libraries and we’re going to introduce a new one here – doParallel – which handles using more of your computer’s processor cores to accomplish tasks in parallel instead of one core at a time. In other words, instead of one task, it can do X at a time in parallel and put the answers together after. library(tidyverse) library(tidymodels) library(zoo) ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric set.seed(1234) require(doParallel) ## Loading required package: doParallel ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loading required package: iterators ## Loading required package: parallel cores &lt;- parallel::detectCores(logical = FALSE) We’ll load our game data. games &lt;- read_csv(&quot;~/Documents/Books/AdvancedSportsDataAnalysis/data/cbblogs1521.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Season = col_character(), ## Date = col_date(format = &quot;&quot;), ## TeamFull = col_character(), ## Opponent = col_character(), ## HomeAway = col_character(), ## W_L = col_character(), ## URL = col_character(), ## Conference = col_character(), ## Team = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. Now we’ll do a spot of feature engineering. I’m going to combine a rolling mean and a cumulative mean with the opponent’s rating to try and predict possessions. rolling &lt;- games %&gt;% mutate( Possessions = .5*(TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA)) + .5*(OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA)), TeamPPP = TeamScore/Possessions) %&gt;% group_by(Team, Season) %&gt;% mutate( Rolling_Mean_Possessions = rollmean(lag(Possessions,n=1), k = 2, fill=Possessions), Rolling_Mean_Turnovers = rollmean(lag(TeamTurnovers,n=1), k = 2, fill=TeamTurnovers), Rolling_Mean_FGA = rollmean(lag(TeamFGA,n=1), k = 2, fill=TeamFGA), Cumulative_Mean_Possessions = cummean(Possessions), ) %&gt;% ungroup() %&gt;% select(Team, Opponent, Date, Season, Possessions, Rolling_Mean_Possessions, Cumulative_Mean_Possessions, Rolling_Mean_Turnovers, Rolling_Mean_FGA, OpponentSRS) %&gt;% na.omit() Per usual, we split our data into training and testing. rolling_split &lt;- initial_split(rolling, prop = .8) rolling_train &lt;- training(rolling_split) rolling_test &lt;- testing(rolling_split) And our recipe. rolling_rec &lt;- recipe(Possessions ~ ., data = rolling_train) %&gt;% update_role(Team, Opponent, Date, Season, new_role = &quot;ID&quot;) summary(rolling_rec) ## # A tibble: 10 x 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Team nominal ID original ## 2 Opponent nominal ID original ## 3 Date date ID original ## 4 Season nominal ID original ## 5 Rolling_Mean_Possessions numeric predictor original ## 6 Cumulative_Mean_Possessions numeric predictor original ## 7 Rolling_Mean_Turnovers numeric predictor original ## 8 Rolling_Mean_FGA numeric predictor original ## 9 OpponentSRS numeric predictor original ## 10 Possessions numeric outcome original To this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to. 5.1 Hyperparameters The hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters: Number of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem. Learn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point. Tree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting. Minimum number of observations in the terminal node – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting). Other settings: Loss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative. Sample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfittting. mtry – the number of predictors that will be randomly sampled at each split when making trees. All of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning. First, we make a mode and label each parameter as tune() xg_mod &lt;- boost_tree( trees = tune(), learn_rate = tune(), tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), ) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;xgboost&quot;, nthread = cores) Let’s make a workflow now that we have our recipe and our model. rolling_wflow &lt;- workflow() %&gt;% add_model(xg_mod) %&gt;% add_recipe(rolling_rec) Now, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. What follows is what parameters the hypercube will tune. xgb_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), rolling_train), learn_rate(), size = 30 ) xgb_grid ## # A tibble: 30 x 7 ## trees tree_depth min_n loss_reduction sample_size mtry learn_rate ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1909 13 35 0.000000609 0.787 2 0.00000000205 ## 2 477 9 21 0.0425 0.432 4 0.000944 ## 3 1508 12 10 0.00000000751 0.124 5 0.00000173 ## 4 1629 3 33 0.0000435 0.702 10 0.0449 ## 5 458 12 26 0.0169 0.488 9 0.0000000597 ## 6 612 2 28 0.0000000157 0.615 2 0.0000350 ## 7 1986 14 22 0.000000000103 0.586 4 0.0000533 ## 8 378 15 38 3.86 0.352 6 0.000637 ## 9 149 3 26 0.0000000322 0.660 8 0.000115 ## 10 1134 4 9 0.000000142 0.874 9 0.00733 ## # … with 20 more rows How do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result. rolling_folds &lt;- vfold_cv(rolling_train) rolling_folds ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [33.3K/3.7K]&gt; Fold01 ## 2 &lt;split [33.3K/3.7K]&gt; Fold02 ## 3 &lt;split [33.3K/3.7K]&gt; Fold03 ## 4 &lt;split [33.3K/3.7K]&gt; Fold04 ## 5 &lt;split [33.3K/3.7K]&gt; Fold05 ## 6 &lt;split [33.3K/3.7K]&gt; Fold06 ## 7 &lt;split [33.3K/3.7K]&gt; Fold07 ## 8 &lt;split [33.3K/3.7K]&gt; Fold08 ## 9 &lt;split [33.3K/3.7K]&gt; Fold09 ## 10 &lt;split [33.3K/3.7K]&gt; Fold10 This part takes about 25-30 minutes on my machine and it will saturate all of your processors, so your computer just needs to sit there. No texting, no YouTube, nothing. Let it burn. set.seed(234) doParallel::registerDoParallel(cores = cores) xgb_res &lt;- tune_grid( rolling_wflow, resamples = rolling_folds, grid = xgb_grid, control = control_grid(save_pred = TRUE) ) ## ## Attaching package: &#39;rlang&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int, ## flatten_lgl, flatten_raw, invoke, list_along, modify, prepend, ## splice ## ## Attaching package: &#39;vctrs&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## data_frame ## The following object is masked from &#39;package:tibble&#39;: ## ## data_frame ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice doParallel::stopImplicitCluster() xgb_res ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [33.3K/3.… Fold01 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,120 × … ## 2 &lt;split [33.3K/3.… Fold02 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,120 × … ## 3 &lt;split [33.3K/3.… Fold03 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,120 × … ## 4 &lt;split [33.3K/3.… Fold04 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 5 &lt;split [33.3K/3.… Fold05 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 6 &lt;split [33.3K/3.… Fold06 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 7 &lt;split [33.3K/3.… Fold07 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 8 &lt;split [33.3K/3.… Fold08 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 9 &lt;split [33.3K/3.… Fold09 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … ## 10 &lt;split [33.3K/3.… Fold10 &lt;tibble [60 × 1… &lt;tibble [0 × … &lt;tibble [111,090 × … So our grid has run on all of our validation samples, and what do we see? collect_metrics(xgb_res) ## # A tibble: 60 x 13 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 1367 4 1 9.37e-9 7.87 0.980 rmse ## 2 3 1367 4 1 9.37e-9 7.87 0.980 rsq ## 3 2 612 28 2 3.50e-5 0.0000000157 0.615 rmse ## 4 2 612 28 2 3.50e-5 0.0000000157 0.615 rsq ## 5 7 1220 30 2 1.53e-7 0.533 0.576 rmse ## 6 7 1220 30 2 1.53e-7 0.533 0.576 rsq ## 7 8 149 26 3 1.15e-4 0.0000000322 0.660 rmse ## 8 8 149 26 3 1.15e-4 0.0000000322 0.660 rsq ## 9 10 1629 33 3 4.49e-2 0.0000435 0.702 rmse ## 10 10 1629 33 3 4.49e-2 0.0000435 0.702 rsq ## # … with 50 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, ## # n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt; Well we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination. show_best(xgb_res, &quot;rmse&quot;) ## # A tibble: 5 x 13 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 1134 9 4 0.00733 0.000000142 0.874 rmse ## 2 7 1687 18 9 0.0143 0.000848 0.909 rmse ## 3 10 1629 33 3 0.0449 0.0000435 0.702 rmse ## 4 9 238 19 6 0.0739 1.03 0.415 rmse ## 5 8 673 24 10 0.00188 0.0000794 0.677 rmse ## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt; The best combination comes up with an RMSE of 4.1708. Second is 4.1769, so very, very close. Let’s capture our best set of hyperparameters. best_rmse &lt;- select_best(xgb_res, &quot;rmse&quot;) And now put that into a final workflow. Pay attention to the main arguments in the output below. final_xgb &lt;- finalize_workflow( rolling_wflow, best_rmse ) final_xgb ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = 9 ## trees = 1134 ## min_n = 9 ## tree_depth = 4 ## learn_rate = 0.00733155081360964 ## loss_reduction = 1.41691119958635e-07 ## sample_size = 0.873517132787965 ## ## Engine-Specific Arguments: ## nthread = cores ## ## Computational engine: xgboost There’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along. We create a fit. xg_fit &lt;- final_xgb %&gt;% fit(data = rolling_train) We can see something things about that fit, including all the iterations of our XGBoost model. Note: our tuned number of trees is 1,665 – and in the workflow fit, you can see 1,665 iterations. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The RMSE goes down with each iteration as we go down the gradient desent. xg_fit %&gt;% pull_workflow_fit() ## parsnip model object ## ## Fit time: 17.1s ## ##### xgb.Booster ## raw: 1.1 Mb ## call: ## xgboost::xgb.train(params = list(eta = 0.00733155081360964, max_depth = 4L, ## gamma = 1.41691119958635e-07, colsample_bytree = 1, min_child_weight = 9L, ## subsample = 0.873517132787965), data = x$data, nrounds = 1134L, ## watchlist = x$watchlist, verbose = 0, objective = &quot;reg:squarederror&quot;, ## nthread = 6L) ## params (as set within xgb.train): ## eta = &quot;0.00733155081360964&quot;, max_depth = &quot;4&quot;, gamma = &quot;1.41691119958635e-07&quot;, colsample_bytree = &quot;1&quot;, min_child_weight = &quot;9&quot;, subsample = &quot;0.873517132787965&quot;, objective = &quot;reg:squarederror&quot;, nthread = &quot;6&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 5 ## niter: 1134 ## nfeatures : 5 ## evaluation_log: ## iter training_rmse ## 1 69.682907 ## 2 69.173973 ## --- ## 1133 3.975097 ## 1134 3.975043 Now, like before, we can bind our predictions using our xg_fit to the rolling_train data. trainresults &lt;- rolling_train %&gt;% bind_cols(predict(xg_fit, rolling_train)) And now see how we did. metrics(trainresults, truth = Possessions, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3.98 ## 2 rsq standard 0.614 ## 3 mae standard 3.05 How about the test data? testresults &lt;- rolling_test %&gt;% bind_cols(predict(xg_fit, rolling_test)) metrics(testresults, truth = Possessions, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 4.09 ## 2 rsq standard 0.602 ## 3 mae standard 3.12 More robust than the random forest, but still a fair drop in RMSE and R squared. Let’s try one where we know the outcome. In Febuary, Nebraska played Maryland back to back and lost both. Here’s the inputs for the second game: poss_prediction &lt;- tibble( Team=&quot;Nebraska&quot;, Opponent=&quot;Maryland&quot;, Date = as.Date(&quot;2021-02-16&quot;), Season = &quot;2020-2021&quot;, Rolling_Mean_Possessions = 72.5000, Cumulative_Mean_Possessions = 74.54211, Rolling_Mean_Turnovers = 10, Rolling_Mean_FGA = 63.0, OpponentSRS = 15.41 ) %&gt;% add_row( Team=&quot;Maryland&quot;, Opponent=&quot;Nebraska&quot;, Date = as.Date(&quot;2021-02-16&quot;), Season = &quot;2020-2021&quot;, Rolling_Mean_Possessions = 61.7500, Cumulative_Mean_Possessions = 67.14239, Rolling_Mean_Turnovers = 10, Rolling_Mean_FGA = 52.0, OpponentSRS = 5.91 ) If I take that tibble and bind my predictions to it, I come up with the following: poss_prediction %&gt;% bind_cols(predict(xg_fit, poss_prediction)) ## # A tibble: 2 x 10 ## Team Opponent Date Season Rolling_Mean_Po… Cumulative_Mean… ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebr… Maryland 2021-02-16 2020-… 72.5 74.5 ## 2 Mary… Nebraska 2021-02-16 2020-… 61.8 67.1 ## # … with 4 more variables: Rolling_Mean_Turnovers &lt;dbl&gt;, ## # Rolling_Mean_FGA &lt;dbl&gt;, OpponentSRS &lt;dbl&gt;, .pred &lt;dbl&gt; Prediction: Nebraska would have 71.8 possessions, Maryland would have 62.18. Add together and divide: 66.99. Reality: Reality: 66.2 Not bad at all. "],["logistic-regression.html", "Chapter 6 Logistic Regression 6.1 Visualizing the decision boundary 6.2 The logistic regression 6.3 Evaluating the fit 6.4 Comparing it to test data 6.5 How well did it do with Nebraska?", " Chapter 6 Logistic Regression Up to this point, we’ve been dealing with problems that lead to a quantitative answer: We want a number. How many points? How many possessions? But there are lots of problems in the world where the answer is qualitative: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not? These are problems of classification and they use many of the same algorithms we’ve used to try and predict those classes. Ultimately, the algorithms will predict the probability that this row is X or Y and make a decision based on that probability. That probability will be somewhere between 0 and 1, with 0 being no chance and 1 being a sure thing. Where this gets interesting is in the middle. library(tidyverse) library(tidymodels) set.seed(1234) What we’re going to do here is calculate possessions in the game, the team’s Points Per Possession and then some cumulative means for those two things. Then we’ll do a little cleanup on it being a home or an away game and we’ll clean up the win/loss column to be simple W or L. games &lt;- read_csv(&quot;data/cbblogs1521.csv&quot;) %&gt;% mutate( Possessions = .5*(TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA)) + .5*(OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA)), PPP = TeamScore/Possessions) %&gt;% group_by(Team, Season) %&gt;% mutate( Cumulative_Mean_Possessions = cummean(Possessions), Cumulative_Mean_PPP = cummean(PPP) ) %&gt;% ungroup() %&gt;% mutate( Location = case_when( str_trim(HomeAway) == &quot;@&quot; ~ &quot;A&quot;, str_trim(HomeAway) == &quot;N&quot; ~ &quot;N&quot;, TRUE ~ &quot;H&quot; ), Outcome = case_when( grepl(&quot;W&quot;, W_L) ~ &quot;W&quot;, grepl(&quot;L&quot;, W_L) ~ &quot;L&quot; ) ) %&gt;% mutate(Outcome = as.factor(Outcome)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Season = col_character(), ## Date = col_date(format = &quot;&quot;), ## TeamFull = col_character(), ## Opponent = col_character(), ## HomeAway = col_character(), ## W_L = col_character(), ## URL = col_character(), ## Conference = col_character(), ## Team = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. Now we’ll use select to get us the fields we need. selectedgames &lt;- games %&gt;% select(Season, Team, Date, Opponent, Location, Outcome, Cumulative_Mean_Possessions, Cumulative_Mean_PPP, TeamSRS) Now what we’ll do is create the reverse side of the game by just renaming things to Opponent_X. That way, we can join them together and create the whole picture of the game – team and opponent. We’ll lose some games, but those will be against Division II opponents and we won’t care. opponentgames &lt;- selectedgames %&gt;% rename(Opponent_Cumulative_Mean_Possessions = Cumulative_Mean_Possessions, Opponent_Cumulative_Mean_PPP = Cumulative_Mean_PPP, OpponentSRS = TeamSRS) Now the join. This works because the Team field in opponent games is the Opponent field in selectedgames. The date of the game and the seson are identical in both sides of the game, so it ensures we’re combining data for the same, correct game. bothsides &lt;- selectedgames %&gt;% left_join(opponentgames, by=c(&quot;Team&quot; = &quot;Opponent&quot;, &quot;Date&quot;, &quot;Season&quot;)) %&gt;% na.omit() %&gt;% select(-Team.y, -Location.y, -Outcome.y) %&gt;% rename(Location = Location.x, Outcome = Outcome.x) 6.1 Visualizing the decision boundary This is just one dimension of the data, but it can illustrate how this works. You can see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. That neatly captures the probabilities we’re looking at here. ggplot() + geom_point(data=bothsides, aes(x=Cumulative_Mean_PPP, y=Opponent_Cumulative_Mean_PPP, color=Outcome)) 6.2 The logistic regression Much of implementing classification algorithms should look familiar by now. The steps we’ve been using are steps we will use again. First, we split into training and testing. log_split &lt;- initial_split(bothsides, prop = .8) log_train &lt;- training(log_split) log_test &lt;- testing(log_split) We create a recipe. In this case, we need to create dummy values for home and away games. Our recipe will put a 1 for home and a 0 for away, for example. We’re also going to normalize our predictors so scale differences don’t create undue influences. log_recipe &lt;- recipe(Outcome ~ ., data = log_train) %&gt;% update_role(Team, Opponent, Date, Season, new_role = &quot;ID&quot;) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_normalize(all_predictors()) summary(log_recipe) ## # A tibble: 12 x 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Season nominal ID original ## 2 Team nominal ID original ## 3 Date date ID original ## 4 Opponent nominal ID original ## 5 Location nominal predictor original ## 6 Cumulative_Mean_Possessions numeric predictor original ## 7 Cumulative_Mean_PPP numeric predictor original ## 8 TeamSRS numeric predictor original ## 9 Opponent_Cumulative_Mean_Possessions numeric predictor original ## 10 Opponent_Cumulative_Mean_PPP numeric predictor original ## 11 OpponentSRS numeric predictor original ## 12 Outcome nominal outcome original Now we define the model. Note the set_mode. log_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) Now we have enough for a workflow. log_workflow &lt;- workflow() %&gt;% add_model(log_mod) %&gt;% add_recipe(log_recipe) And now we fit our model (this can take a few minutes). log_fit &lt;- log_workflow %&gt;% fit(data = log_train) 6.3 Evaluating the fit With logisitic regression, there’s two things we’re looking at: The prediction and the probabilities. We can get those with two different fits and combine them together. trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train) %&gt;% bind_cols(log_train) trainpredict ## # A tibble: 38,058 x 13 ## .pred_class Season Team Date Opponent Location Outcome ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 L 2014-… Abil… 2014-11-19 Tulsa A L ## 2 L 2014-… Abil… 2014-11-22 Duquesne H L ## 3 L 2014-… Abil… 2014-11-29 UC-Rive… A L ## 4 L 2014-… Abil… 2014-12-04 Sacrame… H W ## 5 L 2014-… Abil… 2014-12-06 Houston A L ## 6 L 2014-… Abil… 2014-12-17 Loyola … A L ## 7 L 2014-… Abil… 2014-12-23 Arkansa… N W ## 8 L 2014-… Abil… 2014-12-28 Grand C… A L ## 9 W 2014-… Abil… 2015-01-04 Central… H W ## 10 W 2014-… Abil… 2015-01-10 Nicholl… H W ## # … with 38,048 more rows, and 6 more variables: ## # Cumulative_Mean_Possessions &lt;dbl&gt;, Cumulative_Mean_PPP &lt;dbl&gt;, ## # TeamSRS &lt;dbl&gt;, Opponent_Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_PPP &lt;dbl&gt;, OpponentSRS &lt;dbl&gt; trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train, type=&quot;prob&quot;) %&gt;% bind_cols(trainpredict) trainpredict ## # A tibble: 38,058 x 15 ## .pred_L .pred_W .pred_class Season Team Date Opponent Location Outcome ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 0.981 0.0187 L 2014-… Abil… 2014-11-19 Tulsa A L ## 2 0.954 0.0463 L 2014-… Abil… 2014-11-22 Duquesne H L ## 3 0.918 0.0817 L 2014-… Abil… 2014-11-29 UC-Rive… A L ## 4 0.818 0.182 L 2014-… Abil… 2014-12-04 Sacrame… H W ## 5 0.839 0.161 L 2014-… Abil… 2014-12-06 Houston A L ## 6 0.969 0.0310 L 2014-… Abil… 2014-12-17 Loyola … A L ## 7 0.553 0.447 L 2014-… Abil… 2014-12-23 Arkansa… N W ## 8 0.899 0.101 L 2014-… Abil… 2014-12-28 Grand C… A L ## 9 0.132 0.868 W 2014-… Abil… 2015-01-04 Central… H W ## 10 0.365 0.635 W 2014-… Abil… 2015-01-10 Nicholl… H W ## # … with 38,048 more rows, and 6 more variables: ## # Cumulative_Mean_Possessions &lt;dbl&gt;, Cumulative_Mean_PPP &lt;dbl&gt;, ## # TeamSRS &lt;dbl&gt;, Opponent_Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_PPP &lt;dbl&gt;, OpponentSRS &lt;dbl&gt; There’s several metrics to look at, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. metrics(trainpredict, Outcome, .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.761 ## 2 kap binary 0.522 Another way to look at that is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy. trainpredict %&gt;% conf_mat(Outcome, .pred_class) ## Truth ## Prediction L W ## L 14517 4567 ## W 4530 14444 The roc_auc metric is largely a graphical representation of how well the classifier did. The higher the roc_auc, the better, but too high and you’ve likely overfit the data. We can look at the roc_auc metric for both sides of our prediction. In this case, our model is not great on Wins. roc_auc(trainpredict, truth = Outcome, .pred_W) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.156 But is quite confident on Loses. roc_auc(trainpredict, truth = Outcome, .pred_L) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.844 The advantage of the roc_auc curve is that you can visualize it. roc_data &lt;- roc_curve(trainpredict, truth = Outcome, .pred_L) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.4 Comparing it to test data Now we can apply our fit to the test data to see how robust it is. Short version: Pretty good. Our numbers don’t dip all that much. testpredict &lt;- log_fit %&gt;% predict(new_data = log_test) %&gt;% bind_cols(log_test) testpredict ## # A tibble: 9,514 x 13 ## .pred_class Season Team Date Opponent Location Outcome ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 W 2014-… Abil… 2014-12-22 South C… N W ## 2 L 2014-… Abil… 2015-03-05 Incarna… A L ## 3 L 2014-… Air … 2015-01-31 UNLV A L ## 4 W 2014-… Air … 2015-02-04 New Mex… H W ## 5 L 2014-… Akron 2014-11-23 South C… N W ## 6 W 2014-… Akron 2014-12-04 Western… H W ## 7 W 2014-… Akron 2015-01-03 Coppin … H W ## 8 W 2014-… Akron 2015-02-21 Miami (… H L ## 9 W 2014-… Akron 2015-03-09 Norther… H W ## 10 W 2014-… Alab… 2014-11-25 Jackson… H L ## # … with 9,504 more rows, and 6 more variables: ## # Cumulative_Mean_Possessions &lt;dbl&gt;, Cumulative_Mean_PPP &lt;dbl&gt;, ## # TeamSRS &lt;dbl&gt;, Opponent_Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_PPP &lt;dbl&gt;, OpponentSRS &lt;dbl&gt; testpredict &lt;- log_fit %&gt;% predict(new_data = log_test, type=&quot;prob&quot;) %&gt;% bind_cols(testpredict) testpredict ## # A tibble: 9,514 x 15 ## .pred_L .pred_W .pred_class Season Team Date Opponent Location Outcome ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 0.483 0.517 W 2014-… Abil… 2014-12-22 South C… N W ## 2 0.943 0.0566 L 2014-… Abil… 2015-03-05 Incarna… A L ## 3 0.721 0.279 L 2014-… Air … 2015-01-31 UNLV A L ## 4 0.443 0.557 W 2014-… Air … 2015-02-04 New Mex… H W ## 5 0.684 0.316 L 2014-… Akron 2014-11-23 South C… N W ## 6 0.0183 0.982 W 2014-… Akron 2014-12-04 Western… H W ## 7 0.0224 0.978 W 2014-… Akron 2015-01-03 Coppin … H W ## 8 0.154 0.846 W 2014-… Akron 2015-02-21 Miami (… H L ## 9 0.190 0.810 W 2014-… Akron 2015-03-09 Norther… H W ## 10 0.377 0.623 W 2014-… Alab… 2014-11-25 Jackson… H L ## # … with 9,504 more rows, and 6 more variables: ## # Cumulative_Mean_Possessions &lt;dbl&gt;, Cumulative_Mean_PPP &lt;dbl&gt;, ## # TeamSRS &lt;dbl&gt;, Opponent_Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_PPP &lt;dbl&gt;, OpponentSRS &lt;dbl&gt; metrics(testpredict, Outcome, .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.742 ## 2 kap binary 0.484 testpredict %&gt;% conf_mat(Outcome, .pred_class) ## Truth ## Prediction L W ## L 3536 1200 ## W 1256 3522 roc_auc(testpredict, truth = Outcome, .pred_W) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.175 roc_auc(testpredict, truth = Outcome, .pred_L) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.825 roc_data &lt;- roc_curve(testpredict, truth = Outcome, .pred_L) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.5 How well did it do with Nebraska? Let’s grab predictions for Nebraska from both our test and train data and take a look. nutrain &lt;- trainpredict %&gt;% filter(Team == &quot;Nebraska&quot;, Season == &quot;2020-2021&quot;) nutest &lt;- testpredict %&gt;% filter(Team == &quot;Nebraska&quot;, Season == &quot;2020-2021&quot;) bind_rows(nutrain, nutest) %&gt;% arrange(Date) ## # A tibble: 23 x 15 ## .pred_L .pred_W .pred_class Season Team Date Opponent Location Outcome ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 3.63e-4 1.00 W 2020-… Nebr… 2020-11-25 McNeese… H W ## 2 1.49e-1 0.851 W 2020-… Nebr… 2020-11-26 Nevada H L ## 3 2.42e-2 0.976 W 2020-… Nebr… 2020-11-28 North D… H W ## 4 5.76e-2 0.942 W 2020-… Nebr… 2020-12-01 South D… H W ## 5 5.67e-1 0.433 L 2020-… Nebr… 2020-12-09 Georgia… H L ## 6 9.30e-1 0.0703 L 2020-… Nebr… 2020-12-11 Creight… A L ## 7 9.51e-1 0.0485 L 2020-… Nebr… 2020-12-22 Wiscons… A L ## 8 8.73e-1 0.127 L 2020-… Nebr… 2020-12-25 Michigan H L ## 9 9.42e-1 0.0582 L 2020-… Nebr… 2020-12-30 Ohio St… A L ## 10 6.14e-1 0.386 L 2020-… Nebr… 2021-01-02 Michiga… H L ## # … with 13 more rows, and 6 more variables: Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Cumulative_Mean_PPP &lt;dbl&gt;, TeamSRS &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_Possessions &lt;dbl&gt;, ## # Opponent_Cumulative_Mean_PPP &lt;dbl&gt;, OpponentSRS &lt;dbl&gt; Our model didn’t foresee Teddy Allen hitting a last second shot to beat Penn State, and it was confident we’d beat Nevada early in the season, but alas, we didn’t. Otherwise, it’s been pretty sure we’d lose the games we’ve lost. But this is just looking at season means. How could you improve this? "]]
