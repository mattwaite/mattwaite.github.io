[["index.html", "Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R Chapter 1 Introduction 1.1 Requirements and Conventions 1.2 About this book", " Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R By Matt Waite 2022-02-17 Chapter 1 Introduction The 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic. For the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner. How close? This is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved. The specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here. First, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good. Second, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good. Third, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away. Using logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t. So how much of a corner would the team need to turn? With modeling, we can figure this out. What would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent? As played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa). So how close are they to turning the corner? That close. 1.1 Requirements and Conventions This book is all in the R statistical language. To follow along, you’ll do the following: Install the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer. Install R Studio Desktop. The free version is great. Going forward, you’ll see passages like this: install.packages(&quot;tidyverse&quot;) Don’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do. 1.2 About this book This book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it: It is free for students. The topics will remain the same but the text is going to be constantly tinkered with. What is the work of the author is copyright Matt Waite 2021. The text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either. As such, the whole book – authored in Bookdown – is open sourced on Github. Pull requests welcomed! "],["the-modeling-process-and-linear-regression.html", "Chapter 2 The modeling process and linear regression 2.1 Feature engineering 2.2 Setting up the modeling process 2.3 Predicting based on the model 2.4 Predicting data we haven’t seen before 2.5 Looking locally", " Chapter 2 The modeling process and linear regression One of the most common – and seemingly least rigorous – parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method – looking at how a team is playing, looking at the players, consulting their gut – but rarely ever do you hear of a sports pundit using a model. We’re going to change that. Throughout this book, you’ll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not). Let’s start by looking at predicting how many points the team should score given how well they are shooting. And we’ll use this as a chance to look at linear regression modeling. If you don’t have them already installed, we’ll need the tidyverse and tidymodels for this book. As well as zoo to make rolling means and hoopR for data. install.packages(c(&quot;tidyverse&quot;, &quot;tidymodels&quot;, &quot;zoo&quot;, &quot;hoopR&quot;) After they’ve installed – and if you haven’t this will take a bit – load them. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) For this walkthrough, we’re going to use a dataset of college basketball games from the 14-15 season through current games from hoopR. You can pull those from games from the library like this: Let’s load this data and do a little work on it. The first function pulls the team box scores, then I use dplyr’s separate and mutate_at functions to reformat hoopR’s ways of recording shots made and shots attempted. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) 2.1 Feature engineering Feature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good. Let’s look at basketball games again. A number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score? One problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this: Possessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts) Since we’re trying to predict how many points a team will score, we need to know that. If you look at the data, however, you’ll see that’s not actually in the data. Which is unfortunate. But we can calculate it pretty easily. Then we’ll use the possessions estimate formula to get that, so we can then calculate points per possession. While we’re here, we’ll add true shooting percentage as well – to try and incorporate some free throw shooting into our metrics. We’ll save that to a new dataframe called teamstats. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100 ) %&gt;% ungroup() Now we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar. Split your data into training and testing data sets. A common split is 80/20. Train the model on the training dataset. Evaluate the model on the training data. Apply the model to the testing data. Evaluate the model on the test data. From there, it’s how you want to use the model. We’ll walk through a simple example here, using the simplest model – a linear model. Linear models are something you’ve understood since you took middle school math and learned the equation of a line. Remember y = mx + b? It’s back. And, unlike what you complained bitterly in middle school, it’s very, very useful. What a linear model says, in words is that we can predict y if we multiply a value – a coefficient – by our x value offset with b, which is really the y-intercept, but think of it like where the line starts. Or, expressed as y = mx + b: points = true_shooting_percentage * ? + some starting point. Think of some starting point as what the score should be if the true_shooting_percentage is zero. Should be zero, right? Intuitively, yes, but it won’t always work out so easily. What we’re trying to do here is predict how many points a team should score given their shooting prowess as a team or their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats BEFORE the game – what we knew about the team going into the game in question. We can do that using zoo and rolling means. We’ll add three new columns – the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage. teamstats &lt;- teamstats %&gt;% group_by(team_short_display_name) %&gt;% mutate( rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=4, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=4, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=4, fill=true_shooting_percentage) ) %&gt;% ungroup() 2.2 Setting up the modeling process With most modeling tasks we need to start with setting a random number seed to aid our random splitting of data into training and testing. set.seed(1234) Random numbers play a large role in a lot of data science algorithms, so setting one helps our reproducibility. After that, we split our data. There’s a number of ways to do this – R has a bunch and you’ll find all kinds of examples online – but Tidymodels has made this easy. game_split &lt;- initial_split(teamstats, prop = .8) game_split ## &lt;Analysis/Assess/Total&gt; ## &lt;70265/17567/87832&gt; What does this mean? It says that initial_split divided the data into 68,000+ games in analysis (or training), 17,000+ into assess (or test), of the 86,000+ total records in the dataset. But the split object isn’t useful to us. We need to assign them to dataframes. We do so like this: game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Now we have two dataframes – game_train and game_test – that we can now use for modeling. First step to making a model is to set what type of model this will be. We’re going to name our model object – lm_model works because this is a linear model. We’ll use the linear_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “lm.” lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) We can get a peek at lm_model and make sure we did everything right by just typing it and executing. lm_model ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Now, let’s fit a linear model to our data. We’ll name the fitted model fit_lm and we’ll take our model object that we just created and fit it using the fit function. What goes in the fit function can be read like this: team_score is approximately modeled by the rolling mean of shooting percentage The only thing left is to specify the dataset. fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) Let’s take a look at what the fitted model object tells us about our data. tidy(fit_lm, conf.int = TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13.1 0.417 31.4 1.17e-214 12.3 13.9 ## 2 rolling_shooting_pe… 1.33 0.00945 141. 0 1.31 1.35 The two most important things to see here are the terms and the estimates. Start with rolling_shooting_percentage. What that says is for every 10 percentage points of shooting percentage, a team should score 13 points. HOWEVER, the intercept has something to say about this. What the intercept says is that a team with a big fat zero for shooting percentage is going to score 13 points. Wait … how? Well, are field goals the only way to score in basketball? No. So there’s some of your non-zero intercept. Think again about y = mx + b. We have our terms here: y is team score, m is 1.3 x is the team shooting percentage and b is 13.1. Let’s pretend for a minute that you coached a team that shot 40 percent in college basketball. Our model predicts you would score about 65 points. 2.3 Predicting based on the model Now, we can take the model predictions and bind them to our dataset. This will be a common step throughout this book so we can see what the model predicted vs what the real world produced. trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) Walking through this, we’re creating a dataframe called trainresults, which is game_train with the results of the predict function bound to it. The predict function takes two arguments – the fitted model and the dataset it is being applied to, which in this case is the same dataset. What will result is our game_train dataset with a new column: .pred Our first step in evaluating a linear model is to get the r-squared value. The yardstick library (part of Tidymodels) does this nicely. We tell it to produce metrics on a dataset, and we have to tell it what the real world result is (the truth column) and what the estimate column is (.pred). metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.8 ## 2 rsq standard 0.221 ## 3 mae standard 9.24 We’ll get more into RMSE and MAE later. For now, focus on rsq or r-squared. What that says is that changes in a four game rolling shooting percentage account for 22 percent of the variation in team score. That’s pretty good. Not great, but for one stat, it’s not bad. A way to look at this is with a scatterplot. The geom_smooth creates its own linear model and puts the line of best fit through our dots. ggplot() + geom_point(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score)) + geom_smooth(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 696 rows containing non-finite values (stat_smooth). ## Warning: Removed 696 rows containing missing values (geom_point). As you can see, there’s a lot of dots above the line and below the line. That gap is a called a residual. The residual is the actual thing minus the predicted thing. The truth minus our guess. A positive residual – in this case – is good. It means that player is scoring more than we’d predict they would. A negative residual means they’re not scoring as much as we’d expect. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% mutate(label = case_when( residual &gt; 0 ~ &quot;Positive&quot;, residual &lt; 0 ~ &quot;Negative&quot;) ) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=team_score, color=label)) + geom_smooth(aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 565 rows containing non-finite values (stat_smooth). ## Warning: Removed 565 rows containing missing values (geom_point). Residuals, aside from telling us who is and isn’t playing well, can tell us if a linear model is appropriate for this data. We can use a scatterplot to reveal this. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=residual)) ## Warning: Removed 565 rows containing missing values (geom_point). What we’re looking for is for the dots to be randomly spaced around the plot. It should look like someone spilled Skittles on the floor. This … does. It means a linear model is appropriate here. More on that in the coming chapters. 2.4 Predicting data we haven’t seen before Now we can do the same thing, but with the test data. testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) What do these metrics look like? metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.7 ## 2 rsq standard 0.216 ## 3 mae standard 9.17 If you look at the r-squared value, you’ll note that when we apply the same model to our test data, the amount of variance that we can explain goes down a little. It’s not much, so the model does a decent job of predicting data we haven’t seen before, which is the whole point of creating a model. 2.5 Looking locally We can get clearer picture of what these predictions look like if we look at something we know – like this season’s Nebraska team. What does the model say about how they are doing? First, we can get Nebraska’s games with a filter. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) Now apply the model to the games. nupreds &lt;- nu %&gt;% bind_cols(predict(fit_lm, nu)) To really see this clearly, we’ll calculate the residual, then sort by the residual. Where did the model miss the most, for good or bad? nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 25 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 72.4 27.6 ## 2 2021-11-27 Nebraska South Dakota 83 68.1 14.9 ## 3 2022-01-03 Nebraska Ohio State 79 64.9 14.1 ## 4 2021-12-23 Nebraska Kennesaw State 88 74.9 13.1 ## 5 2022-02-02 Nebraska Michigan 79 67.9 11.1 ## 6 2021-12-08 Nebraska Michigan 67 61.2 5.80 ## 7 2021-11-20 Nebraska Idaho State 78 74.0 4.02 ## 8 2021-11-10 Nebraska Western Illinois 74 70.6 3.44 ## 9 2021-11-17 Nebraska Creighton 69 66.0 2.99 ## 10 2021-11-21 Nebraska Southern 82 79.2 2.81 ## # … with 15 more rows What does this mean? It says the model predicted the team would score 75 against NC State and they put up 100, for a 25 point miss (residual). Might have had something to do with that game going to three overtimes, but alas, our model can’t get everything right. For most games, the prediction is within a few points, with some odd games with larger misses. Linear models are incredibly important to understand — they underpin many of the more advanced methods we’ll talk about going forward — so understanding them now is critical. "],["multiple-regression.html", "Chapter 3 Multiple regression 3.1 A multiple regression speed run 3.2 Picking what moves the needle", " Chapter 3 Multiple regression As we saw in the previous chapter, we can measure how much something can be predicted by another thing. We looked at how many points a team can score based on their shooting percentage. The theory being how well you shoot the ball probably has a lot to say about how many points you score. And what did we find? It’s a part of the story, but not the whole story. But that raises the problem with simple regressions – they’re simple. Anyone who has watched a basketball game knows there’s a lot more to the outcome than just shooting prowess. Enter the multiple regression. Multiple regressions are a step toward reality – where more than one thing influences the outcome. However, the more variance we attempt to explain, the more error and uncertainty we introduce into our model. Let’s begin by loading some libraries and installing a new one: corrr library(tidyverse) library(tidymodels) library(zoo) library(hoopR) library(corrr) For this, we’ll work with our college basketball game data and we’ll continue down the road we started in the last chapter. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion 3.1 A multiple regression speed run First, let’s restore what we did in last chapter with the feature engineering we did, making the different metrics and the rolling numbers. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=2, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, fill=true_shooting_percentage) ) %&gt;% ungroup() Now we’ll split our data into training and testing, creating a linear model predicting score from the shooting percentage and producing the metrics for the results. set.seed(1234) game_split &lt;- initial_split(teamstats, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.0 ## 2 rsq standard 0.321 ## 3 mae standard 8.63 Bottom line: We can predict about 32 percent of the difference in team scores by the shooting percentage. But we know, because we’ve shot hoops in the driveway before, or went through a basketball unit in PE in the third grade, that sure, being a good shooter is important, but how many times you shoot the ball is also important. If you’re a 100 percent shooter, that’s insane, but it probably means you took one shot. Congrats, you scored two points (three if you’re gutsy). One shot is not going to win a game. In our feature engineering, we created another metric – points per posesssion. It’s a measure of efficiency – did you score when you had the ball? We created a rolling metric for this too. To add it to our model, it’s as simple as just adding + rolling_ppp fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage + rolling_ppp, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.97 ## 2 rsq standard 0.445 ## 3 mae standard 7.77 And just like that, by acknowledging reality, we’ve jumped to 44 percent of variance explained and our root mean squared error – the average amount we’re off by – has dropped from 11 to 10. Your temptation now is to start adding things until we get to 1 on the r-squared and 0 on the rmse. The problem with that is called “overfitting” Overfitting is where you produce a model that is too close to your training data, which makes it prone to fail with data you’ve never seen before – the model becomes unreliable when it’s not the training data anymore. A secondary problem you encounter is this: the point of this is to predict future events. In this class, we’re attempting to predict the outcome of things that have not yet happened. That means we are going to be estimating the inputs to these models, inputs that will no doubt have error. So our inputs have a range of possible outcomes, our model is not perfect, so the outcome is going to combine the two. The more elements of your model that you use as inputs, the more error – uncertainty – you are introducing. The point is you want to pick the things that really matter and ignore the rest in some vain quest to get to 100 percent. You won’t get there. 3.2 Picking what moves the needle There are multiple ways to find the right combination of inputs to your models. With multiple regressions, the most common is the correlation matrix. We’re looking to maximize r-squared by choosing inputs that are highly correlated to our target value, but not correlated with other things. Example: We can assume that field_goals_made and field_goal_pct are highly correlated to team_score, but the number of Field Goals made is also highly correlated with the field goal percentage. Using corrr, we can create a correlation matrix in a dataframe to find columns that are highly correlated with our target – team_score. To do this, we need to select the columns we’re working with – our three rolling metrics. teamstats %&gt;% select(team_score, rolling_shooting_percentage, rolling_ppp, rolling_true_shooting_percentage) %&gt;% correlate() ## # A tibble: 4 × 5 ## term team_score rolling_shooting_pe… rolling_ppp rolling_true_shooti… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 team_score NA 0.566 0.665 0.597 ## 2 rolling_shoo… 0.566 NA 0.817 0.930 ## 3 rolling_ppp 0.665 0.817 NA 0.864 ## 4 rolling_true… 0.597 0.930 0.864 NA Reading this can be a lot, and it helps to take some notes as you go. You read up and down and left and right – it’s a matrix. Follow the rolling_true_shooting_percentage row across to the rolling_shooting_percentage column and you’ll see they’re almost perfectly correlated with each other – 1 is a perfect correlation. What does that mean? It means including both is going to just add error without adding much value. They’re so similar. You pick the one that is more highly correlated with team_score – true shooting. So how does this look in a model? Since we already have the features – the columns – in our data and we have it split into training and testing, we just need to create a new fit. new_fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_true_shooting_percentage + rolling_ppp, data = game_train) Let’s take a peek at our model coefficients. tidy(new_fit_lm, conf.int = TRUE) ## # A tibble: 3 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.532 0.323 -1.65 9.98e- 2 -1.17 0.102 ## 2 rolling_true_shootin… 0.180 0.0117 15.4 1.24e-53 0.157 0.203 ## 3 rolling_ppp 60.6 0.577 105. 0 59.5 61.8 What this says is if we can manage one point per possession in a basketball game, we’ll score 60 points. The model says for each 10 points of true shooting percentage, we’ll add another 1.8 points. Now we play the games. newtrainresults &lt;- game_train %&gt;% bind_cols(predict(new_fit_lm, game_train)) metrics(newtrainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.97 ## 2 rsq standard 0.445 ## 3 mae standard 7.76 So … we’re up to an r-squared of .44 and an rmse under 10. The goal here is to add to r-squared and reduce our error metrics. How well does the model do with data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 9.85 ## 2 rsq standard 0.440 ## 3 mae standard 7.70 It’s remarkably stable. Our rmse is all but unchanged as is our r-squared. Until we find other metrics or other methods, this is solid. Let’s put this against a set of games we’re familiar with. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(new_fit_lm, nu)) nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 25 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 71.9 28.1 ## 2 2021-12-23 Nebraska Kennesaw State 88 72.2 15.8 ## 3 2022-02-02 Nebraska Michigan 79 66.6 12.4 ## 4 2022-01-03 Nebraska Ohio State 79 67.2 11.8 ## 5 2021-12-08 Nebraska Michigan 67 56.5 10.5 ## 6 2022-02-10 Nebraska Minnesota 78 68.0 10.0 ## 7 2021-11-10 Nebraska Western Illinois 74 65.7 8.30 ## 8 2021-11-13 Nebraska Sam Houston 74 68.9 5.14 ## 9 2022-02-13 Nebraska Iowa 75 70.1 4.89 ## 10 2021-11-27 Nebraska South Dakota 83 78.3 4.74 ## # … with 15 more rows The question to start thinking about is this – what else could we include? "],["decision-trees-and-random-forests.html", "Chapter 4 Decision trees and random forests 4.1 An intro to pre-processing 4.2 Decision trees 4.3 Random forest", " Chapter 4 Decision trees and random forests Tree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less than x seasons of experience? Do they have more or less then y minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have on average. The upside of decision trees is that if the model is small, you can explain it to anyone. They’re very easy to understand. The trouble with decision trees is that if the model is small, they’re a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree. The most common is the random forest. Let’s implement one. We start with libraries. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) library(corrr) We’ll be using college basketball games again. Let’s load this data and add our rolling metrics right away. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=2, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, fill=true_shooting_percentage), rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=2, fill=turnover_pct), rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=2, fill=free_throw_factor), ) %&gt;% ungroup() More often than not, we need to do more than just use the data we have. Often, with modeling, we need to pre-process our data. Pre-processing can mean a lot of things – fixing dates, creating new features, scaling numbers to be similar – but it’s all about making your models better. 4.1 An intro to pre-processing To simplify things, we’re going to first simplify our data. We want to start with a minimum of columns. We need the columns to help us identify individual records, we need our predictors and we need the outcome we’re trying to predict. modelgames &lt;- teamstats %&gt;% select(team_short_display_name, opponent_name, game_date, season, team_score, rolling_ppp, rolling_free_throw_factor, rolling_turnover_percentage) %&gt;% na.omit() Now we need to split our data into training and testing sets. set.seed(1234) game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Going forward, we’re going to make our lives easier by using workflows. Workflows in tidymodels take in a pre-processing recipe and a model definition and executes those things to make our modeling code slimmer and our lives easier. To start, we need to define a pre-processing recipe. The recipe defines a series of steps that will be performed on your data. We’ll start simple and add our formula from previous work. score_rec &lt;- recipe(team_score ~ rolling_ppp + rolling_turnover_percentage + rolling_free_throw_factor, data = game_train) Another, more flexible way to express this, is using the . to say all predictors. In this case, all predictors is rolling_ppp, rolling_turnover_percentage and rolling_free_throw_factor. What follows is the same as above, just less typing. But we’re also going to add a role to our recipe. In this case, the role is how we’re going to identify each row – an ID. In this case, to identify a game, we need to know the Team, the Opponent, the Date and the Season. What isn’t an ID is a predictor. score_rec &lt;- recipe(team_score ~ ., data = game_train) %&gt;% update_role(team_short_display_name, opponent_name, game_date, season, new_role = &quot;ID&quot;) Now that we’ve created our pre-processing recipe, we can create our model definition. 4.2 Decision trees As discussed earlier, decision trees are essentially a series of if/else statements. Visualized, they look like branches on a tree (thus, decision trees). We’ve already defined a recipe for our data, so now we’re ready to define a model definition. First, we’ll use decision trees to prove a point. tree &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;regression&quot;) Now we’ll create the workflow. In its simplest form, the workflow defines itself as a workflow and then adds a recipe and a model definition. tree_wf &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(tree) Now we can fit the data with our model using the workflow. This applies our recipe to the data without us having to do it, then uses the model definition to do the fitting. tree_fit &lt;- tree_wf %&gt;% fit(data = game_train) What does this produce? Here’s what a basic decision tree looks like. tree_fit %&gt;% pull_workflow_fit() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. ## parsnip model object ## ## Fit time: 476ms ## n= 69525 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 69525 12350080.0 71.46695 ## 2) rolling_ppp&lt; 1.017359 32038 3715305.0 63.82511 ## 4) rolling_ppp&lt; 0.8915218 9752 950488.5 57.15997 * ## 5) rolling_ppp&gt;=0.8915218 22286 2142020.0 66.74168 * ## 3) rolling_ppp&gt;=1.017359 37487 5164835.0 77.99800 ## 6) rolling_ppp&lt; 1.146048 25624 2716267.0 74.97694 * ## 7) rolling_ppp&gt;=1.146048 11863 1709553.0 84.52348 ## 14) rolling_ppp&lt; 1.251591 9302 1140517.0 82.69630 * ## 15) rolling_ppp&gt;=1.251591 2561 425182.4 91.16009 * They can be a bit tough to read, but take the bottom three nodes. It says if rolling_ppp is greater than or equal to 1.17, your score is around 85 points a game. If it’s less than 1.17, you’ll score around 76 points. Easy to understand, right? The algorithm cuts branches when the splits stop reducing error, and there’s a limit But here’s where the crude instrument comes in. Let’s use our decision tree to predict some scores. treeresults &lt;- game_train %&gt;% bind_cols(predict(tree_fit, game_train)) What are the accuracy metrics we get? metrics(treeresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 10.3 ## 2 rsq standard 0.403 ## 3 mae standard 8.05 Our rsquared is about .40, which isn’t terrible. And our MAE says we’re off by 8 points on average, but our RSME says were off by 10.3 points on a different average, indicating there’s some big misses. We can do better. 4.3 Random forest Enter the random forest. A random forest is, as the name implies, a large number of decision trees, and they use a random set of inputs. The algorithm creates a large number of randomly selected training inputs, and randomly chooses the feature input for each branch, creating predictions. The goal is to create uncorrelated forests of trees. The trees all make predictions, and the wisdom of the crowds takes over. In the case of classification algorithm, the most common prediction is the one that gets chosen. In a regression model, the predictions get averaged together. The random part of random forest is in how the number of tree splits get created and how the samples from the data are taken to generate the splits. They’re randomized, which has the effect of limiting the influence of a particular feature and prevents overfitting – where your predictions are so tailored to your training data that they miss badly on the test data. For random forests, we change the model type to rand_forest and set the engine to “ranger.” There’s multiple implementations of the random forest algorithm, and the differences between them are beyond the scope of what we’re doing here. rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) And now we can create our workflow. We first need to define it as a workflow, then add the model and add the recipe. score_wflow &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(rf_mod) score_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Random Forest Model Specification (regression) ## ## Computational engine: ranger With the workflow in place, we can fit our model. Note: this can make your laptop fan go wheeeeee. score_fit &lt;- score_wflow %&gt;% fit(data = game_train) Now we can use a use a new function – pull_workflow_fit, which pulls the fit stats we want to see to evaluate it. score_fit %&gt;% pull_workflow_fit() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. ## parsnip model object ## ## Fit time: 1m 6.3s ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 69525 ## Number of independent variables: 3 ## Mtry: 1 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 103.9164 ## R squared (OOB): 0.4150091 Similar to previous work, we can bind the prediction to our training data and evaluate the model. trainresults &lt;- game_train %&gt;% bind_cols(predict(score_fit, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 4.87 ## 2 rsq standard 0.895 ## 3 mae standard 3.76 Note: The RMSE for this model is down to 4.8. The R-squared is absurdly high. But how does this model handle data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(score_fit, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 10.3 ## 2 rsq standard 0.407 ## 3 mae standard 8.02 How well does the random forest algorithm do with Nebraska’s schedule in 2020-2021 prior to a month-long COVID break? nu &lt;- modelgames %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(score_fit, nu)) nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 25 × 5 ## team_short_display_name opponent_name team_score .pred residual ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebraska NC State 100 87.3 12.7 ## 2 Nebraska Iowa 75 66.9 8.07 ## 3 Nebraska Michigan 79 71.0 7.96 ## 4 Nebraska Kennesaw State 88 81.0 7.00 ## 5 Nebraska Minnesota 78 71.9 6.13 ## 6 Nebraska Indiana 71 65.6 5.36 ## 7 Nebraska Michigan 67 61.7 5.31 ## 8 Nebraska Ohio State 79 74.4 4.56 ## 9 Nebraska Western Illinois 74 69.7 4.31 ## 10 Nebraska South Dakota 83 79.1 3.90 ## # … with 15 more rows Compare this to the multiple regression of the previous chapter. Take just the NC State game as an example. The multiple regression model was off by 28 points – which is a lot, but it’s a triple overtime game. But the random forest managed to be off by 12. "],["xgboost.html", "Chapter 5 XGBoost 5.1 Hyperparameters", " Chapter 5 XGBoost As we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular. Boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on. The path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost, are very popular for solving a wide variety of problems. Let’s re-implement our predictions in an XGBoost algorithm. First, we’ll load libraries and we’re going to introduce a new one here – doParallel – which handles using more of your computer’s processor cores to accomplish tasks in parallel instead of one core at a time. In other words, instead of one task, it can do X at a time in parallel and put the answers together after. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) library(doParallel) cores &lt;- parallel::detectCores(logical = FALSE) We’ll load our game data and do a spot of feature engineering that we used with random forests. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, fill=field_goal_pct), rolling_ppp = rollmean(lag(ppp, n=1), k=2, fill=ppp), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, fill=true_shooting_percentage), rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=2, fill=turnover_pct), rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=2, fill=free_throw_factor), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), rolling_orb = rollmean(lag(orb, n=1), k=2, fill=orb), rolling_drb = rollmean(lag(drb, n=1), k=2, fill=drb) ) modelgames &lt;- newteamstats %&gt;% select(team_short_display_name, opponent_name, game_date, season, team_score, rolling_true_shooting_percentage, rolling_free_throw_factor, rolling_turnover_percentage, rolling_orb, rolling_drb) %&gt;% na.omit() Per usual, we split our data into training and testing. game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) And our recipe. game_rec &lt;- recipe(team_score ~ ., data = game_train) %&gt;% update_role(team_short_display_name, opponent_name, game_date, season, new_role = &quot;ID&quot;) summary(game_rec) To this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to. 5.1 Hyperparameters The hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters: Number of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem. Learn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point. Tree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting. Minimum number of observations in the terminal node – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting). Other settings: Loss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative. Sample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfitting. mtry – the number of predictors that will be randomly sampled at each split when making trees. All of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning. First, we make a mode and label each parameter as tune() xg_mod &lt;- boost_tree( trees = tune(), learn_rate = tune(), tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), ) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;xgboost&quot;, nthread = cores) Let’s make a workflow now that we have our recipe and our model. game_wflow &lt;- workflow() %&gt;% add_model(xg_mod) %&gt;% add_recipe(game_rec) Now, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. What follows is what parameters the hypercube will tune. xgb_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), game_train), learn_rate(), size = 30 ) xgb_grid How do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result. game_folds &lt;- vfold_cv(game_train) game_folds This part takes about 25-30 minutes on my machine and it will saturate all of your processors, so your computer just needs to sit there. No texting, no YouTube, nothing. Let it burn. doParallel::registerDoParallel(cores = cores) xgb_res &lt;- tune_grid( game_wflow, resamples = game_folds, grid = xgb_grid, control = control_grid(save_pred = TRUE) ) doParallel::stopImplicitCluster() xgb_res So our grid has run on all of our validation samples, and what do we see? collect_metrics(xgb_res) Well we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination. show_best(xgb_res, &quot;rmse&quot;) The best combination as of this data update comes up with an RMSE of 9.93. Second is 9.97. Let’s capture our best set of hyperparameters. best_rmse &lt;- select_best(xgb_res, &quot;rmse&quot;) And now put that into a final workflow. Pay attention to the main arguments in the output below. final_xgb &lt;- finalize_workflow( game_wflow, best_rmse ) final_xgb There’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along. We create a fit. xg_fit &lt;- final_xgb %&gt;% fit(data = game_train) We can see something things about that fit, including all the iterations of our XGBoost model. Note: our tuned number of trees is 1,665 – and in the workflow fit, you can see 1,665 iterations. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The RMSE goes down with each iteration as we go down the gradient desent. xg_fit %&gt;% pull_workflow_fit() Now, like before, we can bind our predictions using our xg_fit to the game_train data. trainresults &lt;- game_train %&gt;% bind_cols(predict(xg_fit, game_train)) And now see how we did. metrics(trainresults, truth = team_score, estimate = .pred) How about the test data? testresults &lt;- game_test %&gt;% bind_cols(predict(xg_fit, game_test)) metrics(testresults, truth = team_score, estimate = .pred) Unlike the random forest, not nearly the drop in metrics between train and test. "],["logistic-regression.html", "Chapter 6 Logistic Regression 6.1 Visualizing the decision boundary 6.2 The logistic regression 6.3 Evaluating the fit 6.4 Comparing it to test data 6.5 How well did it do with Nebraska?", " Chapter 6 Logistic Regression Up to this point, we’ve been dealing with problems that lead to a quantitative answer: We want a number. How many points? How many possessions? But there are lots of problems in the world where the answer is a classification: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not? These are problems of classification and they use many of the same algorithms we’ve used to try and predict those classes. Ultimately, the algorithms will predict the probability that this row is X or Y and make a decision based on that probability. That probability will be somewhere between 0 and 1, with 0 being no chance and 1 being a sure thing. Where this gets interesting is in the middle. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) What we need to do here is get both sides of the game. We’ll start with getting the box scores. teamgames &lt;- load_mbb_team_box(seasons = 2019:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:35, as.numeric) Now we’ll create the team side of the game. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, fill=true_shooting_percentage), team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=2, fill=turnover_pct), team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=2, fill=free_throw_factor), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), team_rolling_orb = rollmean(lag(orb, n=1), k=2, fill=orb), team_rolling_drb = rollmean(lag(drb, n=1), k=2, fill=drb) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) team_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() Now we’ll use use the same dataframe and rename some columns to create the opponent side of the game. opponent_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() %&gt;% rename( opponent_id = team_id, opponent_short_display_name = team_short_display_name, opponent_score = team_score, opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage, opponent_rolling_free_throw_factor = team_rolling_free_throw_factor, opponent_rolling_turnover_percentage = team_rolling_turnover_percentage, opponent_rolling_orb = team_rolling_orb, opponent_rolling_drb = team_rolling_drb ) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) Now we’ll join them together. games &lt;- team_side %&gt;% inner_join(opponent_side) ## Joining, by = c(&quot;game_id&quot;, &quot;opponent_id&quot;) The last problem to solve? Who won? We can add this with conditional logic. games &lt;- games %&gt;% mutate( TeamResult = as.factor(case_when( team_score &gt; opponent_score ~ &quot;W&quot;, opponent_score &gt; team_score ~ &quot;L&quot; ))) %&gt;% na.omit() For simplicity, let’s limit the number of columns we’re going to feed our model. modelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, team_rolling_true_shooting_percentage, opponent_rolling_true_shooting_percentage, team_rolling_turnover_percentage, opponent_rolling_turnover_percentage, TeamResult) levels(modelgames$TeamResult) ## [1] &quot;L&quot; &quot;W&quot; modelgames$TeamResult &lt;- relevel(modelgames$TeamResult, ref=&quot;W&quot;) levels(modelgames$TeamResult) ## [1] &quot;W&quot; &quot;L&quot; 6.1 Visualizing the decision boundary This is just one dimension of the data, but it can illustrate how this works. You can see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. That neatly captures the probabilities we’re looking at here. ggplot() + geom_point(data=games, aes(x=team_rolling_true_shooting_percentage, y=opponent_rolling_true_shooting_percentage, color=TeamResult)) 6.2 The logistic regression Much of implementing classification algorithms should look familiar by now. The steps we’ve been using are steps we will use again. First, we split into training and testing. log_split &lt;- initial_split(modelgames, prop = .8) log_train &lt;- training(log_split) log_test &lt;- testing(log_split) We create a recipe. In this case, we need to normalize our predictors so scale differences don’t create undue influences. This will turn all of our numbers into zscores. log_recipe &lt;- recipe(TeamResult ~ ., data = log_train) %&gt;% update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = &quot;ID&quot;) %&gt;% step_normalize(all_predictors()) summary(log_recipe) ## # A tibble: 10 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 game_id numeric ID original ## 2 game_date date ID original ## 3 team_short_display_name nominal ID original ## 4 opponent_short_display_name nominal ID original ## 5 season numeric ID original ## 6 team_rolling_true_shooting_percentage numeric predictor original ## 7 opponent_rolling_true_shooting_percentage numeric predictor original ## 8 team_rolling_turnover_percentage numeric predictor original ## 9 opponent_rolling_turnover_percentage numeric predictor original ## 10 TeamResult nominal outcome original We have four predictors – how well each team shot, and how much each team turned the ball over. Now we define the model. Note the set_mode. log_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) Now we have enough for a workflow. log_workflow &lt;- workflow() %&gt;% add_model(log_mod) %&gt;% add_recipe(log_recipe) And now we fit our model (this can take a few minutes). log_fit &lt;- log_workflow %&gt;% fit(data = log_train) 6.3 Evaluating the fit With logistic regression, there’s two things we’re looking at: The prediction and the probabilities. We can get those with two different fits and combine them together. trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train) %&gt;% bind_cols(log_train) trainpredict ## # A tibble: 31,718 × 11 ## .pred_class game_id game_date team_short_displa… opponent_short_di… season ## &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 W 401170944 2020-02-08 UMBC UMass Lowell 2020 ## 2 L 401370116 2022-01-22 Louisville Notre Dame 2022 ## 3 W 401372022 2021-12-02 Seton Hall Wagner 2022 ## 4 L 401172861 2020-01-18 Boston U Colgate 2020 ## 5 W 401170933 2019-12-01 UMass Lowell Cent Conn St 2020 ## 6 W 401175270 2020-02-07 Pepperdine Santa Clara 2020 ## 7 W 401084759 2019-01-11 Tulsa Cincinnati 2019 ## 8 W 401085630 2019-01-10 Duquesne Fordham 2019 ## 9 W 401172744 2019-12-21 Miami (OH) Bradley 2020 ## 10 W 401369827 2021-11-26 Mississippi St Louisville 2022 ## # … with 31,708 more rows, and 5 more variables: ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train, type=&quot;prob&quot;) %&gt;% bind_cols(trainpredict) trainpredict ## # A tibble: 31,718 × 13 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.786 0.214 W 401170944 2020-02-08 UMBC ## 2 0.0980 0.902 L 401370116 2022-01-22 Louisville ## 3 0.958 0.0424 W 401372022 2021-12-02 Seton Hall ## 4 0.428 0.572 L 401172861 2020-01-18 Boston U ## 5 0.574 0.426 W 401170933 2019-12-01 UMass Lowell ## 6 0.567 0.433 W 401175270 2020-02-07 Pepperdine ## 7 0.737 0.263 W 401084759 2019-01-11 Tulsa ## 8 0.798 0.202 W 401085630 2019-01-10 Duquesne ## 9 0.773 0.227 W 401172744 2019-12-21 Miami (OH) ## 10 0.516 0.484 W 401369827 2021-11-26 Mississippi St ## # … with 31,708 more rows, and 7 more variables: ## # opponent_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; There’s several metrics to look at, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. metrics(trainpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.789 ## 2 kap binary 0.577 Another way to look at that is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy. trainpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 12500 3358 ## L 3347 12513 The roc_auc metric is largely a graphical representation of how well the classifier did. The higher the roc_auc, the better, but too high and you’ve likely overfit the data. We can look at the roc_auc metric for both sides of our prediction. roc_auc(trainpredict, truth = TeamResult, .pred_W) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.873 But is quite confident on Loses. roc_auc(trainpredict, truth = TeamResult, .pred_L) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.127 The advantage of the roc_auc curve is that you can visualize it. roc_data &lt;- roc_curve(trainpredict, truth = TeamResult, .pred_W) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.4 Comparing it to test data Now we can apply our fit to the test data to see how robust it is. Short version: Pretty good. Our numbers don’t dip all that much. testpredict &lt;- log_fit %&gt;% predict(new_data = log_test) %&gt;% bind_cols(log_test) testpredict ## # A tibble: 7,930 × 11 ## .pred_class game_id game_date team_short_displa… opponent_short_di… season ## &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 L 401082352 2019-02-02 Nebraska Illinois 2019 ## 2 W 401082352 2019-02-02 Illinois Nebraska 2019 ## 3 W 401082355 2019-02-15 Illinois Ohio State 2019 ## 4 L 401082375 2019-01-06 Michigan Indiana 2019 ## 5 W 401082376 2019-01-12 Indiana Maryland 2019 ## 6 L 401082376 2019-01-12 Maryland Indiana 2019 ## 7 W 401082379 2019-01-23 Northwestern Indiana 2019 ## 8 W 401082380 2019-01-25 Michigan Indiana 2019 ## 9 L 401082380 2019-01-25 Indiana Michigan 2019 ## 10 L 401082383 2019-02-08 Indiana Iowa 2019 ## # … with 7,920 more rows, and 5 more variables: ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; testpredict &lt;- log_fit %&gt;% predict(new_data = log_test, type=&quot;prob&quot;) %&gt;% bind_cols(testpredict) testpredict ## # A tibble: 7,930 × 13 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.245 0.755 L 401082352 2019-02-02 Nebraska ## 2 0.755 0.245 W 401082352 2019-02-02 Illinois ## 3 0.818 0.182 W 401082355 2019-02-15 Illinois ## 4 0.304 0.696 L 401082375 2019-01-06 Michigan ## 5 0.787 0.213 W 401082376 2019-01-12 Indiana ## 6 0.211 0.789 L 401082376 2019-01-12 Maryland ## 7 0.902 0.0976 W 401082379 2019-01-23 Northwestern ## 8 0.980 0.0198 W 401082380 2019-01-25 Michigan ## 9 0.0207 0.979 L 401082380 2019-01-25 Indiana ## 10 0.0380 0.962 L 401082383 2019-02-08 Indiana ## # … with 7,920 more rows, and 7 more variables: ## # opponent_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; metrics(testpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.784 ## 2 kap binary 0.568 testpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 3068 803 ## L 909 3150 roc_auc(testpredict, truth = TeamResult, .pred_W) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.869 roc_auc(testpredict, truth = TeamResult, .pred_L) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.131 roc_data &lt;- roc_curve(testpredict, truth = TeamResult, .pred_W) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.5 How well did it do with Nebraska? Let’s grab predictions for Nebraska from both our test and train data and take a look. nutrain &lt;- trainpredict %&gt;% filter(team_short_display_name == &quot;Nebraska&quot;, season == 2022) nutest &lt;- testpredict %&gt;% filter(team_short_display_name == &quot;Nebraska&quot;, season == 2022) bind_rows(nutrain, nutest) %&gt;% arrange(game_date) %&gt;% select(.pred_W, .pred_L, .pred_class, TeamResult, everything()) ## # A tibble: 25 × 13 ## .pred_W .pred_L .pred_class TeamResult game_id game_date team_short_displ… ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.722 0.278 W L 401372146 2021-11-10 Nebraska ## 2 0.745 0.255 W W 401371360 2021-11-13 Nebraska ## 3 0.197 0.803 L L 401371964 2021-11-17 Nebraska ## 4 0.939 0.0609 W W 401372147 2021-11-20 Nebraska ## 5 0.981 0.0190 W W 401372148 2021-11-21 Nebraska ## 6 0.969 0.0308 W W 401372149 2021-11-24 Nebraska ## 7 0.684 0.316 W W 401372150 2021-11-27 Nebraska ## 8 0.359 0.641 L L 401370097 2021-12-02 Nebraska ## 9 0.192 0.808 L L 401364344 2021-12-04 Nebraska ## 10 0.0261 0.974 L L 401364348 2021-12-08 Nebraska ## # … with 15 more rows, and 6 more variables: opponent_short_display_name &lt;chr&gt;, ## # season &lt;int&gt;, team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; By our rolling metrics, we shold have beat Kansas State and lost to Kennesaw State, but it nailed the rest. Even the Minnesota win. How could you improve this? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
