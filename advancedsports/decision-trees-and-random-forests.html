<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Decision trees and random forests | Advanced Sports Data Analysis</title>
  <meta name="description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Decision trees and random forests | Advanced Sports Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Decision trees and random forests | Advanced Sports Data Analysis" />
  
  <meta name="twitter:description" content="This is the companion text to the University of Nebraska-Lincoln’s SPMC 460: Advanced Sports Data Analysis" />
  

<meta name="author" content="By Matt Waite" />


<meta name="date" content="2021-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-regression-and-feature-engineering.html"/>
<link rel="next" href="xgboost.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Sports Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#requirements-and-conventions"><i class="fa fa-check"></i><b>1.1</b> Requirements and Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-modeling-process.html"><a href="the-modeling-process.html"><i class="fa fa-check"></i><b>2</b> The modeling process</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-modeling-process.html"><a href="the-modeling-process.html#setting-up-the-modeling-process"><i class="fa fa-check"></i><b>2.1</b> Setting up the modeling process</a></li>
<li class="chapter" data-level="2.2" data-path="the-modeling-process.html"><a href="the-modeling-process.html#predicting-based-on-the-model"><i class="fa fa-check"></i><b>2.2</b> Predicting based on the model</a></li>
<li class="chapter" data-level="2.3" data-path="the-modeling-process.html"><a href="the-modeling-process.html#predicting-data-we-havent-seen-before"><i class="fa fa-check"></i><b>2.3</b> Predicting data we haven’t seen before</a></li>
<li class="chapter" data-level="2.4" data-path="the-modeling-process.html"><a href="the-modeling-process.html#looking-locally"><i class="fa fa-check"></i><b>2.4</b> Looking locally</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html"><i class="fa fa-check"></i><b>3</b> Multiple regression and feature engineering</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#a-multiple-regression-speed-run"><i class="fa fa-check"></i><b>3.1</b> A multiple regression speed run</a></li>
<li class="chapter" data-level="3.2" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#picking-what-moves-the-needle"><i class="fa fa-check"></i><b>3.2</b> Picking what moves the needle</a></li>
<li class="chapter" data-level="3.3" data-path="multiple-regression-and-feature-engineering.html"><a href="multiple-regression-and-feature-engineering.html#feature-engineering"><i class="fa fa-check"></i><b>3.3</b> Feature engineering</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html"><i class="fa fa-check"></i><b>4</b> Decision trees and random forests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#an-intro-to-pre-processing"><i class="fa fa-check"></i><b>4.1</b> An intro to pre-processing</a></li>
<li class="chapter" data-level="4.2" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision trees</a></li>
<li class="chapter" data-level="4.3" data-path="decision-trees-and-random-forests.html"><a href="decision-trees-and-random-forests.html#random-forest"><i class="fa fa-check"></i><b>4.3</b> Random forest</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>5</b> XGBoost</a>
<ul>
<li class="chapter" data-level="5.1" data-path="xgboost.html"><a href="xgboost.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#visualizing-the-decision-boundary"><i class="fa fa-check"></i><b>6.1</b> Visualizing the decision boundary</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> The logistic regression</a></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluating-the-fit"><i class="fa fa-check"></i><b>6.3</b> Evaluating the fit</a></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#comparing-it-to-test-data"><i class="fa fa-check"></i><b>6.4</b> Comparing it to test data</a></li>
<li class="chapter" data-level="6.5" data-path="logistic-regression.html"><a href="logistic-regression.html#how-well-did-it-do-with-nebraska"><i class="fa fa-check"></i><b>6.5</b> How well did it do with Nebraska?</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Sports Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees-and-random-forests" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Decision trees and random forests</h1>
<p>Tree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less seasons of experience? Do they have more or less minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have.</p>
<p>The upside of decision trees is that if the model is small, you can explain it to anyone. They’re very easy to understand. The trouble with decision trees is that if the model is small, they’re a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree.</p>
<p>The most common is the random forest.</p>
<p>Let’s implement one. We start with libraries.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="decision-trees-and-random-forests.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb69-2"><a href="decision-trees-and-random-forests.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span></code></pre></div>
<p>We’ll be using college basketball games again.</p>
<pre><p><strong>For this walkthrough:</strong></p><p><a href="http://mattwaite.github.io/sportsdatafiles/cbblogs1521.csv">
  <button class="btn btn-danger"><i class="fa fa-save"></i> Download csv file</button>
</a></p></pre>
<p>Let’s load this data and add our possession metrics right away.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="decision-trees-and-random-forests.html#cb70-1" aria-hidden="true" tabindex="-1"></a>games <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/cbblogs1521.csv&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb70-2"><a href="decision-trees-and-random-forests.html#cb70-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(TeamFGPCT <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb70-3"><a href="decision-trees-and-random-forests.html#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb70-4"><a href="decision-trees-and-random-forests.html#cb70-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">TeamPossessions =</span> TeamFGA <span class="sc">-</span> TeamOffRebounds <span class="sc">+</span> TeamTurnovers <span class="sc">+</span> (.<span class="dv">475</span> <span class="sc">*</span> TeamFTA), </span>
<span id="cb70-5"><a href="decision-trees-and-random-forests.html#cb70-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">TeamPPP =</span> TeamScore<span class="sc">/</span>TeamPossessions</span>
<span id="cb70-6"><a href="decision-trees-and-random-forests.html#cb70-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   .default = col_double(),
##   Season = col_character(),
##   Date = col_date(format = &quot;&quot;),
##   TeamFull = col_character(),
##   Opponent = col_character(),
##   HomeAway = col_character(),
##   W_L = col_character(),
##   URL = col_character(),
##   Conference = col_character(),
##   Team = col_character()
## )
## ℹ Use `spec()` for the full column specifications.</code></pre>
<p>More often than not, we need to do more than just use the data we have. Often, with modeling, we need to pre-process our data. Pre-processing can mean a lot of things – fixing dates, creating new features, scaling numbers to be similar – but it’s all about making your models better.</p>
<div id="an-intro-to-pre-processing" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> An intro to pre-processing</h2>
<p>To simplify things, we’re going to first simplify our data. We want to start with a minimum of columns. We need the columns to help us identify individual records, we need our predictors and we need the outcome we’re trying to predict.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="decision-trees-and-random-forests.html#cb72-1" aria-hidden="true" tabindex="-1"></a>modelgames <span class="ot">&lt;-</span> games <span class="sc">%&gt;%</span></span>
<span id="cb72-2"><a href="decision-trees-and-random-forests.html#cb72-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Team, Opponent, Date, Season, TeamScore, TeamPossessions, TeamPPP)</span></code></pre></div>
<p>Now we need to split our data into training and testing sets.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="decision-trees-and-random-forests.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb73-2"><a href="decision-trees-and-random-forests.html#cb73-2" aria-hidden="true" tabindex="-1"></a>game_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(modelgames, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb73-3"><a href="decision-trees-and-random-forests.html#cb73-3" aria-hidden="true" tabindex="-1"></a>game_train <span class="ot">&lt;-</span> <span class="fu">training</span>(game_split)</span>
<span id="cb73-4"><a href="decision-trees-and-random-forests.html#cb73-4" aria-hidden="true" tabindex="-1"></a>game_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(game_split)</span></code></pre></div>
<p>In simple cases, this might be enough. If the data we’re looking at is on the scale, it probably would be enough. But here we have possessions – a counting stat – and points per possession, a ratio.</p>
<p>Going forward, we’re going to make our lives easier by using workflows. Workflows in tidymodels take in a pre-processing recipe and a model definition and executes those things to make our modeling code slimmer and our lives easier.</p>
<p>To start, we need to define a pre-processsing recipe. The recipe defines a series of steps that will be performed on your data. We’ll start simple and add our formula from previous work.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="decision-trees-and-random-forests.html#cb74-1" aria-hidden="true" tabindex="-1"></a>score_rec <span class="ot">&lt;-</span> </span>
<span id="cb74-2"><a href="decision-trees-and-random-forests.html#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(TeamScore <span class="sc">~</span> TeamPossessions <span class="sc">+</span> TeamPPP, <span class="at">data =</span> game_train)</span></code></pre></div>
<p>Another, more flexible way to express this, is using the . to say all predictors. In this case, all predictors is TeamPossessions and TeamPPP. What follows is the same as above, just less typing.</p>
<p>But we’re also going to add a role to our recipe. In this case, the role is how we’re going to identify each row – an ID. In this case, to identify a game, we need to know the Team, the Opponent, the Date and the Season.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="decision-trees-and-random-forests.html#cb75-1" aria-hidden="true" tabindex="-1"></a>score_rec <span class="ot">&lt;-</span> </span>
<span id="cb75-2"><a href="decision-trees-and-random-forests.html#cb75-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(TeamScore <span class="sc">~</span> ., <span class="at">data =</span> game_train) <span class="sc">%&gt;%</span></span>
<span id="cb75-3"><a href="decision-trees-and-random-forests.html#cb75-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_role</span>(Team, Opponent, Date, Season, <span class="at">new_role =</span> <span class="st">&quot;ID&quot;</span>)</span></code></pre></div>
<p>Now, we’re going to use steps to transform our data. Some models are affected by data being on different scales, so we might have to normalize them. Here’s how to do that:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="decision-trees-and-random-forests.html#cb76-1" aria-hidden="true" tabindex="-1"></a>score_rec <span class="ot">&lt;-</span> </span>
<span id="cb76-2"><a href="decision-trees-and-random-forests.html#cb76-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">recipe</span>(TeamScore <span class="sc">~</span> ., <span class="at">data =</span> game_train) <span class="sc">%&gt;%</span></span>
<span id="cb76-3"><a href="decision-trees-and-random-forests.html#cb76-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_role</span>(Team, Opponent, Date, Season, <span class="at">new_role =</span> <span class="st">&quot;ID&quot;</span>)</span></code></pre></div>
<p>Now that we’ve created our pre-processing recipe, we can create our model definition.</p>
</div>
<div id="decision-trees" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Decision trees</h2>
<p>As discussed earlier, decision trees are essentially a series of if/else statements. Visualized, they look like branches on a tree (thus, decision trees).</p>
<p>We’ve already defined a recipe for our data, so now we’re ready to define a model definition. First, we’ll use decision trees to prove a point.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="decision-trees-and-random-forests.html#cb77-1" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>() <span class="sc">%&gt;%</span></span>
<span id="cb77-2"><a href="decision-trees-and-random-forests.html#cb77-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb77-3"><a href="decision-trees-and-random-forests.html#cb77-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<p>Now we’ll create the workflow. In its simplest form, the workflow defines itself as a workflow and then adds a recipe and a model definition.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="decision-trees-and-random-forests.html#cb78-1" aria-hidden="true" tabindex="-1"></a> tree_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb78-2"><a href="decision-trees-and-random-forests.html#cb78-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(score_rec) <span class="sc">%&gt;%</span></span>
<span id="cb78-3"><a href="decision-trees-and-random-forests.html#cb78-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(tree)</span></code></pre></div>
<p>Now we can fit the data with our model using the workflow. This applies our recipe to the data without us having to do it, then uses the model definition to do the fitting.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="decision-trees-and-random-forests.html#cb79-1" aria-hidden="true" tabindex="-1"></a>tree_fit <span class="ot">&lt;-</span> </span>
<span id="cb79-2"><a href="decision-trees-and-random-forests.html#cb79-2" aria-hidden="true" tabindex="-1"></a>  tree_wf <span class="sc">%&gt;%</span> </span>
<span id="cb79-3"><a href="decision-trees-and-random-forests.html#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> game_train)</span></code></pre></div>
<p>What does this produce? Here’s what a basic decision tree looks like.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="decision-trees-and-random-forests.html#cb80-1" aria-hidden="true" tabindex="-1"></a>tree_fit <span class="sc">%&gt;%</span> </span>
<span id="cb80-2"><a href="decision-trees-and-random-forests.html#cb80-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_workflow_fit</span>() </span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  147ms 
## n= 51254 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 51254 8894038.00  71.32136  
##    2) TeamPPP&lt; 0.9996455 24589 1936661.00  62.09203  
##      4) TeamPPP&lt; 0.8667459 9442  467747.90  55.08494  
##        8) TeamPPP&lt; 0.758596 2750   96392.41  48.67382 *
##        9) TeamPPP&gt;=0.758596 6692  211874.50  57.71952  
##         18) TeamPossessions&lt; 71.1375 3819   50554.05  54.16968 *
##         19) TeamPossessions&gt;=71.1375 2873   49225.28  62.43822 *
##      5) TeamPPP&gt;=0.8667459 15147  716330.50  66.45996  
##       10) TeamPossessions&lt; 72.0375 9076  170270.50  62.41836 *
##       11) TeamPossessions&gt;=72.0375 6071  176173.70  72.50206 *
##    3) TeamPPP&gt;=0.9996455 26665 2931439.00  79.83214  
##      6) TeamPossessions&lt; 72.7125 17175 1027871.00  75.35470  
##       12) TeamPPP&lt; 1.142428 10801  267742.70  71.40117  
##         24) TeamPossessions&lt; 66.1625 4151   62736.49  67.08432 *
##         25) TeamPossessions&gt;=66.1625 6650   79365.98  74.09579 *
##       13) TeamPPP&gt;=1.142428 6374  305224.30  82.05413  
##         26) TeamPossessions&lt; 66.4875 2753   92280.17  77.50527 *
##         27) TeamPossessions&gt;=66.4875 3621  112668.70  85.51257 *
##      7) TeamPossessions&gt;=72.7125 9490  936111.40  87.93541  
##       14) TeamPPP&lt; 1.163417 6629  252017.30  83.51727  
##         28) TeamPossessions&lt; 80.3125 5124   84955.94  81.29801 *
##         29) TeamPossessions&gt;=80.3125 1505   55903.96  91.07309 *
##       15) TeamPPP&gt;=1.163417 2861  254880.00  98.17232  
##         30) TeamPPP&lt; 1.293652 2156   90844.47  94.83952 *
##         31) TeamPPP&gt;=1.293652 705   66851.31 108.36450 *</code></pre>
<p>They can be a bit tough to read, but take the bottom three nodes, starting with 15. It says if TeamPPP is greater than or equal to 1.01 – you’re scoring more than a point per possession – your score is around 98 points a game. But the terminal nodes – the actual decisions – within this branch – say if it’s less than 1.97 per possession, your score is 95.83, but if it’s greater or equal to 1.97 – and there’s 511 games where teams were scoring nearly a bucket every trip up the floor – their score is 110.47.</p>
<p>Easy to understand, right? The algorithm cuts branches when the splits stop reducing error, and there’s a limit But here’s where the crude instrument comes in.</p>
<p>Let’s use our decision tree to predict some scores.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="decision-trees-and-random-forests.html#cb82-1" aria-hidden="true" tabindex="-1"></a>treeresults <span class="ot">&lt;-</span> game_train <span class="sc">%&gt;%</span></span>
<span id="cb82-2"><a href="decision-trees-and-random-forests.html#cb82-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(tree_fit, game_train))</span></code></pre></div>
<p>What are the accuracy metrics we get?</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="decision-trees-and-random-forests.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(treeresults, <span class="at">truth =</span> TeamScore, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       4.81 
## 2 rsq     standard       0.866
## 3 mae     standard       3.69</code></pre>
<p>Our rsquared is .8668, which would under most circumstances would be great. And our RSME says were off by almost five points on average, which is a little worse than our multiple regression analysis.</p>
<p>We can do better.</p>
</div>
<div id="random-forest" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Random forest</h2>
<p>Enter the random forest. A random forest is, as the name implies, a large number of decision trees, and they use a random set of inputs. The trees all make predictions, and the wisdom of the crowds takes over. In the case of classification algorithm, the most common prediction is the one that gets chosen. In a regression model, the predictions get averaged together.</p>
<p>The random part of random forest is in how the number of tree splits get created and how the samples from the data are taken to generate the splits. They’re randomized, which has the effect of limiting the influence of a particular feature and prevents overfitting.</p>
<p>For random forests, we change the model type to rand_forest and set the engine to “ranger.” There’s multiple implementations of the random forest algorithm, and the differences between them are beyond the scope of what we’re doing here.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="decision-trees-and-random-forests.html#cb85-1" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>() <span class="sc">%&gt;%</span></span>
<span id="cb85-2"><a href="decision-trees-and-random-forests.html#cb85-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb85-3"><a href="decision-trees-and-random-forests.html#cb85-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<p>And now we can create our workflow. We first need to define it as a workflow, then add the model and add the recipe.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="decision-trees-and-random-forests.html#cb86-1" aria-hidden="true" tabindex="-1"></a>score_wflow <span class="ot">&lt;-</span> </span>
<span id="cb86-2"><a href="decision-trees-and-random-forests.html#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb86-3"><a href="decision-trees-and-random-forests.html#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(score_rec) <span class="sc">%&gt;%</span> </span>
<span id="cb86-4"><a href="decision-trees-and-random-forests.html#cb86-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(rf_mod)</span>
<span id="cb86-5"><a href="decision-trees-and-random-forests.html#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="decision-trees-and-random-forests.html#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="decision-trees-and-random-forests.html#cb86-7" aria-hidden="true" tabindex="-1"></a>score_wflow</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: rand_forest()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 0 Recipe Steps
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Random Forest Model Specification (regression)
## 
## Computational engine: ranger</code></pre>
<p>With the workflow in place, we can fit our model.</p>
<p>Note: this can make your laptop fan go wheeeeee.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="decision-trees-and-random-forests.html#cb88-1" aria-hidden="true" tabindex="-1"></a>score_fit <span class="ot">&lt;-</span> </span>
<span id="cb88-2"><a href="decision-trees-and-random-forests.html#cb88-2" aria-hidden="true" tabindex="-1"></a>  score_wflow <span class="sc">%&gt;%</span> </span>
<span id="cb88-3"><a href="decision-trees-and-random-forests.html#cb88-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> game_train)</span></code></pre></div>
<p>Now we can use a use a new function – pull_workflow_fit, which pulls the fit stats we want to see to evaluate it.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="decision-trees-and-random-forests.html#cb89-1" aria-hidden="true" tabindex="-1"></a>score_fit <span class="sc">%&gt;%</span> </span>
<span id="cb89-2"><a href="decision-trees-and-random-forests.html#cb89-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_workflow_fit</span>() </span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  16s 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      51254 
## Number of independent variables:  2 
## Mtry:                             1 
## Target node size:                 5 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.1402611 
## R squared (OOB):                  0.9991917</code></pre>
<p>Similar to previous work, we can bind the prediction to our training data and evaluate the model.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="decision-trees-and-random-forests.html#cb91-1" aria-hidden="true" tabindex="-1"></a>trainresults <span class="ot">&lt;-</span> game_train <span class="sc">%&gt;%</span></span>
<span id="cb91-2"><a href="decision-trees-and-random-forests.html#cb91-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(score_fit, game_train))</span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="decision-trees-and-random-forests.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(trainresults, <span class="at">truth =</span> TeamScore, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      0.192 
## 2 rsq     standard      1.00  
## 3 mae     standard      0.0518</code></pre>
<p>Note: The RMSE for this model is down to .2. The R-squared is absurdly high.</p>
<p>But how does this model handle data it hasn’t seen before?</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="decision-trees-and-random-forests.html#cb94-1" aria-hidden="true" tabindex="-1"></a>testresults <span class="ot">&lt;-</span> game_test <span class="sc">%&gt;%</span></span>
<span id="cb94-2"><a href="decision-trees-and-random-forests.html#cb94-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(score_fit, game_test))</span></code></pre></div>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="decision-trees-and-random-forests.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span>(testresults, <span class="at">truth =</span> TeamScore, <span class="at">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      0.343 
## 2 rsq     standard      0.999 
## 3 mae     standard      0.0786</code></pre>
<p>How well does the random forest algorithm do with Nebraska’s schedule in 2020-2021 prior to a month-long COVID break?</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="decision-trees-and-random-forests.html#cb97-1" aria-hidden="true" tabindex="-1"></a>nu <span class="ot">&lt;-</span> games <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Season <span class="sc">==</span> <span class="st">&quot;2020-2021&quot;</span>, Team <span class="sc">==</span> <span class="st">&quot;Nebraska&quot;</span>)</span>
<span id="cb97-2"><a href="decision-trees-and-random-forests.html#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="decision-trees-and-random-forests.html#cb97-3" aria-hidden="true" tabindex="-1"></a>nupreds <span class="ot">&lt;-</span> nu <span class="sc">%&gt;%</span></span>
<span id="cb97-4"><a href="decision-trees-and-random-forests.html#cb97-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(<span class="fu">predict</span>(score_fit, nu))</span>
<span id="cb97-5"><a href="decision-trees-and-random-forests.html#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="decision-trees-and-random-forests.html#cb97-6" aria-hidden="true" tabindex="-1"></a>nupreds <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">Residual =</span> TeamScore <span class="sc">-</span> .pred) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">desc</span>(Residual)) <span class="sc">%&gt;%</span> <span class="fu">select</span>(Team, Opponent, TeamScore, .pred, Residual)</span></code></pre></div>
<pre><code>## # A tibble: 24 x 5
##    Team     Opponent      TeamScore .pred Residual
##    &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 Nebraska Doane College       110 110.   0.210  
##  2 Nebraska Illinois             70  69.9  0.119  
##  3 Nebraska Ohio State           54  53.9  0.107  
##  4 Nebraska Maryland             50  49.9  0.0671 
##  5 Nebraska Penn State           83  82.9  0.0558 
##  6 Nebraska Minnesota            78  78.0  0.0133 
##  7 Nebraska Michigan             69  69.0  0.00554
##  8 Nebraska Creighton            74  74.0  0.00550
##  9 Nebraska Wisconsin            53  53.0  0.00496
## 10 Nebraska Georgia Tech         64  64.0  0.00410
## # … with 14 more rows</code></pre>
<p>Compare this to the multiple regression of the previous chapter. Take just the Doane game as an example. The multiple regression model was off by three points – which is really good when you think about it. But the random forest managed to be a tenth of that off. It’s off by just a third of a point. It can’t get much better than that.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-regression-and-feature-engineering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="xgboost.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-randomforest.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["advancedsportsdataanalysis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
