[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism With R and the Tidyverse",
    "section": "",
    "text": "1 Introduction\nIf you were at all paying attention in pre-college science classes, you have probably seen this equation:\nIn English, that says we can know how far something has travelled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra."
  },
  {
    "objectID": "index.html#modern-data-journalism",
    "href": "index.html#modern-data-journalism",
    "title": "Data Journalism With R and the Tidyverse",
    "section": "1.1 Modern data journalism",
    "text": "1.1 Modern data journalism\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform."
  },
  {
    "objectID": "index.html#installations",
    "href": "index.html#installations",
    "title": "Data Journalism With R and the Tidyverse",
    "section": "1.2 Installations",
    "text": "1.2 Installations\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Data Journalism With R and the Tidyverse",
    "section": "1.3 About this book",
    "text": "1.3 About this book\nThis book is the collection of class materials for the author’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the author is copyright Matt Waite 2024.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Bookdown – is open sourced on Github. Pull requests welcomed!"
  },
  {
    "objectID": "index.html#what-well-cover",
    "href": "index.html#what-well-cover",
    "title": "Data Journalism With R and the Tidyverse",
    "section": "1.4 What we’ll cover",
    "text": "1.4 What we’ll cover\n\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nIntermediate data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nText analysis\nAdvanced analysis: Correlations and regressions\nAdvanced analysis: Logistic regression\nWriting with and about data\nData journalism ethics"
  },
  {
    "objectID": "publicrecords.html#federal-law",
    "href": "publicrecords.html#federal-law",
    "title": "2  Public records",
    "section": "2.1 Federal law",
    "text": "2.1 Federal law\nYour access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking.\nThe Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.\nWhy?\n\nThere is no real timetable with FOIA. Requests can take months, even years.\nAs a journalist, you can ask that your request be expedited.\nGuess what? That requires review. More delays.\nExemptions are broad. National security, personal privacy, often overused.\nDenied? You can appeal. More delays.\n\nThe law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.\nPost 9/11, the Bush administration rolled back many agency rules. Obama ordered a “presumption of openness” but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.\nResult? FOIA is in trouble.\nSPJ is a good resource."
  },
  {
    "objectID": "publicrecords.html#state-law",
    "href": "publicrecords.html#state-law",
    "title": "2  Public records",
    "section": "2.2 State law",
    "text": "2.2 State law\nStates are – generally – more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not.\nNebraska? Somewhere in the middle. Better than some, worse than others.\nUnder Nebraska law:\n\nYou are entitled to see and copy public records.\nThey can charge you a fee for those copies.\nThey do not have to give you records in a format different from what they keep them in.\n\nWith a written request, Nebraska public officials have four days to respond. If Nebraska public officials deny your request, they must do so in writing specifying their reasons. If it will take more than four days, they must tell you, in writing, why and how long it will take.\nThe Reporters Committee For Freedom of the Press is a good resource.\nPlease and thank you will get you more records than any lawyer or well written request. When requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.\nBe. Nice.\nHunting for records is like any other kind of reporting – you have to do research. You have to ask questions. What records do you keep? For how long?\nA good source of info? Records retention schedules, often required by law or administrative rule at an agency. Nebraska’s is particularly good.\nFor an example, let’s say we wanted to look at nursing homes in Nebraska, because they’re closing down quickly. If we just wanted a data set of licensed nursing homes in the state, our options online are … not good. The Department of Health and Human Services publishes a list online, but it’s a horribly formatted PDF.\nBut what does the agency keep? Looking at DHHS’s public health departments records retention schedule, you’ll find a lot more. And that opens doors for public records requests."
  },
  {
    "objectID": "basics.html#adding-libraries-part-1",
    "href": "basics.html#adding-libraries-part-1",
    "title": "3  R basics",
    "section": "3.1 Adding libraries, part 1",
    "text": "3.1 Adding libraries, part 1\nThe real strength of any given programming language is the external libraries that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nFor this class, we’re going to need several external libraries.\nThe first library we’re going to use is called Swirl. So in the console, type install.packages('swirl') and hit enter. That installs swirl.\nNow, to use the library, type library(swirl) and hit enter. That loads swirl. Then type swirl() and hit enter. Now you’re running swirl. Follow the directions on the screen. When you are asked, you want to install course 1 R Programming: The basics of programming in R. Then, when asked, you want to do option 1, R Programming, in that course.\nWhen you are finished with the course – it will take just a few minutes – type 0 to exit (it will not be clear that’s what you do when you are done)."
  },
  {
    "objectID": "basics.html#adding-libraries-part-2",
    "href": "basics.html#adding-libraries-part-2",
    "title": "3  R basics",
    "section": "3.2 Adding libraries, part 2",
    "text": "3.2 Adding libraries, part 2\nWe’ll mostly use two libraries for analysis – dplyr and ggplot2. To get them, and several other useful libraries, we can install a single collection of libraries called the tidyverse. Type this into your console: install.packages('tidyverse')\nNOTE: This is a pattern. You should always install libraries in the console.\nThen, to help us with learning and replication, we’re going to use R Notebooks. So we need to install that library. Type this into your console: install.packages('rmarkdown')"
  },
  {
    "objectID": "replication.html#the-stylebook",
    "href": "replication.html#the-stylebook",
    "title": "4  Data journalism in the age of replication",
    "section": "4.1 The stylebook",
    "text": "4.1 The stylebook\nTroy Thibodeaux, the editor of the AP’s data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.\nFrom the beginning, they had a fairly clear idea of what they wanted to do – think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.\nWhen the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.\n“From our perspective, this is a core value for us,” he said. “Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other.”\nThibodeaux said the AP data team has two audiences when it comes to replication – they have the readers of the work, and members of the collective who may want to do their own work with the data.\n“This is something that’s essential to the way we work,” he said. “And it’s important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable.”"
  },
  {
    "objectID": "replication.html#replication",
    "href": "replication.html#replication",
    "title": "4  Data journalism in the age of replication",
    "section": "4.2 Replication",
    "text": "4.2 Replication\nMeyer, now 86, said he’s delighted to see replication up for discussion now, but warned that we shouldn’t take it too far.\n“Making the analysis replicable was something I worried about from the very beginning,” he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.\nAnd, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer’s analysis by slicing the data ever thinner until the differences weren’t significant – gaming the analysis to criticize the stories.\nMeyer believes replication is vitally important, but doesn’t believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.\nBut journalists should be sharing their data and analysis steps. And it doesn’t need to be complicated, he said.\n“Replication is a theoretical standard, not a requirement that every investigator duplicate his or her own work for every project,” he said. “Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence.”\nBut as simple as that sounds, it’s not so simple. Ask social scientists.\nAndrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE in February that difficulties with replication in empirical research are pervasive.\n“When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there,” Gelman wrote. “End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science.”\nSo goes science, so goes journalism.\nUntil a recent set of exceptions, journalists rarely shared data. The “nerd box” – a sidebar story that explains how a news organization did what they did – is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.\nIt was a form born in print.\nAs newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.\nJournalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.\nEnter the AP Stylebook.\nThe AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the “methodology description in the story or accompanying materials should provide a road map to replicate the analysis”, meaning someone else could do the replication post publication.\nInternally, the AP Stylebook says: “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”\nThere are two problems here.\nFirst is that journalism, unlike science, has no history of replication. There is no Scientific Method for stories. There is no Research Methods class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn’t a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.\nThe second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don’t have a second person who would know what to do.\nNot having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team “is an incredible luxury” at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.\nThibodeaux, for his part, wants to see fewer “lone nerds in the corner” – it’s too much pressure. That person gets too much credibility from people who don’t understand what they do, and they get too much blame when a mistake is made.\nSo what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline? And what does this mean for how data journalism is being taught, particularly at a time when only half of accredited journalism programs teach any data journalism at all?\nAre we walking ourselves into our own replication crisis?"
  },
  {
    "objectID": "replication.html#goodbye-excel",
    "href": "replication.html#goodbye-excel",
    "title": "4  Data journalism in the age of replication",
    "section": "4.3 Goodbye Excel?",
    "text": "4.3 Goodbye Excel?\nFor decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the One Tool You Can’t Live Without. Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.\nThe Stylebook says at a minimum, today’s data journalists should keep a log that details:\n\nThe source of the data, making sure to work on a copy of the data and not the original file.\nData dictionaries or any other supporting documentation of the data.\n“Description of all steps required to transform the data and perform the analysis.”\n\nThe trouble with Excel is, unless you are keeping meticulous notes on what steps you are taking, there’s no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around – a practical step that also cuts off another path to being able to replicate the results.\nAn increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.\nCombined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the Los Angeles Times, the New York Times, FiveThirtyEight, and Buzzfeed.\nPeter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to find airplanes being used to spy on people in American cities. Published with the story is the code Aldous used to build his case.\n“I think of it this way: As a journalist, I don’t like to simply trust what people tell me. Sometimes sources lie. Sometimes they’re just mistaken. So I like to verify what I’m told,” he wrote in an email. “By the same token, why should someone reading one of my articles believe my conclusions, if I don’t provide the evidence that explains how I reached them?”\nThe methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn’t a discussion about if the methodology would be published because it was assumed – “it’s written into our DNA at BuzzFeed News.”\n“My background is in science journalism, and before that (way back in the 1980s) in science,” Aldous said. “In science, there’s been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we’re seeing a similar shift in data journalism. Simply saying what you’ve done is not as powerful as providing the means for others to repeat and build on your work.”\nThibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is “lovely.”\n“That to me is the shining city on the hill,” Thibodeaux said. “We’re not going to get there, and I don’t think we have to for every story and every use case, and I don’t think it’s necessarily practical for every person working with data to get to that point.”\nThere’s a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn’t going to advocate for any single one of them, he said. They’re just arguing for transparency and replicability, even if that means doing more work.\n“There’s a certain burden that comes with transparency,” he said. “And I think we have to accept that burden.”\nThe question, Thibodeaux said, is what is sufficient? What’s enough transparency? What does someone need for replicability?\n“Maybe we do have to set a higher standard – the more critical the analysis is to the story, and the more complex that analysis is, that’s going to push the bar on what is a sufficient methodology statement,” he said. “And it could end up being a whole code repo in order to just say, this isn’t black magic, here’s how we got it if you’re so interested.”"
  },
  {
    "objectID": "replication.html#receptivity-is-high",
    "href": "replication.html#receptivity-is-high",
    "title": "4  Data journalism in the age of replication",
    "section": "4.4 “Receptivity … is high”",
    "text": "4.4 “Receptivity … is high”\nThough written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.\n“For the new methods to gain currency in journalism, two things must happen,” he wrote. “Editors must feel the need strongly enough to develop the in-house capacity for systematic research … The second need, of course, is for the editors to be able to find the talent to fill this need.”\nMeyer optimistically wrote that journalism schools were prepared to provide that talent – they were not then, and only small handful are now – but students were unlikely to be drawn to these new skills if they didn’t see a chance to use those skills in their careers.\nIt’s taken 45 years, but we are now at this point.\n“The potential for receptivity, especially among the younger generation of newspaper managers, is high,” Meyer wrote."
  },
  {
    "objectID": "replication.html#replication-in-notebooks",
    "href": "replication.html#replication-in-notebooks",
    "title": "4  Data journalism in the age of replication",
    "section": "4.5 Replication in notebooks",
    "text": "4.5 Replication in notebooks\nFor our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.\nIn an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio, including Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.\nFor the rest of the class, we’re going to be working in notebooks. In notebooks, you will both run your code and explain each step, much as I am doing here.\nTo start a notebook, you click on the green plus in the top left corner and go down to R Notebook. Do that now.\n\n\n\n\n\nYou will see that the notebook adds a lot of text for you. It tells you how to work in notebooks – and you should read it. The most important parts are these:\nTo add text, simply type. To add code you can click on the Insert button on the toolbar or by pressing Cmd+Option+I on Mac or Ctl+Alt+I on Windows.\nHighlight all that text and delete it. You should have a blank document. This document is called a R Markdown file – it’s a special form of text, one that you can style, and one you can include R in the middle of it. Markdown is a simple markup format that you can use to create documents. So first things first, let’s give our notebook a big headline. Add this:\n# My awesome notebook\nNow, under that, without any markup, just type This is my awesome notebook.\nUnder that, you can make text bold by writing It is **really** awesome.\nIf you want it italics, just do this on the next line: No, it's _really_ awesome. I swear.\nTo see what it looks like without the markup, click the Preview or Knit button in the toolbar. That will turn your notebook into a webpage, with the formatting included.\nThroughout this book, we’re going to use this markdown to explain what we are doing and, more importantly, why we are doing it. Explaining your thinking is a vital part of understanding what you are doing.\nThat explaination, plus the code, is the real power of notebooks. To add a block of code, follow the instructions from above: click on the Insert button on the toolbar or by pressing Cmd+Option+I on Mac or Ctl+Alt+I on Windows.\nIn that window, use some of the code from above and add two numbers together. To see it run, click the green triangle on the right. That runs the chunk. You should see the answer to your addition problem.\nAnd that, just that, is the foundation you need to start this book."
  },
  {
    "objectID": "databasics.html#rows-and-columns",
    "href": "databasics.html#rows-and-columns",
    "title": "5  Data, structures and types",
    "section": "5.1 Rows and columns",
    "text": "5.1 Rows and columns\nData, oversimplifying it a bit, is information organized. Generally speaking, it’s organized into rows and columns. Rows, generally, are individual elements. A crime. A county. An accident. Columns, generally, are components of the data, sometimes called variables. So if each row is a crime, the first column might be the type. The second is the date and time. The third is the location. And so on.\n\n\n\n\n\nOne of the critical components of data analysis, especially for beginners, is having a mental picture of your data. What does each row mean? What does each column in each row signify? How many rows do you have? How many columns?\n\nEXERCISE: I love orange Skittles. What are my chances of getting more orange Skittles than other colors in a fun sized packet? Each person in the class must track their package and everyone else using a spreadsheet. What differences between sheets emerge? What similarities?"
  },
  {
    "objectID": "databasics.html#types",
    "href": "databasics.html#types",
    "title": "5  Data, structures and types",
    "section": "5.2 Types",
    "text": "5.2 Types\nThere are scores of data types in the world, and R has them. In this class, we’re primarily going to be dealing with data frames, and each element of our data frames will have a data type.\nTypically, they’ll be one of four types of data:\n\nNumeric: a number, like the number of car accidents in a year or the number of journalism majors.\nCharacter: Text, like a name, a county, a state.\nDate: Fully formed dates – 2019-01-01 – have a special date type. Elements of a date, like a year (ex. 2019) are not technically dates, so they’ll appear as numeric data types.\nLogical: Rare(ish), but every now and then we’ll have a data type that’s Yes or No, True or False, etc.\n\nQuestion: Is a zip code a number? Is a jersey number a number? Trick question, because the answer is no. Numbers are things we do math on. If the thing you want is not something you’re going to do math on – can you add two phone numbers together? – then make it a character type. If you don’t, most every software system on the planet will drop leading zeros. For example, every zip code in Boston starts with 0. If you record that as a number, your zip code will become a four digit number, which isn’t a zip code anymore."
  },
  {
    "objectID": "databasics.html#a-simple-way-to-get-data",
    "href": "databasics.html#a-simple-way-to-get-data",
    "title": "5  Data, structures and types",
    "section": "5.3 A simple way to get data",
    "text": "5.3 A simple way to get data\nThe hardest part of doing data journalism is often getting the data. In news, there’s scores of organizations and agencies collecting data, and zero standards on how it’s being collected.\nIf we’re lucky – huge IF in news – the data we want is in a downloadable format. If we’re a little less lucky, there’s a way to get the data on the web. And maybe that data is in a simple table. If so, we can pull that data directly into Google Sheets.\nThe Lincoln Police Department publishes a daily summary of calls. Some days – like when it snows – that data becomes news. So let’s pretend that it snowed today and we need to see how many accidents the Lincoln Police responded to and what percentage of their call load that represents.\nOpen a browser and go to the LPD’s log page. Now, in a new tab, log into Google Docs/Drive and open a new spreadsheet. In the first cell of the first row, copy and paste this formula in:\n=importHTML(\"http://cjis.lincoln.ne.gov/~lpd/cfstoday.htm\",\"table\",1)\nThis is function, with three inputs. The function is importHTML and the three inputs in order are the url of the page, the HTML tag we’re after (a\n\ntag in our case) and the number of the tag you’re after. So our function says go to the LPD page and get the first table tag you find. Fortunately for us, there’s only one.If your version worked right, you’ve got the data from that page in a spreadsheet.\n5.4 Cleaning the data\nThe first thing we need to do is recognize that we don’t have data, really. We have the results of a formula. You can tell by putting your cursor on that field, where you’ll see the formula again. This is where you’d look:\n\n\n\n\n\nThe solution is easy:\nEdit > Select All or type command/control A Edit > Copy or type command/control C Edit > Paste Special > Values Only or type command/control shift V\nYou can verify that it worked by looking in that same row 1 column A, where you’ll see the formula is gone.\n\n\n\n\n\nNow you have data, but look closely. At the bottom of the data, you have the total number of calls. More often than not, and particularly the deeper into this book we go, you want to delete that. So click on the number next to that Total Calls line to highlight it and go up to Edit > Delete Row XX where XX is the row number.\nAfter you’ve done that, you can export it for use in R. Go to File > Download as > Comma Separated Values.\n\n\n\n  \n      \n         4  Data journalism in the age of replication\n                \n  \n  \n      \n        6  Aggregates"
  },
  {
    "objectID": "databasics.html#cleaning-the-data",
    "href": "databasics.html#cleaning-the-data",
    "title": "5  Data, structures and types",
    "section": "5.4 Cleaning the data",
    "text": "5.4 Cleaning the data\nThe first thing we need to do is recognize that we don’t have data, really. We have the results of a formula. You can tell by putting your cursor on that field, where you’ll see the formula again. This is where you’d look:\n\n\n\n\n\nThe solution is easy:\nEdit > Select All or type command/control A Edit > Copy or type command/control C Edit > Paste Special > Values Only or type command/control shift V\nYou can verify that it worked by looking in that same row 1 column A, where you’ll see the formula is gone.\n\n\n\n\n\nNow you have data, but look closely. At the bottom of the data, you have the total number of calls. More often than not, and particularly the deeper into this book we go, you want to delete that. So click on the number next to that Total Calls line to highlight it and go up to Edit > Delete Row XX where XX is the row number.\nAfter you’ve done that, you can export it for use in R. Go to File > Download as > Comma Separated Values."
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "6  Aggregates",
    "section": "6.1 Importing data",
    "text": "6.1 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read data from a csv file – a comma-separated values file.\nThe CSV file we’re going to read from is a Nebraska Game and Parks Commission dataset of confirmed mountain lion sightings in Nebraska. There are, on occasion, fierce debates about mountain lions and if they should be hunted in Nebraska. This dataset can tell us some interesting things about that debate.\nSo step 1 is to import the data. The code looks something like this, but hold off copying it just yet:\nmountainlions <- read_csv(\"~/Documents/Data/mountainlions.csv\")\nLet’s unpack that.\nThe first part – mountainlions – is the name of your variable. A variable is just a name of a thing. In this case, our variable is a dataframe, which is R’s way of storing data. We can call this whatever we want. I always want to name dataframes after what is in it. In this case, we’re going to import a dataset of mountain lion sightings from the Nebraska Game and Parks Commission. Variable names, by convention are one word all lower case. You can end a variable with a number, but you can’t start one with a number.\nThe <- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called mountainlions and stuffing all this data into it.\nThe read_csv bits are pretty obvious, except for one thing. What happens in the quote marks is the path to the data. In there, I have to tell R where it will find the data.\n\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (mountainlions.csv).\n\nIn my case, the file path I’ve got starts with a ~ character. That’s a shortcut for my home directory. It’s the same on your computer. Your home directory is where your Documents, Desktop and Downloads directories are. I’ve got a folder called Documents in my home directory, and in there is a folder called Data that has the file called mountainlions.csv in it. Thus, ~/Documents/Data/mountainlions.csv\nSome people – insane people – leave the data in their downloads folder. The data path then would be ~/Downloads/nameofthedatafilehere.csv on PC or Mac.\nA quick way to find your data file? The tab key. If you start your code dataframenamehere <- read_csv(\") and after typing the first quote mark you hit tab, it will show you the files in the folder you are in. With that, you can start to narrow in on where you need to go.\n\n\nSo what you put in your import step will be different from mine. Your first task is to import the data. Here’s mine. Use the tab key to find your data file and get the correct path.\n\nmountainlions <- read_csv(\"data/mountainlions.csv\")\n\nRows: 393 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Cofirm Type, COUNTY, Date\ndbl (1): ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we can inspect the data we imported. What does it look like? To do that, we use head(mountainlions) to show the headers and the first six rows of data. If we wanted to see them all, we could just simply enter mountainlions and run it.\nTo get the number of records in our dataset, we run nrow(mountainlions)\n\nhead(mountainlions)\n\n# A tibble: 6 × 4\n     ID `Cofirm Type` COUNTY       Date    \n  <dbl> <chr>         <chr>        <chr>   \n1     1 Track         Dawes        9/14/91 \n2     2 Mortality     Sioux        11/10/91\n3     3 Mortality     Scotts Bluff 4/21/96 \n4     4 Mortality     Sioux        5/9/99  \n5     5 Mortality     Box Butte    9/29/99 \n6     6 Track         Scotts Bluff 11/12/99\n\n\n\nnrow(mountainlions)\n\n[1] 393"
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "6  Aggregates",
    "section": "6.2 Group by and count",
    "text": "6.2 Group by and count\nSo what if we wanted to know how many mountain lion sightings there were in each county?\nTo do that by hand, we’d have to take each of the 393 records and sort them into a pile. We’d put them in groups and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |>. The best way to read that operator, in my opinion, is to interpret that as “and then do this.”\nWe’re going to establish a pattern that will come up again and again throughout this book: data |> function. The first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |> group_by(FIELD NAME) |> summarize(VARIABLE NAME = AGGREGATE FUNCTION(FIELD NAME))\nHere’s the code:\n\nmountainlions |>\n  group_by(COUNTY) |>\n  summarise(\n    total = n()\n  )\n\n# A tibble: 42 × 2\n   COUNTY    total\n   <chr>     <int>\n 1 Banner        6\n 2 Blaine        3\n 3 Box Butte     4\n 4 Brown        15\n 5 Buffalo       3\n 6 Cedar         1\n 7 Cherry       30\n 8 Custer        8\n 9 Dakota        3\n10 Dawes       111\n# ℹ 32 more rows\n\n\nSo let’s walk through that. We start with our dataset – mountainlions – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the counties, signified by the field name COUNTY, which you could get from looking at head(mountainlions). After we group the data, we need to count them up. In dplyr, we use summarize which can do more than just count things. Inside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the counties: total = n(), says create a new field, called total and set it equal to n(), which might look weird, but it’s common in stats. The number of things in a dataset? Statisticians call in n. There are n number of incidents in this dataset. So n() is a function that counts the number of things there are.\nAnd when we run that, we get a list of counties with a count next to them. But it’s not in any order. So we’ll add another And Then Do This |> and use arrange. Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the county with the most mountain lion sightings, we need to sort it in descending order. That looks like this:\n\nmountainlions |>\n  group_by(COUNTY) |>\n  summarise(\n    count = n()\n  ) |> arrange(desc(count))\n\n# A tibble: 42 × 2\n   COUNTY       count\n   <chr>        <int>\n 1 Dawes          111\n 2 Sioux           52\n 3 Sheridan        35\n 4 Cherry          30\n 5 Scotts Bluff    26\n 6 Keya Paha       20\n 7 Brown           15\n 8 Rock            11\n 9 Lincoln         10\n10 Custer           8\n# ℹ 32 more rows\n\n\nWe can, if we want, group by more than one thing. So how are these sightings being confirmed? To do that, we can group by County and “Cofirm Type”, which is how the state misspelled Confirm. But note something in this example below:\n\nmountainlions |>\n  group_by(COUNTY, `Cofirm Type`) |>\n  summarise(\n    count = n()\n  ) |> arrange(desc(count))\n\n`summarise()` has grouped output by 'COUNTY'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 93 × 3\n# Groups:   COUNTY [42]\n   COUNTY       `Cofirm Type`      count\n   <chr>        <chr>              <int>\n 1 Dawes        Trail Camera Photo    41\n 2 Sioux        Trail Camera Photo    40\n 3 Dawes        Track                 19\n 4 Keya Paha    Trail Camera Photo    18\n 5 Cherry       Trail Camera Photo    17\n 6 Dawes        Mortality             17\n 7 Sheridan     Trail Camera Photo    16\n 8 Dawes        Photo                 13\n 9 Dawes        DNA                   11\n10 Scotts Bluff Trail Camera Photo    11\n# ℹ 83 more rows\n\n\nSee it? When you have a field name that has two words, readr wraps it in back ticks, which is next to the 1 key on your keyboard. You can figure out which fields have back ticks around it by looking at the output of readr. Pay attention to that, because it’s coming up again in the next section and will be a part of your homework."
  },
  {
    "objectID": "aggregates.html#other-aggregates-mean-and-median",
    "href": "aggregates.html#other-aggregates-mean-and-median",
    "title": "6  Aggregates",
    "section": "6.3 Other aggregates: Mean and median",
    "text": "6.3 Other aggregates: Mean and median\nIn the last example, we grouped some data together and counted it up, but there’s so much more you can do. You can do multiple measures in a single step as well.\nLet’s look at some salary data from the University of Nebraska.\n\nsalaries <- read_csv(\"data/nusalaries1819.csv\")\n\nRows: 13039 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Employee, Position, Campus, Department\nnum (3): Budgeted Annual Salary, Salary from State Aided Funds, Salary from ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(salaries)\n\n# A tibble: 6 × 7\n  Employee                     Position Campus Department Budgeted Annual Sala…¹\n  <chr>                        <chr>    <chr>  <chr>                       <dbl>\n1 Abbey, Bryce M               Associa… UNK    Kinesiolo…                  61276\n2 Abbott, Frances M            Staff S… UNL    FM&P Faci…                  37318\n3 Abboud, Cheryl A             Adminis… UNMC   Surgery-U…                  76400\n4 Abdalla, Maher Y             Asst Pr… UNMC   Pathology…                  74774\n5 Abdelkarim, Ahmed Mohamed A… Post-Do… UNMC   Surgery-T…                  43516\n6 Abdel-Monem, Tarik L         Researc… UNL    Public Po…                  58502\n# ℹ abbreviated name: ¹​`Budgeted Annual Salary`\n# ℹ 2 more variables: `Salary from State Aided Funds` <dbl>,\n#   `Salary from Other Funds` <dbl>\n\n\nIn summarize, we can calculate any number of measures. Here, we’ll use R’s built in mean and median functions to calculate … well, you get the idea.\n\nsalaries |>\n  summarise(\n    count = n(),\n    mean_salary = mean(`Budgeted Annual Salary`),\n    median_salary = median(`Budgeted Annual Salary`)\n  )\n\n# A tibble: 1 × 3\n  count mean_salary median_salary\n  <int>       <dbl>         <dbl>\n1 13039      62065.         51343\n\n\nSo there’s 13,039 employees in the database, spread across four campuses plus the system office. The mean or average salary is about $62,000, but the median salary is slightly more than $51,000.\nWhy?\nLet’s let sort help us.\n\nsalaries |> arrange(desc(`Budgeted Annual Salary`))\n\n# A tibble: 13,039 × 7\n   Employee             Position        Campus Department Budgeted Annual Sala…¹\n   <chr>                <chr>           <chr>  <chr>                       <dbl>\n 1 Frost, Scott A       Head Coach-Foo… UNL    Athletics                 5000000\n 2 Miles, Timothy S     Head Coach-Bas… UNL    Athletics                 2375000\n 3 Moos, William H      Athletic Direc… UNL    Athletics                 1000000\n 4 Gold, Jeffrey P      Chancellor      UNMC   Office of…                 853338\n 5 Chinander, Erik J    Assistant Coac… UNL    Athletics                  800000\n 6 Walters, Troy M      Assistant Coac… UNL    Athletics                  700000\n 7 Cook, John G         Head Coach-Vol… UNL    Athletics                  675000\n 8 Williams, Amy M      Head Coach-Wom… UNL    Athletics                  626750\n 9 Bounds, Hank M       President       UNCA   Office of…                 540000\n10 Austin Jr, Gregory D Assistant Coac… UNL    Athletics                  475000\n# ℹ 13,029 more rows\n# ℹ abbreviated name: ¹​`Budgeted Annual Salary`\n# ℹ 2 more variables: `Salary from State Aided Funds` <dbl>,\n#   `Salary from Other Funds` <dbl>\n\n\nOh, right. In this dataset, the university pays a football coach $5 million. Extremes influence averages, not medians, and now you have your answer.\nSo when choosing a measure of the middle, you have to ask yourself – could I have extremes? Because a median won’t be sensitive to extremes. It will be the point at which half the numbers are above and half are below. The average or mean will be a measure of the middle, but if you have a bunch of low paid people and then one football coach, the average will be wildly skewed. Here, because there’s so few highly paid football coaches compared to people who make a normal salary, the number is only slightly skewed in the grand scheme, but skewed nonetheless."
  },
  {
    "objectID": "aggregates.html#even-more-aggregates",
    "href": "aggregates.html#even-more-aggregates",
    "title": "6  Aggregates",
    "section": "6.4 Even more aggregates",
    "text": "6.4 Even more aggregates\nThere’s a ton of things we can do in summarize – we’ll work with more of them as the course progresses – but here’s a few other questions you can ask.\nWhich department on campus has the highest wage bill? And what is the highest and lowest salary in the department? And how wide is the spread between salaries? We can find that with sum to add up the salaries to get the total wage bill, min to find the minumum salary, max to find the maximum salary and sd to find the standard deviation in the numbers.\n\nsalaries |> \n  group_by(Campus, Department) |> \n  summarize(\n    total = sum(`Budgeted Annual Salary`), \n    avgsalary = mean(`Budgeted Annual Salary`), \n    minsalary = min(`Budgeted Annual Salary`),\n    maxsalary = max(`Budgeted Annual Salary`),\n    stdev = sd(`Budgeted Annual Salary`)) |> arrange(desc(total))\n\n`summarise()` has grouped output by 'Campus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 804 × 7\n# Groups:   Campus [5]\n   Campus Department                  total avgsalary minsalary maxsalary  stdev\n   <chr>  <chr>                       <dbl>     <dbl>     <dbl>     <dbl>  <dbl>\n 1 UNL    Athletics                  3.56e7   118508.     12925   5000000 3.33e5\n 2 UNMC   Pathology/Microbiology     1.36e7    63158.      1994    186925 3.41e4\n 3 UNL    Agronomy & Horticulture    8.98e6    66496.      5000    208156 4.01e4\n 4 UNMC   Anesthesiology             7.90e6    78237.     10000    245174 3.59e4\n 5 UNL    School of Natural Resourc… 6.86e6    65995.      2400    194254 3.28e4\n 6 UNL    College of Law             6.70e6    77953.      1000    326400 7.23e4\n 7 UNL    University Television      6.44e6    55542.     16500    221954 2.75e4\n 8 UNL    University Libraries       6.27e6    51390.      1200    215917 2.68e4\n 9 UNMC   Pharmacology/Exp Neurosci… 6.24e6    58911.      2118    248139 4.29e4\n10 UNMC   CON-Omaha Division         6.11e6    78304.      3000    172522 4.48e4\n# ℹ 794 more rows\n\n\nSo again, no surprise, the UNL athletic department has the single largest wage bill at nearly $36 million. The average salary in the department is $118,508 – more than double the univeristy as a whole, again thanks to Scott Frost’s paycheck."
  },
  {
    "objectID": "mutating.html#another-use-of-mutate",
    "href": "mutating.html#another-use-of-mutate",
    "title": "7  Mutating data",
    "section": "7.1 Another use of mutate",
    "text": "7.1 Another use of mutate\nNote in our data we have separate State and County name fields. If we were publishing this, we wouldn’t want that.\nSo how can we fix that? Mutate! And a new function to combine text together called paste. Paste allows us to merge fields together easily with a separator. In our case, we want to combine the county name and the state name with a comma and a space between them.\n\npopulation |> \n  mutate(\n    change = ((POPESTIMATE2018 - POPESTIMATE2017)/POPESTIMATE2017)*100,\n    location = paste(CTYNAME, STNAME, sep=\", \")) |> \n  arrange(desc(change))\n\n# A tibble: 3,142 × 15\n   STNAME         CTYNAME        CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010\n   <chr>          <chr>                  <dbl>             <dbl>           <dbl>\n 1 Texas          Loving County             82                82              84\n 2 Colorado       San Juan Coun…           699               699             708\n 3 North Dakota   McKenzie Coun…          6360              6359            6411\n 4 Kentucky       Lee County              7887              7887            7718\n 5 North Dakota   Williams Coun…         22398             22399           22588\n 6 Texas          Comal County          108472            108485          109270\n 7 Texas          Kenedy County            416               413             417\n 8 Texas          Kaufman County        103350            103363          103890\n 9 North Carolina Brunswick Cou…        107431            107429          108065\n10 Florida        Walton County          55043             55043           55211\n# ℹ 3,132 more rows\n# ℹ 10 more variables: POPESTIMATE2011 <dbl>, POPESTIMATE2012 <dbl>,\n#   POPESTIMATE2013 <dbl>, POPESTIMATE2014 <dbl>, POPESTIMATE2015 <dbl>,\n#   POPESTIMATE2016 <dbl>, POPESTIMATE2017 <dbl>, POPESTIMATE2018 <dbl>,\n#   change <dbl>, location <chr>\n\n\n\nEXERCISE: What happens when you sort it in ascending order? Delete the desc part in arrange and see what happens. How would you describe this list?"
  },
  {
    "objectID": "dates.html#the-hard-way",
    "href": "dates.html#the-hard-way",
    "title": "8  Working with dates",
    "section": "8.1 The hard way",
    "text": "8.1 The hard way\nFirst, we’ll import tidyverse like we always do.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nWe’re going to use a dataset of parking tickets at UNL. If we do this the old way – using read.csv – this is what we get:\n\ntickets <- read.csv(\"data/tickets.csv\")\nhead(tickets)\n\n  Citation                Date        Location                    Violation\n1 15078429 2012-04-02 07:15:00   North Stadium                Expired Meter\n2 24048318 2012-04-02 07:22:00         Housing    No Valid Permit Displayed\n3 24048320 2012-04-02 07:26:00 14th & W Street    No Valid Permit Displayed\n4 15078430 2012-04-02 07:36:00  Champions Club Parking in Unauthorized Area\n5 18074937 2012-04-02 07:39:00          Sandoz                Expired Meter\n6 18074938 2012-04-02 07:40:00          Sandoz                Expired Meter\n\n\nNote the date is a factor, not a date. We have to fix that. There’s a lot of ways to fix dates. The base R way is to use formatting. The code is … a little odd … but it’s useful to know if you get a really odd date format. What you are doing is essentially parsing the date into it’s component parts then reassmbling it into a date using formatting.\n\nnewtickets <- tickets |> mutate(\n    CleanDate = as.POSIXct(Date, format=\"%Y-%m-%d %H:%M:%S\")\n)\n\nhead(newtickets)\n\n  Citation                Date        Location                    Violation\n1 15078429 2012-04-02 07:15:00   North Stadium                Expired Meter\n2 24048318 2012-04-02 07:22:00         Housing    No Valid Permit Displayed\n3 24048320 2012-04-02 07:26:00 14th & W Street    No Valid Permit Displayed\n4 15078430 2012-04-02 07:36:00  Champions Club Parking in Unauthorized Area\n5 18074937 2012-04-02 07:39:00          Sandoz                Expired Meter\n6 18074938 2012-04-02 07:40:00          Sandoz                Expired Meter\n            CleanDate\n1 2012-04-02 07:15:00\n2 2012-04-02 07:22:00\n3 2012-04-02 07:26:00\n4 2012-04-02 07:36:00\n5 2012-04-02 07:39:00\n6 2012-04-02 07:40:00\n\n\nCleanDate is now a special date format that includes times.\nYou can almost read the code that created it: The format of the date is %Y, which means a four digit year DASH %m or two digit month DASH %d or two digit day SPACE %H or two digit hour COLON %M or two digit minute COLON %S or two digit second. You can remix that as you need. If you had a date that was 20021212 then you would do format=\"%Y%m%d\" and so on.\nThere is a library called lubridate that can parse some common date problems. If it’s not already installed, just run install.packages('lubridate')\n\nlibrary(lubridate)\n\nLubridate can handle this tickets data easier with one of it’s many functions. The functions parse dates given a basic pattern. In this case, our data is in a very common pattern of year month date hours minutes seconds. Lubridate has a function called ymd_hms.\n\nlubridatetickets <- tickets |> mutate(\n    CleanDate = ymd_hms(Date)\n)\n\nhead(lubridatetickets)\n\n  Citation                Date        Location                    Violation\n1 15078429 2012-04-02 07:15:00   North Stadium                Expired Meter\n2 24048318 2012-04-02 07:22:00         Housing    No Valid Permit Displayed\n3 24048320 2012-04-02 07:26:00 14th & W Street    No Valid Permit Displayed\n4 15078430 2012-04-02 07:36:00  Champions Club Parking in Unauthorized Area\n5 18074937 2012-04-02 07:39:00          Sandoz                Expired Meter\n6 18074938 2012-04-02 07:40:00          Sandoz                Expired Meter\n            CleanDate\n1 2012-04-02 07:15:00\n2 2012-04-02 07:22:00\n3 2012-04-02 07:26:00\n4 2012-04-02 07:36:00\n5 2012-04-02 07:39:00\n6 2012-04-02 07:40:00\n\n\nThat’s less code and less weirdness, so that’s good.\nBut to get clean data, I’ve installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don’t think your data will always be perfect and you won’t have to do these things.\nStill, there’s got to be a better way. And there is.\nFortunately, readr anticipates some date formattings and can automatically handle this (indeed it uses lubridate under the hood). The change in your code? You just use read_csv instead of read.csv\n\ntickets <- read_csv(\"data/tickets.csv\")\n\nRows: 161315 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Citation, Location, Violation\ndttm (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(tickets)\n\n# A tibble: 6 × 4\n  Citation Date                Location        Violation                   \n  <chr>    <dttm>              <chr>           <chr>                       \n1 15078429 2012-04-02 07:15:00 North Stadium   Expired Meter               \n2 24048318 2012-04-02 07:22:00 Housing         No Valid Permit Displayed   \n3 24048320 2012-04-02 07:26:00 14th & W Street No Valid Permit Displayed   \n4 15078430 2012-04-02 07:36:00 Champions Club  Parking in Unauthorized Area\n5 18074937 2012-04-02 07:39:00 Sandoz          Expired Meter               \n6 18074938 2012-04-02 07:40:00 Sandoz          Expired Meter               \n\n\nAnd just like that, the dates are formatted correctly.\nBut you’re not done with lubridate yet. It has some interesting pieces parts we’ll use elsewhere.\nWhat’s a question you might have about parking tickets on campus involving dates?\nHow about what month are the most tickets issued? We could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.\nSo to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.\n\ntickets |> \n  mutate(Month = floor_date(Date, \"month\")) |> \n  group_by(Month) |> \n  summarise(total = n()) |>\n  arrange(desc(total))\n\n# A tibble: 56 × 2\n   Month               total\n   <dttm>              <int>\n 1 2014-10-01 00:00:00  5177\n 2 2015-04-01 00:00:00  4913\n 3 2014-09-01 00:00:00  4645\n 4 2015-09-01 00:00:00  4541\n 5 2015-10-01 00:00:00  4403\n 6 2015-03-01 00:00:00  4392\n 7 2016-02-01 00:00:00  4314\n 8 2016-09-01 00:00:00  4221\n 9 2016-03-01 00:00:00  4194\n10 2012-10-01 00:00:00  4173\n# ℹ 46 more rows\n\n\nSo the most tickets in this dataset were issued in September of 2014. April of 2015 was second. Then two Septembers and an October.\nAny guesses why those months?\nI’ll give you a hint. It involves 90,000 people gathering in a big building on campus in the fall and one day in April or late March every spring."
  },
  {
    "objectID": "filters.html#combining-filters",
    "href": "filters.html#combining-filters",
    "title": "9  Filters and selections",
    "section": "9.1 Combining filters",
    "text": "9.1 Combining filters\nSo let’s say we wanted to know how many full professors make more than $100,000. We can do this a number of ways. The first is we can chain together a whole lot of filters.\n\nprofs <- salaries |> filter(Campus == \"UNL\") |> filter(Position == \"Professor\") |> filter(`Budgeted Annual Salary` > 100000)\n\nnrow(profs)\n\n[1] 312\n\n\nSo that gives us 312 full professors – that’s the top rank of tenured professors – who make more than $100,000. But that’s silly and repetitive, no? We can do better using boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all three things must be true to be included. With OR, any of those three things can be true and it will be included. An assistant professor making $100k at UNO will get included because they make $100k. One of the conditions is true.\nHere’s the difference.\n\nandprofs <- salaries |> filter(Campus == \"UNL\" & Position == \"Professor\" & `Budgeted Annual Salary` > 100000)\n\nnrow(andprofs)\n\n[1] 312\n\n\nSo AND gives us the same answer we got before. What does OR give us?\n\norprofs <- salaries |> filter(Campus == \"UNL\" | Position == \"Professor\" | `Budgeted Annual Salary` > 100000)\n\nnrow(orprofs)\n\n[1] 7248\n\n\nSo there’s 7,248 unique people in the NU system who are at UNL (6,079 to be exact), are full Professors (1,086 of them), or make more than $100,000 (1,604) of them. Included in that list? Football coach Scott Frost, who makes … ahem … more than $100,000. A smidge more."
  },
  {
    "objectID": "datasmells.html#wrong-type",
    "href": "datasmells.html#wrong-type",
    "title": "10  Data Cleaning Part I: Data smells",
    "section": "10.1 Wrong Type",
    "text": "10.1 Wrong Type\nFirst, let’s look at Wrong Type Of Data. We can sniff that out by looking at the output of readr\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\ntickets <- read_csv(\"data/tickets.csv\")\n\nRows: 161315 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Citation, Location, Violation\ndttm (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRight away, we see there’s 104,265 parsing errors. Why? Look closely. The Citation number that readr interprets from the first rows comes in at a number. But 56,000 rows in, those citation numbers start having letters in them, and letters are not numbers.\nThe cheap way to fix this is to change the guess_max parameter of readr to just use more than a few rows to guess the column types. It’ll go a little slower, but it’ll fix the problem.\n\ntickets <- read_csv(\"data/tickets.csv\", guess_max = 60000)\n\nRows: 161315 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Citation, Location, Violation\ndttm (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFor this, things seem to be good. Date appears to be in date format, things that aren’t numbers appear to be text. That’s a good start."
  },
  {
    "objectID": "datasmells.html#missing-data",
    "href": "datasmells.html#missing-data",
    "title": "10  Data Cleaning Part I: Data smells",
    "section": "10.2 Missing Data",
    "text": "10.2 Missing Data\nThe second smell we can find in code is missing data. We can do that through a series of Group By and Count steps.\n\ntickets |> group_by(Location) |> tally()\n\n# A tibble: 247 × 2\n   Location                        n\n   <chr>                       <int>\n 1 1001 Y Street                 508\n 2 10th & Q Street               303\n 3 10th & U Street               222\n 4 1101 Y Street                  83\n 5 11th & Y Street                38\n 6 1235 Military Road             33\n 7 1320 Q                          1\n 8 13th & R Lot                 4918\n 9 14th & Avery Lot             1601\n10 14th & Avery Parking Garage  2494\n# ℹ 237 more rows\n\n\nWhat we’re looking for here are blanks: Tickets without a location. Typically, those will appear first or last, depending on several things, so it’s worth checking the front and back of our data.\nWhat about ticket type?\n\ntickets |> group_by(Violation) |> tally()\n\n# A tibble: 25 × 2\n   Violation                          n\n   <chr>                          <int>\n 1 Damage Property                   25\n 2 Displaying Altered Permit      23280\n 3 Displaying Counterfeit Permit     18\n 4 Displaying Stolen Permit           4\n 5 Expired Meter                  45072\n 6 Failure to Pay[on exit]          251\n 7 Failure to Reg. Veh to Permit     53\n 8 Failure to Register Veh w/ UNL   113\n 9 False Lost/Stolen Permit Rept    927\n10 Falsify Permit Application         3\n# ℹ 15 more rows\n\n\nNone here either, so that’s good. It means our tickets will always have a location and a violation type."
  },
  {
    "objectID": "datasmells.html#gaps-in-data",
    "href": "datasmells.html#gaps-in-data",
    "title": "10  Data Cleaning Part I: Data smells",
    "section": "10.3 Gaps in data",
    "text": "10.3 Gaps in data\nLet’s now look at gaps in data. It’s been my experience that gaps in data often have to do with time, so let’s first look at ticket dates, so we can see if there’s any big jumps in data. You’d expect the numbers to change, but not by huge amounts. Huge change would indicate, more often than not, that the data is missing. Let’s start with Date. If we’re going to work with dates, we should have lubridate handy for floor_date.\n\nlibrary(lubridate)\n\n\ntickets |> group_by(floor_date(Date, \"month\")) |> tally()\n\n# A tibble: 56 × 2\n   `floor_date(Date, \"month\")`     n\n   <dttm>                      <int>\n 1 2012-04-01 00:00:00          3473\n 2 2012-05-01 00:00:00          2572\n 3 2012-06-01 00:00:00          2478\n 4 2012-07-01 00:00:00          2134\n 5 2012-08-01 00:00:00          3774\n 6 2012-09-01 00:00:00          4138\n 7 2012-10-01 00:00:00          4173\n 8 2012-11-01 00:00:00          3504\n 9 2012-12-01 00:00:00          1593\n10 2013-01-01 00:00:00          3078\n# ℹ 46 more rows\n\n\nFirst thing to notice: our data starts in April 2012. So 2012 isn’t a complete year. Then, scroll through. Look at December 2013 - March 2014. The number of tickets drops to about 10 percent of normal. That’s … odd. And then let’s look at the end – November 2016. So not a complete year in 2016 either."
  },
  {
    "objectID": "datasmells.html#internal-inconsistency",
    "href": "datasmells.html#internal-inconsistency",
    "title": "10  Data Cleaning Part I: Data smells",
    "section": "10.4 Internal inconsistency",
    "text": "10.4 Internal inconsistency\nAny time you are going to focus on something, you should check it for consistency inside the data set. So let’s pretend the large number of Displaying Altered Permit tickets caught your attention and you want to do a story about tens of thousands of students being caught altering their parking permits to reduce their costs parking on campus. Good story right? Before you go calling the parking office for comment, I’d check that data first.\n\ntickets |> filter(Violation == \"Displaying Altered Permit\") |> group_by(floor_date(Date, \"month\")) |> tally()\n\n# A tibble: 29 × 2\n   `floor_date(Date, \"month\")`     n\n   <dttm>                      <int>\n 1 2012-04-01 00:00:00          1072\n 2 2012-05-01 00:00:00          1283\n 3 2012-06-01 00:00:00          1324\n 4 2012-07-01 00:00:00          1357\n 5 2012-08-01 00:00:00          2249\n 6 2012-09-01 00:00:00          1797\n 7 2012-10-01 00:00:00          1588\n 8 2012-11-01 00:00:00          1183\n 9 2012-12-01 00:00:00           458\n10 2013-01-01 00:00:00          1132\n# ℹ 19 more rows\n\n\nSo this charge exists when our data starts, but scroll forward: In October 2013, there’s 1,081 tickets written. A month later, only 121. A month after that? 1. And then one sporadically for three more years.\nSomething major changed. What is it? That’s why you are a reporter. Go ask. But we know our data doesn’t support the story we started with.\nAnd that’s what Data Smells are designed to do: stop you from going down a bad path."
  },
  {
    "objectID": "datasmells.html#a-shortcut-summary",
    "href": "datasmells.html#a-shortcut-summary",
    "title": "10  Data Cleaning Part I: Data smells",
    "section": "10.5 A Shortcut: Summary",
    "text": "10.5 A Shortcut: Summary\nOne quick way to get some data smells is to use R’s built in summary function. What summary does is run summary statistics on each column of your dataset. Some of the output is … underwhelming … but some is really useful. For example, looking at min and max for dates can point to bad data there. Min and max will also show you out of range numbers – numbers far too big or small to make sense.\nThe syntax is simple.\n\nsummary(tickets)\n\n   Citation              Date                          Location        \n Length:161315      Min.   :2012-04-02 07:15:00.00   Length:161315     \n Class :character   1st Qu.:2013-05-06 09:42:30.00   Class :character  \n Mode  :character   Median :2014-10-17 12:03:00.00   Mode  :character  \n                    Mean   :2014-08-13 19:36:52.66                     \n                    3rd Qu.:2015-10-08 07:31:30.00                     \n                    Max.   :2016-11-03 13:59:19.00                     \n  Violation        \n Length:161315     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn this case, the output doesn’t do much for us except dates. Looking at the min and max will tell us if we have any out of range dates. In this case, we do not."
  },
  {
    "objectID": "janitor.html#cleaning-headers",
    "href": "janitor.html#cleaning-headers",
    "title": "11  Data Cleaning Part II: Janitor",
    "section": "11.1 Cleaning headers",
    "text": "11.1 Cleaning headers\nOne of the first places we can start with cleaning data is cleaning the headers. Every system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are one word, lower case, without special characters. If you’ve noticed readr output with backticks around headers like Incident Date, it’s because of the space. Headers that start with numbers or are just a number – 2002 – also get backticks in readr.\nThere is an external library in R called janitor that makes fixing headers trivially simple. You can install it by running install.packages(\"janitor\") in your console.\nLoad libraries like we always do.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nLet’s load a new dataset – the list of active inmates in the Nebraska prison system.\n\ninmates <- read_csv(\"data/activeinmates.csv\")\n\nNew names:\n• `FIRST NAME` -> `FIRST NAME...3`\n• `MIDDLE NAME` -> `MIDDLE NAME...4`\n• `NAME EXTENSION` -> `NAME EXTENSION...5`\n• `FIRST NAME` -> `FIRST NAME...7`\n• `MIDDLE NAME` -> `MIDDLE NAME...8`\n• `NAME EXTENSION` -> `NAME EXTENSION...9`\n• `` -> `...31`\n• `` -> `...32`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 7674 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (22): COMMITTED LAST NAME, FIRST NAME...3, MIDDLE NAME...4, NAME EXTENSI...\ndbl  (6): ID NUMBER, MIN MONTH, MIN DAY, MAX MONTH, MAX DAY, GOOD TIME LAW\nlgl  (4): NAME EXTENSION...9, GUN CLAUSE, ...31, ...32\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFrom the output of readr, you can see all kinds of problems from the get go. Two columns are missing names entirely. Three columns repeat – first name, middle name and name extension. And many field names have spaces or other not-allowed characters. Not to mention: All of them are in ALL CAPS.\nJanitor makes this easy to fix. How easy? This easy.\n\ninmates |> clean_names()\n\n# A tibble: 7,674 × 32\n   id_number committed_last_name first_name_3 middle_name_4 name_extension_5\n       <dbl> <chr>               <chr>        <chr>         <chr>           \n 1      6145 KANE                THOMAS       <NA>          <NA>            \n 2     20841 ARNOLD              WILLIAM      L             <NA>            \n 3     25324 WALKER              RICHARD      T             <NA>            \n 4     25565 ALVAREZ             THOMAS       A             <NA>            \n 5     26103 ADAMS               BRIAN        J             <NA>            \n 6     26547 KIRBY               RONALD       EUGENE        <NA>            \n 7     27471 NIEMANN             MAX          A             <NA>            \n 8     27666 ORTIZ               LAWRENCE     <NA>          <NA>            \n 9     27767 POINDEXTER          EDWARD       <NA>          <NA>            \n10     27778 DITTRICH            LADDIE       F             <NA>            \n# ℹ 7,664 more rows\n# ℹ 27 more variables: legal_last_name <chr>, first_name_7 <chr>,\n#   middle_name_8 <chr>, name_extension_9 <lgl>, date_of_birth <chr>,\n#   race_desc <chr>, gender <chr>, facility <chr>,\n#   current_sentence_pardoned_or_commuted_date <chr>, gun_clause <lgl>,\n#   sentence_begin_date <chr>, min_term_year <chr>, min_month <dbl>,\n#   min_day <dbl>, max_term_year <chr>, max_month <dbl>, max_day <dbl>, …\n\n\nJust like that, all lower case, all one word, no backticks necessary to confuse our code later on.\nAnother thing janitor does well is to make it easy to drop empty columns. Remember the two unnamed columns in the data? Turns out they’re unnamamed because there’s nothing in them. Nada. Blank. We could use select but janitor reduces the labor involved there.\nFirst, let’s see how many columns we have.\n\ninmates |> ncol()\n\n[1] 32\n\n\nAnd by using remove_empty(\"cols\"), how many do we get rid of?\n\ninmates |> clean_names() |> remove_empty(\"cols\") |> ncol()\n\n[1] 28\n\n\nSo this tells us there’s three completely empty columns in our data. So why keep them around.\nSo we can run all of this together and get a dataset with useful columns and clean headers.\n\ninmates |> clean_names() |> remove_empty(\"cols\") -> clean_headers_inmates"
  },
  {
    "objectID": "janitor.html#duplicates",
    "href": "janitor.html#duplicates",
    "title": "11  Data Cleaning Part II: Janitor",
    "section": "11.2 Duplicates",
    "text": "11.2 Duplicates\nOne of the most difficult problems to fix in data is duplicates in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. One trick is determining if a duplicate is indeed a duplicate.\nSo the question is, do we have any inmates repeated? Anyone in prison twice?\nHere we’ll use a function called get_dupes. And we’ll use the inmate’s last name, first name and date of birth. The likelihood that someone has the same name and date of birth is very small, so if there are no duplicates, we should get zero records returned.\n\nclean_headers_inmates |> get_dupes(committed_last_name, first_name_3, date_of_birth)\n\n# A tibble: 2 × 29\n  committed_last_name first_name_3 date_of_birth dupe_count id_number\n  <chr>               <chr>        <chr>              <int>     <dbl>\n1 WALLACE             PAMELA       7/11/1966              2     99240\n2 WALLACE             PAMELA       7/11/1966              2     99955\n# ℹ 24 more variables: middle_name_4 <chr>, name_extension_5 <chr>,\n#   legal_last_name <chr>, first_name_7 <chr>, middle_name_8 <chr>,\n#   race_desc <chr>, gender <chr>, facility <chr>,\n#   current_sentence_pardoned_or_commuted_date <chr>,\n#   sentence_begin_date <chr>, min_term_year <chr>, min_month <dbl>,\n#   min_day <dbl>, max_term_year <chr>, max_month <dbl>, max_day <dbl>,\n#   parole_eligibility_date <chr>, earliest_possible_release_date <chr>, …\n\n\nUh oh. We get two Pamela Wallaces born on the same day in 1966. But is it a duplicate record? Look closely. Two different id_numbers. In two different facilities on two different dates. Two different sentencing dates. Is it a duplicate record or the same person entering the system two different times?"
  },
  {
    "objectID": "janitor.html#inconsistency",
    "href": "janitor.html#inconsistency",
    "title": "11  Data Cleaning Part II: Janitor",
    "section": "11.3 Inconsistency",
    "text": "11.3 Inconsistency\nJanitor also has some handy tools for our data smells. One is called tabyl, which creates a table of unique records in a single field.\nSo does the Department of Corrections record gender consistently? tabyl will tell us and will tell us a little bit about the data.\n\nclean_headers_inmates |> tabyl(gender)\n\n gender    n    percent\n FEMALE  732 0.09538702\n   MALE 6942 0.90461298\n\n\nSo the Department of Corrections clearly doesn’t buy into more modern sentiments about gender, but they are at least consistent. Every inmate has a gender – no NAs – and note that 90 percent of inmates are men.\nHow about race?\n\nclean_headers_inmates |> tabyl(race_desc)\n\n        race_desc    n      percent valid_percent\n            ASIAN   61 0.0079489184  0.0079520271\n            BLACK 2037 0.2654417514  0.2655455612\n         HISPANIC 1059 0.1379984363  0.1380524052\n  NATIVE AMERICAN  349 0.0454782382  0.0454960240\n            OTHER   56 0.0072973677  0.0073002216\n PACIFIC ISLANDER    7 0.0009121710  0.0009125277\n            WHITE 4102 0.5345321866  0.5347412332\n             <NA>    3 0.0003909304            NA\n\n\nThree people do not have a race – and according to the Census Bureau, Hispanic is not a race, it’s an ethnicity – but otherwise, it looks solid. There’s very little in the way of inconsistency.\nHow about what facilities they are in?\n\nclean_headers_inmates |> tabyl(facility)\n\n                       facility    n    percent valid_percent\n  COMMUNITY CORRECTIONS-LINCOLN 1276 0.16627574    0.16887242\n    COMMUNITY CORRECTIONS-OMAHA  368 0.04795413    0.04870302\n DIAGNOSTIC & EVALUATION CENTER  778 0.10138129    0.10296453\n    LINCOLN CORRECTIONAL CENTER  571 0.07440709    0.07556908\n NEBRASKA CORR CENTER FOR WOMEN  480 0.06254887    0.06352567\n    NEBRASKA CORR YOUTH FACILTY   83 0.01081574    0.01098465\n    NEBRASKA STATE PENITENTIARY 1588 0.20693250    0.21016411\n      OMAHA CORRECTIONAL CENTER 1059 0.13799844    0.14015352\n TECUMSEH STATE COR INSTITUTION 1104 0.14386239    0.14610905\n                WORK ETHIC CAMP  249 0.03244722    0.03295394\n                           <NA>  118 0.01537660            NA\n\n\nNot sure how I feel about 118 inmates not having a facility. That’s probably worth investigating. At least one, I know about – it lists the inmate as having escaped in the 1960s and never found. Not sure about the others.\nBut sometimes, NAs are not bad data. Sometimes they’re just NA. Let’s look at inst_release_type or how inmates were released.\n\nclean_headers_inmates |> tabyl(inst_release_type)\n\n    inst_release_type    n      percent valid_percent\n DISCRETIONARY PAROLE 1391 0.1812614021  0.5506730008\n               ESCAPE   51 0.0066458170  0.0201900238\n     MANDATORY PAROLE    2 0.0002606203  0.0007917656\n            RE-PAROLE    3 0.0003909304  0.0011876485\n      RELEASED TO PRS 1079 0.1406046390  0.4271575614\n                 <NA> 5148 0.6708365911            NA\n\n\nBy far the largest group here is NA. Why is that? They haven’t been released yet. They’re still in prison."
  },
  {
    "objectID": "openrefine.html#refinr-open-refine-in-r",
    "href": "openrefine.html#refinr-open-refine-in-r",
    "title": "12  Data Cleaning Part III: Open Refine",
    "section": "12.1 Refinr, Open Refine in R",
    "text": "12.1 Refinr, Open Refine in R\nWhat is Open Refine?\nOpen Refine is a series of tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(refinr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nLet’s load a simple dataset where we know there’s a simple problem. Let’s load the dataset of mountainlion sightings.\n\nmountainlions <- read_csv(\"https://mattwaite.github.io/datajournalismfiles/mountainlions.csv\")\n\nRows: 393 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Cofirm Type, COUNTY, Date\ndbl (1): ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe issue in this dataset, if you look carefully, is that there’s two Sheridan counties – a Sheridan and a sheridan.\n\nmountainlions |> group_by(COUNTY) |> tally()\n\n# A tibble: 42 × 2\n   COUNTY        n\n   <chr>     <int>\n 1 Banner        6\n 2 Blaine        3\n 3 Box Butte     4\n 4 Brown        15\n 5 Buffalo       3\n 6 Cedar         1\n 7 Cherry       30\n 8 Custer        8\n 9 Dakota        3\n10 Dawes       111\n# ℹ 32 more rows\n\n\nThe first merging technique we’ll try is the key_collision_merge. The key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching. So in this case, it finds sheridan and Sheridan and recognizes that the keys match, and since Sheridan is more common, it uses that one.\nOne rule you should follow: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called CleanCounty and put the results of key collision merge there.\nThen, to show it worked, I’ll do the same group and count.\n\nmountainlions |> \n  mutate(CleanCounty = key_collision_merge(COUNTY)) |>\n  group_by(CleanCounty) |> tally()\n\n# A tibble: 41 × 2\n   CleanCounty     n\n   <chr>       <int>\n 1 Banner          6\n 2 Blaine          3\n 3 Box Butte       4\n 4 Brown          15\n 5 Buffalo         3\n 6 Cedar           1\n 7 Cherry         30\n 8 Custer          8\n 9 Dakota          3\n10 Dawes         111\n# ℹ 31 more rows\n\n\nAnd just like that, instead of 35 and 2 in two different Sheridan counties, we have 37 in one Sheridan County."
  },
  {
    "objectID": "openrefine.html#more-complex-issues",
    "href": "openrefine.html#more-complex-issues",
    "title": "12  Data Cleaning Part III: Open Refine",
    "section": "12.2 More complex issues",
    "text": "12.2 More complex issues\nLet’s load a dataset of the charges Nebraska prison inmates were convicted of, which is why they’re in prison. We’ll also use janitor’s clean_names function to give us usable headers.\n\ncharges <- read_csv(\"data/charges.csv\")  |> clean_names()\n\nRows: 19226 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): OFFENSE MINIMUM YEAR OR TERM, OFFENSE MAXIMUM YEAR OR TERM, OFFENSE...\ndbl (5): ID NUMBER, MINIMUM MONTH, MINIMUM DAY, MAXIMUM MONTH, MAXIMUM DAY\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe problematic – and among the most interesting – fields in this dataset is the name of the charges. What is the most common charge keeping someone in prison?\nI’m not going to run the list here because it’s long – thousands of lines long. You should run it yourself:\ncharges |> tabyl(offense_arrest_desc)\nYou’ll see right away that there’s problems. There’s dozens upon dozens of charges that are the same thing, just slightly different. There’s 4003 unique charges, and many of them are duplicates.\n\ncharges |> group_by(offense_arrest_desc) |> tally(sort=TRUE) |> nrow()\n\n[1] 4003\n\n\nSo how does key_collision_merge do with this?\n\ncharges |>\n  mutate(\n    clean_charges = key_collision_merge(offense_arrest_desc)\n    ) |>\n  group_by(clean_charges) |>\n  tally() |>\n  nrow()\n\n[1] 3420\n\n\nCuts down the duplicates by 583. But since the charges are often multiple words, we should try using n_gram_merge, which looks a multiple words.\nHere’s an example using sensible defaults for weighting – unfortunately the documentation doesn’t do much to explain what they are.\n\ncharges |> \n  mutate(\n    clean_charges = n_gram_merge(\n      offense_arrest_desc, weight = c(d = 0.2, i = 0.2, s = 1, t = 1))) |> \n  group_by(clean_charges) |> \n  tally() |>\n  nrow()\n\n[1] 3036\n\n\nCuts it down by almost 1000. That seems pretty good. Here’s a different method, using a method that turns words into phonetic spellings called soundex.\n\ncharges |> \n  mutate(\n    clean_charges = n_gram_merge(\n      offense_arrest_desc, method = \"soundex\", useBytes = TRUE\n      )) |> \n  group_by(clean_charges) |> \n  tally() |>\n  nrow()\n\n[1] 2688\n\n\nCut it down by almost 1400!\nBUT.\nAre they right?\nWe have no idea. Let’s look at the first 30 rows.\n\ncharges |> \n  mutate(\n    clean_charges = n_gram_merge(\n      offense_arrest_desc, method = \"soundex\", useBytes = TRUE\n      )) |>\n  filter(clean_charges != offense_arrest_desc) |> select(offense_arrest_desc, clean_charges) |> head(30)\n\n# A tibble: 30 × 2\n   offense_arrest_desc            clean_charges                 \n   <chr>                          <chr>                         \n 1 ASSLT WI INFLICT BODILY INJURY ASSLT W/I INFLCT BODILY INJURY\n 2 STAB W/I TO KILL WOUND OR MAIM STAB W/I KILL, WOUND, OR MAIM \n 3 USE OF WEAPON TO COMMIT FELONY USE WEAPON TO COMMIT FELONY   \n 4 3RD DEGREE ASSAULT ON OFFICER  ASSAULT ON OFFICER 3RD DEGREE \n 5 3RD DEGREE ASSAULT ON OFFICER  ASSAULT ON OFFICER 3RD DEGREE \n 6 POSS CONTROLLED SUBSTANCE      POSSESS CONTROLLED SUBSTANCE  \n 7 MANUF/DIST CONT SUBST - MARIJ. DISTRIBUTION OF C/S-MARIJUANA \n 8 POSS DEADLY WEAP BY FELON      POSSESS DEADLY WEAPON BY FELON\n 9 POSS FIREARM BY FELON          POS FIREARM BY FELON          \n10 3RD DGR ASSAULT ON AN OFFICER  ASSAULT ON AN OFFICER 3RD DEGR\n# ℹ 20 more rows\n\n\nIf you look carefully, you’ll see a lot of success here. But look at line 23. The charge is theft by taking $0-500. The clean version? Theft by taking $5000. That’s a big difference, and a bad miss.\nSo can we trust automated data cleaning?\nThis note from the documentation is exceedingly important:\n\nThis package is NOT meant to replace OpenRefine for every use case. For situations in which merging accuracy is the most important consideration, OpenRefine is preferable. Since the merging steps in refinr are automated, there will usually be more false positive merges, versus manually selecting clusters to merge in OpenRefine."
  },
  {
    "objectID": "openrefine.html#manually-cleaning-data-with-open-refine",
    "href": "openrefine.html#manually-cleaning-data-with-open-refine",
    "title": "12  Data Cleaning Part III: Open Refine",
    "section": "12.3 Manually cleaning data with Open Refine",
    "text": "12.3 Manually cleaning data with Open Refine\nOpen Refine is free software. You should download and install it. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief. For bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design, but still).\nAfter you install it, run it. Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project.\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project.\n\n\n\n\n\nThe real power in Open Refine is in faceting. In our case, we’re specifically going to use text faceting. Next to the OFFENSE ARREST DESC header, click the down arrow, then facet, then text facet.\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique offenses are there: 4,082. And, there’s a button on the right of the box that says Cluster. Click that.\n\n\n\n\n\nThe default clustering algorithm used is key collision, using the fingerprint function. This is the same method we used with Sheridan County above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. Then, below that, you can see what those clusters are. Then, using human judgement, you can say if you agree with the cluster. If you do, click the merge checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result.\n\n\n\n\n\nNow begins the fun part: You have to look at all 303 clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method. Rinse and repeat. You’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nIf it’s not, it’ll go on forever.\n\n\n\n\n\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no good answer. You have to find it yourself."
  },
  {
    "objectID": "pdfs.html#when-it-looks-good-but-goes-wrong",
    "href": "pdfs.html#when-it-looks-good-but-goes-wrong",
    "title": "13  Cleaning Data Part IV: PDFs",
    "section": "13.1 When it looks good, but goes wrong",
    "text": "13.1 When it looks good, but goes wrong\nEvery year, the University of Nebraska-Lincoln publishes dozens of PDFs that give you interesting demographic information about students, the faculty and a variety of other things. But all of it – every little bit of it – is in a PDF. And most of them are designed to look “nice” not convey data. Even when they do very obviously look like they came from a spreadsheet – like someone printed the spreadsheet to a PDF so they could put it on the web – it doesn’t work.\nA perfect example of this is the data showing the breakdown of students by degree, major, race and sex. Open it up, it looks like a spreadsheet but in a PDF.\n\n\n\n\n\nDownload and install Tabula. Tabula works much the same way as Open Refine does – it works in the browser by spinning up a small webserver in your computer.\nWhen Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import.\nAfter it imports, click autodetect tables. You’ll see red boxes appear around what Tabula believes are the tables. You’ll see it does a pretty good job at this.\n\n\n\n\n\nIf you like what you see, click Preview and Export Extracted Data.\nAnd here’s where it all starts to go wrong.\nYou’ll see at first it looks good – you get a reasonable representation of the table. But look at the first line.\n\n\n\n\n\nFirst, the misspelling of college is disturbing. Did a university document misspell it? No. Which means Tabula is reading two ls as one. That’s … not good.\nSecond, notice how the College of Agri Sci and Natl Resources, which was in it’s own column before have been merged, somewhat inartfully, into the first column of major names. There is no major Colege of Agri Sci and Agribusiness. Same with Natl Resources Agricultural & Env Sci Comm. Those aren’t things.\nNote the empty column between Major1 and DegreeId.\nNow scroll down some more.\n\n\n\n\n\nNotice Architecture has the same merging of college name and first major problems as the first one does, but note the blank column is missing.\nLook at Arts and Sciences. Arts and Sciences are now in their own column, as the data shows, but there’s now empty names that shouldn’t be. What are those?\nIn short, it’s a mess.\nHere’s the sad truth: THIS IS PRETTY GOOD. Open it in a spreadsheet and a little copying and pasting work while double checking the right names line up with the right rows and you’re in business. As converted PDFs, this isn’t bad.\nIt beats typing it out."
  },
  {
    "objectID": "pdfs.html#when-it-works-well.",
    "href": "pdfs.html#when-it-works-well.",
    "title": "13  Cleaning Data Part IV: PDFs",
    "section": "13.2 When it works well.",
    "text": "13.2 When it works well.\nEach month, the Nebraska Department of Revenue releases the monthly tax receipts of the state, and forecasts into the future what tax receipts might be in the near future. They do this for planning purposes – the Legislature needs to know how much money the state may have when the new budget is put into place so they know how much money they have to spend.\nThe announcement comes in a press release. Each press release includes a table showing the current number, the predicted number, and difference. Of course it’s a PDF.\nLet’s look at the most recent month as of this writing: January 2020. Download it, open it in Tabula and hit Autodetect tables.\nYou’ll note it finds no tables on the first page. Which is good, because there aren’t any. Let’s look at the third page. It finds a table, but is it one?\n\n\n\n\n\nLet’s hit the X in the top right on that one.\nThat leaves page 2. It finds two tables there. Let’s just grab the first. Hit X on the second and click to preview the extracted data.\nThis looks good. So let’s export it to a csv."
  },
  {
    "objectID": "pdfs.html#cleaning-up-the-data-in-r",
    "href": "pdfs.html#cleaning-up-the-data-in-r",
    "title": "13  Cleaning Data Part IV: PDFs",
    "section": "13.3 Cleaning up the data in R",
    "text": "13.3 Cleaning up the data in R\nThe good news is that we have data we don’t have to retype. The bad news is, it’s hardly in importable shape.\nLet’s load libraries.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nTo import this, we need one row of headers. We have three. And we need headers that make sense.\nWe can spell these out in the import step. First, we’ll use skip to skip the first three lines. Then we’ll spell out the column names by hand in a col_names bit. Here’s how it looks.\n\nreceipts <- read_csv(\"data/tabula-General_Fund_Receipts_January_2020.csv\", skip = 3, col_names = c(\"Month\", \"TotalActualNetReceipts\", \"TotalProjectedNetReceipts\", \"Difference\", \"PercentDifference\", \"CumulativeActualNetReceipts\", \"CumulativeProjectedNetReceipts\", \"CumulativeDifference\",\"CumulativePercentDifference\"), skip_empty_rows = TRUE)\n\nRows: 7 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Month, TotalActualNetReceipts, TotalProjectedNetReceipts, Differenc...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we have a harder part.\nThe columns come in as character columns. Why? Because the state puts commas and $ and % in them, which R does not interpret as anything except text. So we need to get rid of them. We can mutate columns and use a function called gsub that finds a string and replaces it with something. So in our case, we’re going to gsub(\",\",\"\", fieldname). The unfortunate part is we have a lot of colunns and a lot of fixes. So this is going to require a lot of code. It is repetitive, though, so we can copy and paste and adjust with most of it.\nAt the end, we need to use a function called mutate_at and convert the columns that aren’t text into numbers.\nAnd one last thing: If we do many months of this, we should note which report this comes from. We can do this with mutate as well.\nHere’s what that looks like:\n\nreceipts |> mutate(\n  TotalActualNetReceipts = gsub(\",\",\"\",TotalActualNetReceipts),\n  TotalActualNetReceipts = gsub(\"\\\\$\",\"\",TotalActualNetReceipts),\n  TotalProjectedNetReceipts = gsub(\",\",\"\",TotalProjectedNetReceipts),\n  TotalProjectedNetReceipts = gsub(\"\\\\$\",\"\",TotalProjectedNetReceipts),\n  Difference = gsub(\",\",\"\",Difference),\n  Difference = gsub(\"\\\\$\",\"\",Difference),\n  PercentDifference = gsub(\"\\\\%\",\"\",PercentDifference),\n  CumulativeActualNetReceipts = gsub(\",\",\"\",CumulativeActualNetReceipts),\n  CumulativeActualNetReceipts = gsub(\"\\\\$\",\"\",CumulativeActualNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\",\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\"\\\\$\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeDifference = gsub(\",\",\"\",CumulativeDifference),\n  CumulativeDifference = gsub(\"\\\\$\",\"\",CumulativeDifference),\n  CumulativePercentDifference = gsub(\"\\\\%\",\"\",CumulativePercentDifference)\n  ) |> mutate_at(vars(-Month), as.numeric) |> mutate(ReportMonth = \"January 2020\")\n\n# A tibble: 7 × 10\n  Month     TotalActualNetReceipts TotalProjectedNetReceipts Difference\n  <chr>                      <dbl>                     <dbl>      <dbl>\n1 July                   284883132                 271473079   13410054\n2 August                 462019974                 440504016   21515958\n3 September              551908013                 510286143   41621870\n4 October                289723434                 266204529   23518905\n5 November               431787603                 404934524   26853079\n6 December               472926836                 421455999   51470837\n7 January                467698460                 412661036   55037424\n# ℹ 6 more variables: PercentDifference <dbl>,\n#   CumulativeActualNetReceipts <dbl>, CumulativeProjectedNetReceipts <dbl>,\n#   CumulativeDifference <dbl>, CumulativePercentDifference <dbl>,\n#   ReportMonth <chr>\n\n\nWe can now reuse this with other months after we harvest the data out of it."
  },
  {
    "objectID": "merging.html#combining-data",
    "href": "merging.html#combining-data",
    "title": "14  Combining and joining",
    "section": "14.1 Combining data",
    "text": "14.1 Combining data\nIn the last assignment, we harvested data out of PDFs. Let’s reuse what we did there and merge three months of reports from the Department of Revenue together. For mine, I have January 2020, December 2019, and November 2019.\nLet’s do what we need to import them properly. Unlike the last example, I’ve merged it all into one step for each of the three datasets.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\nreceiptsJan20 <- read_csv(\"data/tabula-General_Fund_Receipts_January_2020.csv\", skip = 3, col_names = c(\"Month\", \"TotalActualNetReceipts\", \"TotalProjectedNetReceipts\", \"Difference\", \"PercentDifference\", \"CumulativeActualNetReceipts\", \"CumulativeProjectedNetReceipts\", \"CumulativeDifference\",\"CumulativePercentDifference\"), skip_empty_rows = TRUE) |> mutate(\n  TotalActualNetReceipts = gsub(\",\",\"\",TotalActualNetReceipts),\n  TotalActualNetReceipts = gsub(\"\\\\$\",\"\",TotalActualNetReceipts),\n  TotalProjectedNetReceipts = gsub(\",\",\"\",TotalProjectedNetReceipts),\n  TotalProjectedNetReceipts = gsub(\"\\\\$\",\"\",TotalProjectedNetReceipts),\n  Difference = gsub(\",\",\"\",Difference),\n  Difference = gsub(\"\\\\$\",\"\",Difference),\n  PercentDifference = gsub(\"\\\\%\",\"\",PercentDifference),\n  CumulativeActualNetReceipts = gsub(\",\",\"\",CumulativeActualNetReceipts),\n  CumulativeActualNetReceipts = gsub(\"\\\\$\",\"\",CumulativeActualNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\",\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\"\\\\$\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeDifference = gsub(\",\",\"\",CumulativeDifference),\n  CumulativeDifference = gsub(\"\\\\$\",\"\",CumulativeDifference),\n  CumulativePercentDifference = gsub(\"\\\\%\",\"\",CumulativePercentDifference)\n  ) |> mutate_at(vars(-Month), as.numeric) |> mutate(ReportMonth = \"January 2020\")\n\nRows: 7 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Month, TotalActualNetReceipts, TotalProjectedNetReceipts, Differenc...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nreceiptsDec19 <- read_csv(\"data/tabula-General_Fund_Receipts_December_2019.csv\", skip = 3, col_names = c(\"Month\", \"TotalActualNetReceipts\", \"TotalProjectedNetReceipts\", \"Difference\", \"PercentDifference\", \"CumulativeActualNetReceipts\", \"CumulativeProjectedNetReceipts\", \"CumulativeDifference\",\"CumulativePercentDifference\"), skip_empty_rows = TRUE) |> mutate(\n  TotalActualNetReceipts = gsub(\",\",\"\",TotalActualNetReceipts),\n  TotalActualNetReceipts = gsub(\"\\\\$\",\"\",TotalActualNetReceipts),\n  TotalProjectedNetReceipts = gsub(\",\",\"\",TotalProjectedNetReceipts),\n  TotalProjectedNetReceipts = gsub(\"\\\\$\",\"\",TotalProjectedNetReceipts),\n  Difference = gsub(\",\",\"\",Difference),\n  Difference = gsub(\"\\\\$\",\"\",Difference),\n  PercentDifference = gsub(\"\\\\%\",\"\",PercentDifference),\n  CumulativeActualNetReceipts = gsub(\",\",\"\",CumulativeActualNetReceipts),\n  CumulativeActualNetReceipts = gsub(\"\\\\$\",\"\",CumulativeActualNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\",\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\"\\\\$\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeDifference = gsub(\",\",\"\",CumulativeDifference),\n  CumulativeDifference = gsub(\"\\\\$\",\"\",CumulativeDifference),\n  CumulativePercentDifference = gsub(\"\\\\%\",\"\",CumulativePercentDifference)\n  ) |> mutate_at(vars(-Month), as.numeric) |> mutate(ReportMonth = \"December 2019\")\n\nRows: 6 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Month, TotalActualNetReceipts, TotalProjectedNetReceipts, Differenc...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nreceiptsNov19 <- read_csv(\"data/tabula-General_Fund_Receipts_November_12-13-2019.csv\", skip = 3, col_names = c(\"Month\", \"TotalActualNetReceipts\", \"TotalProjectedNetReceipts\", \"Difference\", \"PercentDifference\", \"CumulativeActualNetReceipts\", \"CumulativeProjectedNetReceipts\", \"CumulativeDifference\",\"CumulativePercentDifference\"), skip_empty_rows = TRUE) |> mutate(\n  TotalActualNetReceipts = gsub(\",\",\"\",TotalActualNetReceipts),\n  TotalActualNetReceipts = gsub(\"\\\\$\",\"\",TotalActualNetReceipts),\n  TotalProjectedNetReceipts = gsub(\",\",\"\",TotalProjectedNetReceipts),\n  TotalProjectedNetReceipts = gsub(\"\\\\$\",\"\",TotalProjectedNetReceipts),\n  Difference = gsub(\",\",\"\",Difference),\n  Difference = gsub(\"\\\\$\",\"\",Difference),\n  PercentDifference = gsub(\"\\\\%\",\"\",PercentDifference),\n  CumulativeActualNetReceipts = gsub(\",\",\"\",CumulativeActualNetReceipts),\n  CumulativeActualNetReceipts = gsub(\"\\\\$\",\"\",CumulativeActualNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\",\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeProjectedNetReceipts = gsub(\"\\\\$\",\"\",CumulativeProjectedNetReceipts),\n  CumulativeDifference = gsub(\",\",\"\",CumulativeDifference),\n  CumulativeDifference = gsub(\"\\\\$\",\"\",CumulativeDifference),\n  CumulativePercentDifference = gsub(\"\\\\%\",\"\",CumulativePercentDifference)\n  ) |> mutate_at(vars(-Month), as.numeric) |> mutate(ReportMonth = \"November 2019\")\n\nRows: 5 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Month, TotalActualNetReceipts, TotalProjectedNetReceipts, Differenc...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAll three of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is rbind. The good news about rbind is that it is very simple. The bad news – you can only merge two things at a time.\nSince we have three things, we’re going to do this in steps. First, we’ll create a dataframe that will hold it all and we’ll populate it with two of our revenue dataframes rbinded together. Then, we’ll overwrite our new dataframe with the results of that dataframe with the third revenue dataframe.\n\npredictions1 <- rbind(receiptsJan20, receiptsDec19)\n\npredictions2 <- rbind(predictions1, receiptsNov19)\n\npredictions2\n\n# A tibble: 18 × 10\n   Month     TotalActualNetReceipts TotalProjectedNetReceipts Difference\n   <chr>                      <dbl>                     <dbl>      <dbl>\n 1 July                   284883132                 271473079   13410054\n 2 August                 462019974                 440504016   21515958\n 3 September              551908013                 510286143   41621870\n 4 October                289723434                 266204529   23518905\n 5 November               431787603                 404934524   26853079\n 6 December               472926836                 421455999   51470837\n 7 January                467698460                 412661036   55037424\n 8 July                   284883132                 271473079   13410054\n 9 August                 462019974                 440504016   21515958\n10 September              551908013                 510286143   41621870\n11 October                289723434                 266204529   23518905\n12 November               431787603                 404934524   26853079\n13 December               472926836                 421455999   51470837\n14 July                   284883132                 271473079   13410054\n15 August                 462019974                 440504016   21515958\n16 September              551908013                 510286143   41621870\n17 October                289723434                 266204529   23518905\n18 November               431787603                 404934524   26853079\n# ℹ 6 more variables: PercentDifference <dbl>,\n#   CumulativeActualNetReceipts <dbl>, CumulativeProjectedNetReceipts <dbl>,\n#   CumulativeDifference <dbl>, CumulativePercentDifference <dbl>,\n#   ReportMonth <chr>\n\n\nAnd boom, like that, we have 18 rows of data instead of three dataframes of 5, 6, and 7 respectively."
  },
  {
    "objectID": "merging.html#joining-data",
    "href": "merging.html#joining-data",
    "title": "14  Combining and joining",
    "section": "14.2 Joining data",
    "text": "14.2 Joining data\nMore difficult is when you have two separate tables that are connected by a common element or elements.\nLet’s return to our fatal accident data. In reality, the Fatality Analysis Reporting System data has 27 tables in it – everything from details about the damage to the paperwork done.\nLet’s just merge two of them and just for the state of Nebraska – download the accidents and the people.\nOften, when talking about relational data files like this, there’s substantial amounts of documentation that go with it to tell you how these things are related and what codes mean. The FARS data is no different. You should open it and click on the PERSON Data File.\n\nST_CASE should be used to merge the Person data file with the Accident data file for a set of all motorists and non-motorists.\n\nSo that’s what we’re going to do.\n\naccidents <- read_csv(\"data/neaccidents.csv\")\n\nRows: 201 Columns: 52\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): TWAY_ID, TWAY_ID2, RAIL\ndbl (49): STATE, ST_CASE, VE_TOTAL, VE_FORMS, PVH_INVL, PEDS, PERNOTMVIT, PE...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\npersons <- read_csv(\"data/nepersons.csv\")\n\nRows: 553 Columns: 62\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (62): STATE, ST_CASE, VE_FORMS, VEH_NO, PER_NO, STR_VEH, COUNTY, DAY, MO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, notice something in the environment about your dataset: there are 201 accidents but 553 persons. That means there’s not quite 3 people involved in every accident on average between drivers and passengers. Some are single car, single person crashes. Some involve a lot of people.\nTo put these two tables together, we need to use something called a join. There are different kinds of join. It’s better if you think of two tables sitting next to each other. A left_join takes all the records from the left table and only the records that match in the right one. A right_join does the same thing. An inner_join takes only the records where they are equal. There’s one other join – a full_join which returns all rows of both, regardless of if there’s a match – but I’ve never once had a use for a full join.\nIn the PERSON Data File documentation, we see that column that connects these two tables together is the ST_CASE column.\nSo we can do this join multiple ways and get a similar result. We can put the person file on the left and the accident on the right and use a left join to get them all together. And we use by= to join by the right column. And to avoid rendering hundreds of rows of data, I’m going to count the rows at the end. The reason I’m going this is important: Rule 1 in joining data is having an idea of what you are expecting to get. So with a left join with people on the left, I have 553 people, so I expect to get 553 rows when I’m done.\n\npersons |> left_join(accidents, by=\"ST_CASE\") |> nrow()\n\n[1] 553\n\n\nRemove the nrow and run it again for yourself. See how there are several columns that end with .X? That means they’re duplicates. There’s a solid chance they are the same in both tables. By default, dplyr will do a “natural” join, where it’ll match all the matching columns in both tables. So if we take out the by, it’ll use all the common columns between the tables. That may not be right – our documentation says ST_CASE is how they are related – but let’s try it. If it works, we should get 553 rows.\n\npersons |> left_join(accidents) \n\nJoining with `by = join_by(STATE, ST_CASE, VE_FORMS, COUNTY, DAY, MONTH, HOUR,\nMINUTE, RUR_URB, FUNC_SYS, HARM_EV, MAN_COLL, SCH_BUS)`\n\n\n# A tibble: 553 × 101\n   STATE ST_CASE VE_FORMS VEH_NO PER_NO STR_VEH COUNTY   DAY MONTH  HOUR MINUTE\n   <dbl>   <dbl>    <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1    31  310001        2      1      1       0     55     1     1     0     20\n 2    31  310001        2      2      1       0     55     1     1     0     20\n 3    31  310002        1      1      1       0    157     7     1    22      0\n 4    31  310003        1      0      1       1     55    10     1     6     57\n 5    31  310003        1      1      1       0     55    10     1     6     57\n 6    31  310004        2      1      1       0     79    10     1    16     55\n 7    31  310004        2      2      1       0     79    10     1    16     55\n 8    31  310005        2      1      1       0    119    10     1     4     30\n 9    31  310005        2      2      1       0    119    10     1     4     30\n10    31  310006        1      0      1       1    109    11     1     6     25\n# ℹ 543 more rows\n# ℹ 90 more variables: RUR_URB <dbl>, FUNC_SYS <dbl>, HARM_EV <dbl>,\n#   MAN_COLL <dbl>, SCH_BUS <dbl>, MAKE <dbl>, MAK_MOD <dbl>, BODY_TYP <dbl>,\n#   MOD_YEAR <dbl>, TOW_VEH <dbl>, SPEC_USE <dbl>, EMER_USE <dbl>,\n#   ROLLOVER <dbl>, IMPACT1 <dbl>, FIRE_EXP <dbl>, AGE <dbl>, SEX <dbl>,\n#   PER_TYP <dbl>, INJ_SEV <dbl>, SEAT_POS <dbl>, REST_USE <dbl>,\n#   REST_MIS <dbl>, AIR_BAG <dbl>, EJECTION <dbl>, EJ_PATH <dbl>, …\n\n\nSo instead of just one column, it used 13. And we got the same answer. And we don’t have any columns with .X after it anymore. So we’re good to move forward.\nLet’s save our joined data to a new dataframe.\n\npersonaccidents <- persons |> left_join(accidents)\n\nJoining with `by = join_by(STATE, ST_CASE, VE_FORMS, COUNTY, DAY, MONTH, HOUR,\nMINUTE, RUR_URB, FUNC_SYS, HARM_EV, MAN_COLL, SCH_BUS)`\n\n\nNow, with our joined data, we can answer questions that come from both datasets. So what if we looked at median age of drivers who died broken down by what kind of roadway the accident happened on? We can do this now because the accident data has the roadway information information and the age and who was driving and what type of injury they sustained comes from the person table.\nWe get this by using filters followed by a group by and summarize. In the data documentation linked above, look in the PERSON Data File to get the appropriate filters. In this case, we want PER_TYPE of 1 (the driver) and an INJ_SEV of 4, which means death. In the ACCCIDENT Data File section, we learn it’s the ROUTE we want to group by.\n\npersonaccidents |> \n  filter(PER_TYP == 1) |> \n  filter(INJ_SEV == 4) |> \n  group_by(ROUTE) |> \n  summarize(\n    count = n(), \n    avgage = mean(AGE), \n    medage = median(AGE))\n\n# A tibble: 5 × 4\n  ROUTE count avgage medage\n  <dbl> <int>  <dbl>  <dbl>\n1     1    15   40.5     33\n2     2    51   53.1     51\n3     3    31   42.3     42\n4     4    37   37.3     36\n5     6    18   40.4     35\n\n\nAccording to our query, 15 accidents happened on interstates, and the median age of those was the lowest of all at 33. The most accidents were on US Highways, which makes sense because there’s a lot more lane miles of US Highways than Interstates in Nebraska and pretty much every other state. But the second most common is county roads. And the median age of drivers there was quite low at 36.\nLet’s break this down one more step. What if we added RUR_URB – if the accident happened in rural or urban place. A common feeling in a rural state like Nebraska is that urban interstate is a ribbon of insanity. But is it?\n\npersonaccidents |> \n  filter(PER_TYP == 1) |> \n  filter(INJ_SEV == 4) |> \n  group_by(RUR_URB, ROUTE) |> \n  summarize(\n    count = n(), \n    avgage = mean(AGE), \n    medage = median(AGE))\n\n`summarise()` has grouped output by 'RUR_URB'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   RUR_URB [2]\n  RUR_URB ROUTE count avgage medage\n    <dbl> <dbl> <int>  <dbl>  <dbl>\n1       1     1    12   45.3   42.5\n2       1     2    41   52.5   51  \n3       1     3    27   41.9   42  \n4       1     4    37   37.3   36  \n5       2     1     3   21.3   21  \n6       2     2    10   55.3   54.5\n7       2     3     4   45     35  \n8       2     6    18   40.4   35  \n\n\nIn 2018, only 3 of the 15 deaths on interestates were in urban areas. All the rest were in rural areas. And all 37 drivers who died in accidents on county roads were in rural areas. Most of the driver deaths on US Highways were in rural places as well."
  },
  {
    "objectID": "rvest.html#a-more-difficult-example",
    "href": "rvest.html#a-more-difficult-example",
    "title": "15  Scraping data with Rvest",
    "section": "15.1 A more difficult example",
    "text": "15.1 A more difficult example\nThe Nebraska Legislature, with it’s unique unicameral or one house structure, does some things a little differently than other legislatures. Example: Bills have to go through three rounds of debate before getting passed. There’s General File (round 1), Select File (round 2), and Final Reading (round 3).\nSo what does a day where they do more than general file look like? Like this.\nHow do we scrape that?\n\nharderurl <- \"https://nebraskalegislature.gov/calendar/agenda.php?day=2020-02-21\"\n\n\nharderagenda <- harderurl |>\n  read_html() |>\n  html_nodes(\"table\") |>\n  html_table()\n\nYou can see that harderagenda has a list of four instead of a list of one. We can see each item in the list has a dataframe. We can see them individually. Here’s the first:\n\nharderagenda[[1]]\n\n# A tibble: 4 × 3\n  Document                                                Introducer Description\n  <chr>                                                   <chr>      <chr>      \n1 \"Currently or Pendingon Floor\\n                       … Kolterman  Define the…\n2 \"Currently or Pendingon Floor\\n                       … Geist      Change and…\n3 \"Currently or Pendingon Floor\\n                       … Chambers   Change pro…\n4 \"Currently or Pendingon Floor\\n                       … Gragert    Provide fo…\n\n\nHere’s the second:\n\nharderagenda[[2]]\n\n# A tibble: 4 × 3\n  Document                                                Introducer Description\n  <chr>                                                   <chr>      <chr>      \n1 \"Currently or Pendingon Floor\\n                       … Crawford   Change the…\n2 \"Currently or Pendingon Floor\\n                       … Lindstrom  Change pro…\n3 \"Currently or Pendingon Floor\\n                       … Quick      Change the…\n4 \"Currently or Pendingon Floor\\n                       … Hunt       Adopt the …\n\n\nSo we can merge those together no problem, but how would we know what stage each bill is at?\nLook at the page – we can see that the bills are separated by a big headline like “SELECT FILE: 2020 PRIORITY BILLS”. To separate these, we need to grab those and then add them to each bill using mutate.\nHere’s how we grab them:\n\nlabels <- harderurl |>\n  read_html() |>\n  html_nodes(\".card-header\") |>\n  html_text()\n\nAnother list. If you look at the first, it’s at the top of the page with no bills. Here’s the second:\n\nlabels[[2]]\n\n[1] \"\\n                    SELECT FILE:  2020 PRIORITY BILLS\\n                                    \"\n\n\nSo we know can see there’s some garbage in there we want to clean out. We can use a new library called stringr to trim the excess spaces and gsub to strip the newline character: \\n.\n\nharderagenda[[1]] |> \n  mutate(Document = gsub(\"Currently or Pendingon Floor\\n \",\"\",Document)) |>\n  mutate(Stage = labels[[2]]) |>\n  mutate(Stage = gsub(\"\\n\",\"\",Stage)) |>\n  mutate(Stage = str_trim(Stage, side = \"both\"))\n\n# A tibble: 4 × 4\n  Document                                          Introducer Description Stage\n  <chr>                                             <chr>      <chr>       <chr>\n1 \"                                               … Kolterman  Define the… SELE…\n2 \"                                               … Geist      Change and… SELE…\n3 \"                                               … Chambers   Change pro… SELE…\n4 \"                                               … Gragert    Provide fo… SELE…\n\n\nNow it’s just a matter grinding through the items in the list.\nNOTE: This is grossly inefficient and very manual. And, we’d have to change this for every day we want to scrape. As such, this is not the “right” way to do this. We’ll cover that in the next chapter.\n\nharderagenda1 <- harderagenda[[1]] |> \n  mutate(Document = gsub(\"Currently or Pendingon Floor\\n \",\"\",Document)) |>\n  mutate(Stage = labels[[2]]) |> mutate(Stage = gsub(\"\\n\",\"\",Stage)) |>\n  mutate(Stage = str_trim(Stage, side = \"both\"))\n\n\nharderagenda2 <- harderagenda[[2]] |> \n  mutate(Document = gsub(\"Currently or Pendingon Floor\\n \",\"\",Document)) |>\n  mutate(Stage = labels[[3]]) |> mutate(Stage = gsub(\"\\n\",\"\",Stage)) |>\n  mutate(Stage = str_trim(Stage, side = \"both\"))\n\n\nharderagenda3 <- harderagenda[[3]] |> \n  mutate(Document = gsub(\"Currently or Pendingon Floor\\n \",\"\",Document)) |>\n  mutate(Stage = labels[[4]]) |> mutate(Stage = gsub(\"\\n\",\"\",Stage)) |>\n  mutate(Stage = str_trim(Stage, side = \"both\"))\n\n\nharderagenda4 <- harderagenda[[4]] |> \n  mutate(Document = gsub(\"Currently or Pendingon Floor\\n \",\"\",Document)) |>\n  mutate(Stage = labels[[5]]) |> mutate(Stage = gsub(\"\\n\",\"\",Stage)) |>\n  mutate(Stage = str_trim(Stage, side = \"both\"))\n\nNow we merge:\n\nlargeragenda <- rbind(harderagenda1, harderagenda2)\nlargeragenda <- rbind(largeragenda, harderagenda3)\nlargeragenda <- rbind(largeragenda, harderagenda4)\n\nAnd now we have a dataset of all bills and what stage they’re at for that day.\n\nlargeragenda\n\n# A tibble: 10 × 4\n   Document                                         Introducer Description Stage\n   <chr>                                            <chr>      <chr>       <chr>\n 1 \"                                              … Kolterman  Define the… \"SEL…\n 2 \"                                              … Geist      Change and… \"SEL…\n 3 \"                                              … Chambers   Change pro… \"SEL…\n 4 \"                                              … Gragert    Provide fo… \"SEL…\n 5 \"                                              … Crawford   Change the… \"GEN…\n 6 \"                                              … Lindstrom  Change pro… \"GEN…\n 7 \"                                              … Quick      Change the… \"GEN…\n 8 \"                                              … Hunt       Adopt the … \"GEN…\n 9 \"                                              … Agricultu… Adopt the … \"GEN…\n10 \"                                              … Howard     Congratula… \"LEG…"
  },
  {
    "objectID": "advancedrvest.html",
    "href": "advancedrvest.html",
    "title": "16  Advanced rvest",
    "section": "",
    "text": "Continuing a discussion from the last chapter, this is an example of when it goes from Easy to Moderately Difficult.\nIn the last exercise, we scraped bills out of one day of floor activity in the Nebraska Legislature. What if we wanted all of them? Let’s say we wanted to keep a running scoreboard of who has introduced the most bills that have seen the floor?\nHere’s how to use some programming knowledge with R to grab all days and merge them together. First we start with libraries, as we always do.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n\nNow we’ll start the trickery. So we need to know what days the legislature is in session. Fortunately, the legislature tells us that in a drop down menu. Inspect the dropdown menu of this page. See the list?\n\n\n\n\n\nIf it’s HTML in the page, we can grab it, so let’s do that. We start with the URL to any day, really.\n\nurl <- \"https://nebraskalegislature.gov/calendar/dayview.php?day=2020-02-03\"\n\nI’m going to create a thing called a calendar and store the list of option values in there. So when I’m done, I’ll have a list of dates.\n\ncalendar <- url |>\n  read_html() |>\n  html_nodes(xpath = '//*[@id=\"leg_day\"]') |>\n  html_nodes(\"option\") |>\n  html_attrs()\n\n\ncalendar[1][1]\n\n[[1]]\nNULL\n\n\nNow this part gets tough, but if you follow, it’s logical. It’s step by step, really.\nFirst, I noticed at the top of the agenda pages is a link to the daily agenda in csv format. Convenient, that.\n\n\n\n\n\nIf you look at that url, it looks like this:\nhttps://nebraskalegislature.gov/calendar/agenda.php?day=2020-01-14&print=csv\nIf we can change the date in that url to each day of the session, we’d get a csv file for each day of the session that we can merge together.\nLet’s break down the steps to do that.\n\nI need a place to store this. A lazy way? Just import a day, filter everything out of it, and I’ve got an empty table I can rbind stuff into.\nI need to set up a loop. For each day in the calendar, do some stuff.\nI need to clean out the word selected (an HTML thing) from one of the dates.\nI need to just scrape days that have business pending on the floor. So I can skip the first few days and I can skip any days into the future.\nI need to take my date from calendar and add them to the url. Then I need to take that agenda url and add the csv parts.\nThen I can use read_csv like we have been to read the CSV url directly.\nThen I just rbind my new csv to the one I created.\nThis isn’t going to work perfectly every time, so I’m going to add some tryCatch statements that say try this, and if it doesn’t work, do nothing.\nThen, because I’m a decent fellow, I’m going to pause my query for three seconds to give their servers a break.\n\nLet’s run it.\n\nallbills <- read_csv(\"https://nebraskalegislature.gov/calendar/agenda.php?day=2020-01-14&print=csv\", col_types = cols()) |> mutate(Date = as.Date(\"2020-01-01\"))|> filter(`Section Header` == \"Blah\")\n\nfor (i in calendar){\n  date <- i[1]\n  checkdate <- as.Date(date)\n  \n  if (checkdate <= Sys.Date() & checkdate > as.Date(\"2020-01-13\")) {\n\n    agendaurl <- paste(\"https://nebraskalegislature.gov/calendar/agenda.php?day=\", i, sep=\"\")\n    csvurl <-paste(agendaurl, \"&print=csv\", sep=\"\")\n  tryCatch(\n    agenda <- read_csv(csvurl, col_types = cols()) |> mutate(Date = checkdate), \n    error = function(e){NA})\n  tryCatch(\n    allbills <- rbind(allbills, agenda), \n    error = function(e){NA})\n  Sys.sleep(3)\n  }\n}\n\nAnd after that, I have a dataset of entries of bills on the floor. So if I want to see who has had the most bills on the floor – including repeats – I could answer that now.\n\nallbills |> group_by(Introducer) |> tally(sort=TRUE) |> na.omit() |> top_n(5)\n\nSelecting by n\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: Introducer <chr>, n <int>\n\n\nSenator Kolterman, collect your prize.\nA note about advanced scraping – every site is different. Every time you want to scrape a site, you’ll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it."
  },
  {
    "objectID": "visualizing1.html#bar-charts",
    "href": "visualizing1.html#bar-charts",
    "title": "17  Visualizing your data for reporting",
    "section": "17.1 Bar charts",
    "text": "17.1 Bar charts\nThe first kind of chart we’ll create is a simple bar chart. It’s a chart designed to show differences between things – the magnitude of one, compared to the next, and the next, and the next. So if we have thing, like a county, or a state, or a group name, and then a count of that group, we can make a bar chart.\nSo what does the chart of the top 10 counties with the most mountain sightings look like?\nFirst, we’ll create a dataframe of those top 10, called topsightings.\n\nmountainlions |>\n  group_by(COUNTY) |>\n  summarise(\n    total = n()\n  ) |> top_n(10, total) -> topsightings\n\nNow ggplot. The first thing we do with ggplot is invoke it, which creates the canvas. In ggplot, we work with geometries – the shape that the data will take – and aesthetics – the data that will take shape. In a bar chart, we first pass in the data to the geometry, then set the aesthetic. We tell ggplot what the x value is – in a bar chart, that’s almost always your grouping variable. Then we tell it the weight of the bar – the number that will set the height.\n\nggplot() + geom_bar(data=topsightings, aes(x=COUNTY, weight=total))\n\n\n\n\nThe bars look good, but he order makes no sense. In ggplot, we use reorder, and we reorder the x value based on the weight, like this:\n\nggplot() + geom_bar(data=topsightings, aes(x=reorder(COUNTY, total), weight=total))\n\n\n\n\nBetter, but it looks … not great on the bottom. We can fix that by flipping the coordinates.\n\nggplot() + geom_bar(data=topsightings, aes(x=reorder(COUNTY, total), weight=total)) + coord_flip()\n\n\n\n\nArt? No. Tells you the story? Yep. And for reporting purposes, that’s enough."
  },
  {
    "objectID": "visualizing1.html#line-charts",
    "href": "visualizing1.html#line-charts",
    "title": "17  Visualizing your data for reporting",
    "section": "17.2 Line charts",
    "text": "17.2 Line charts\nLine charts show change over time. It works the much the same as a bar chart, code wise, but instead of a weight, it uses a y. And if you have more than one group in your data, it takes a group element.\nThe secret to knowing if you have a line chart is if you have a date. The secret to making a line chart is your x value is almost always a date.\nTo make this easier, I’ve created a new version of the county population estimates data that it formatted for charting. Let’s import it:\n\npopulationestimates <- read_csv(\"data/countypopulationslong.csv\")\n\nRows: 28278 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): STNAME, CTYNAME, Year, Name\ndbl  (1): Population\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see, I’ve switched the data from the years going wide to the right to each line being one county, one year. And, I’ve added a date column, which is the estimates date.\nNow, if we tried to make a line chart of all 3,142 counties, we’d get a mess. But, it’s the first mistake people make in creating a line chart, so let’s do that.\n\nggplot() + geom_line(data=populationestimates, aes(x=Date, y=Population, group=Name))\n\n\n\n\nAnd what do we learn from this? There’s one very, very big county, some less big counties, and a ton of smaller counties. So many we can’t see Them, and because the numbers are so big, any changes are dwarfed.\nSo let’s thin the herd here. How about we just look at Lancaster County, Nebraska.\n\nlancaster <- populationestimates |> filter(Name == \"Lancaster County, Nebraska\")\n\nNow let’s chart it.\n\nggplot() + geom_line(data=lancaster, aes(x=Date, y=Population, group=Name))\n\n\n\n\nGrowing like gangbusters, right? Well, not exactly. Note the y axis doesn’t start at 0.\n\nggplot() + geom_line(data=lancaster, aes(x=Date, y=Population, group=Name)) + scale_y_continuous(limits = c(0, 320000))\n\n\n\n\nMore accurate.\nBut how does that compare to Omaha? Geoms work in layers. We can just add Omaha. First, we need a dataframe with Douglas County in it.\n\ndouglas <- populationestimates |> filter(Name == \"Douglas County, Nebraska\")\n\nWith that, we just add another geom_line. We will also need to adjust our y axis limits to expand to fit Omaha.\n\nggplot() + geom_line(data=lancaster, aes(x=Date, y=Population, group=Name)) + geom_line(data=douglas, aes(x=Date, y=Population, group=Name)) + scale_y_continuous(limits = c(0, 650000))\n\n\n\n\nSo both counties are growing, but from the line we can see that Omaha is growing just ever so slightly faster. No accounting for taste."
  },
  {
    "objectID": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "href": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "title": "18  Writing with numbers",
    "section": "18.1 How to write about numbers without overwhelming with numbers.",
    "text": "18.1 How to write about numbers without overwhelming with numbers.\nWriting complex stories is often a battle against that complexity. You don’t want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.\nWhere you can, do the following:\n\nUsing ratios instead of percents\nOften, it’s better to put it in counts of 10. 6 of 10, 4 of 10. It’s easy to translate that from a percentage to a ratio.\nBut be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?\nIf a ratio doesn’t make sense, round. There’s 287,401 people in Lincoln, according to the Census Bureau. It’s easier, and no less accurate, to say there’s more than 287,000 people in Lincoln.\n\nA critical question your writing should answer: As compared to what?\nHow does this compare to the average? The state? The nation? The top? The bottom?\nOne of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book Paving Paradise was comparing approvals and denials.\nWe were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.\nThey said yes 12,000 times. They said no once.\nThat one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no."
  },
  {
    "objectID": "writingwithdata.html#when-exact-numbers-matter",
    "href": "writingwithdata.html#when-exact-numbers-matter",
    "title": "18  Writing with numbers",
    "section": "18.2 When exact numbers matter",
    "text": "18.2 When exact numbers matter\nSometimes ratios and rounding are not appropriate.\nThis is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don’t say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It’s 75.\nYou don’t say almost 30 deaths. It’s 28.\nWhere this also comes into play is any time there are deaths: Do not round bodies."
  },
  {
    "objectID": "writingwithdata.html#an-example",
    "href": "writingwithdata.html#an-example",
    "title": "18  Writing with numbers",
    "section": "18.3 An example",
    "text": "18.3 An example\nRead this story from USA Today and the Arizona Republic. Notice first that the top sets up a conflict: People say one thing, and that thing is not true.\n\nNo one could have anticipated such a catastrophe, people said. The fire’s speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.\n\n\nThose declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.\n\nThe first voice you hear? An expert who studies wildfires.\n\nPhillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: “Fire is natural. But the disaster happens because people didn’t know to leave, or couldn’t leave. It didn’t have to happen.”\n\nThen notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it’s quite simple – the averaging together of pixels on a 1-5 scale.\nThen, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.\n\nAcross the West, 526 small communities — more than 10 percent of all places — rank higher.\n\nAnd that is how it’s done. Simplify, round, ratios: simple metrics, powerful results."
  },
  {
    "objectID": "ethics.html#problems",
    "href": "ethics.html#problems",
    "title": "19  Ethics in data journalism",
    "section": "19.1 Problems",
    "text": "19.1 Problems\nThe first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.\nAnd that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.\nThat was a problem because here are the things we could not know:\n\nWas this person wrongly arrested?\nWas this person innocent?\nWere the charges dropped against this person?\nDid this person lie about any of their information?"
  },
  {
    "objectID": "ethics.html#the-googlebot",
    "href": "ethics.html#the-googlebot",
    "title": "19  Ethics in data journalism",
    "section": "19.2 The Googlebot",
    "text": "19.2 The Googlebot\nSo it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.\nBut here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.\nSo, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.\nThe second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.\nThat freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.\nThe second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.\nSo, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.\nWe’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero."
  },
  {
    "objectID": "ethics.html#data-lifetimes",
    "href": "ethics.html#data-lifetimes",
    "title": "19  Ethics in data journalism",
    "section": "19.3 Data Lifetimes",
    "text": "19.3 Data Lifetimes\nThe life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.\nWe couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.\nThere’s a difference between frictionless and unobstructed sharing and some reasonable constraints.\nWe couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.\nWe couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook."
  },
  {
    "objectID": "ethics.html#you-are-a-data-provider",
    "href": "ethics.html#you-are-a-data-provider",
    "title": "19  Ethics in data journalism",
    "section": "19.4 You Are a Data Provider",
    "text": "19.4 You Are a Data Provider\nThe last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.\nWhat was going on?\nAfter some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.\nWhat we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?\nWhoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.\nThat was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.\nAmazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.\nSo at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.\nWhile the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward."
  },
  {
    "objectID": "ethics.html#ethical-data",
    "href": "ethics.html#ethical-data",
    "title": "19  Ethics in data journalism",
    "section": "19.5 Ethical Data",
    "text": "19.5 Ethical Data\nWe live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.\nWhat I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?\nBecause it can have consequences.\nOn Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.\nWithin days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.\nBy February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.\nThere’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.\nBecause you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”\nSo before you write the first line of code, ask these questions:\n\nThis data is public, but is it widely available? And does making it widely available and easy to use change anything?\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download?\n\nYour answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind."
  },
  {
    "objectID": "installations.html",
    "href": "installations.html",
    "title": "20  Installations",
    "section": "",
    "text": "You’re going to do things most of you aren’t used to doing with your computer in this class. In order to do that, you need to clean up your computer. I’ve seen what your computer looks like. It’s disgusting.\n\n20.0.1 Part 1: Update and patch your operating system\nOn a Mac:\n\nOpen System Preferences.\nDepending on how old your Mac OS is, you might see this:\n\n\nOr you might see this:\n\n\nCheck and see if you have the latest version of the Mac OS installed. If your computer says “Your Mac is up to date”, then you’re good to go, regardless of what comes next.\nIf you aren’t on Sonoma and you can update to it, you should do it. This will take some time – hours, so don’t do it when you need your laptop – but it’s important for you and your computer to stay up to date on operating systems.\nWhen you’re done, make sure you click the Automatically keep my Mac up to date box and install those updates regularly. Don’t ignore them. Don’t snooze them. Install them.\nWith an up-to-date operating system, now install the command line tools. To do this, click on the magnifying glass in the top right of the screen and type terminal. Hit enter – the first entry is the terminal app.\nIn the terminal app, type xcode-select --install and hit enter. Let it run.\n\n\nOn Windows:\n\nType Updates into the Cortana search then click Check for updates\n\n\n\nAfter the search for updates completes, apply any that you have. Depending on if you’d done this recently or if you have automatic updates set, this might take a long time or go very quickly.\n\n\n\nWhen you’re done, make sure you set up automatic updates for your Windows machine and install those updates regularly. Don’t ignore them. Don’t snooze them. Install them.\n\n\n\n20.0.2 Part 2: Install R and R Studio\n\nGo here. Go to Step 1 and click Download and Install R\n\nIf you’re on a Mac, click on Download R for MacOS. If you have a newer Mac with an M1/M2/M3 chip, you want the arm64 version. If you’re on an older Mac with an Intel chip, you want the X86_64 version.\nIf you’re on Windows, install the base package AND install Rtools. When either downloads, run the executable and accept the defaults and license agreement.\n\n\nGo back to here. Go to Step 2 and click R Studio Desktop for your version.\n\nMac users:\n\n\n\nMake sure you drag the R Studio icon into the Applications folder icon.\n\n\nWindows users:\nYou can find it by typing RStudio into the Cortana search.\n\n\n20.0.3 Part 3: Installing R libraries\n\nOpen R Studio. It should show the Console view by default. We’ll talk a lot more about the console later.\nCopy and paste this into the console and hit enter:\n\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"lubridate\", \"janitor\", \"learnr\", \"remotes\", \"devtools\", \"waffle\", \"ggrepel\", \"ggbeeswarm\", \"ggbump\", \"ggalt\", \"ggtext\", \"rvest\"))\n\n\n\n20.0.4 Part 4: Install Slack\n\nInstall Slack on your computer and your phone (you can find Slack in whatever app store you use). The reason I want it on both is because you are going to ask me for help with code via Slack. Do not use screenshots unless specifically asked. I want you to copy and paste your code. You can’t do that on a phone. So you need the desktop version. But I can usually solve your problem within a few minutes if you respond right away, and I know that you have your phone on you and are checking it. So the desktop version is for work, the phone version is for notifications.\nEmail me the address you want connected to Slack. Use one you’ll actually check.\nWhen you get the Slack invitation email, log in to the class slack via the apps, not the website.\nAdd the #r channel for general help I’ll send to everyone in the channel and, if you want, the #jobstuff channel for news about jobs I come across.\n\n\n\n\n20.0.5 Part 5: Install the tutorials\nTo get the tutorials, do the following.\n\nOpen R Studio. \nR Studio defaults to the console view. This is good, This is where you want to be.\n\n\n\nIn the console, enter the following:\ndevtools::install_github(\"mattwaite/DataJournalismTutorials\", force=TRUE)\nYou should see some automated output. If you are told there are newer libraries and asked if you want to install them, just hit enter. When it is done, quit R Studio and restart it. This is what it will look like when done."
  }
]