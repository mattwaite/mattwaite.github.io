[["index.html", "Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R Chapter 1 Introduction 1.1 Requirements and Conventions 1.2 About this book", " Advanced Sports Data Analysis Modeling and Machine Learning for sports analysis in R By Matt Waite 2022-02-27 Chapter 1 Introduction The 2020 college football season, for most fans, will be one to forget. The season started unevenly for most teams, schedules were shortened, non-conference games were rare, few fans saw their team play in person, all because of the COVID-19 global pandemic. For the Nebraska Cornhuskers, it was doubly forgettable. Year three of Scott Frost turned out to be another dud, with the team going 3-5. A common refrain from the coaching staff throughout the season, often after disappointing losses, was this: The team is close to turning a corner. How close? This is where modeling comes in in sports. Using modeling, we can determine what we should expect given certain inputs. To look at Nebraska’s season, let’s build a model of the season using three inputs based on narratives around the season: The offense struggled to score, the offense really struggled with turnovers, and the defense improved. The specifics of how to do this will be the subject of this whole book, so we’re going to focus on a simple explanation here. First, we’re going to create a measure of offensive efficiency – points per yard of offense. So if you roll up 500 yards of offense but only score 21 points, you’ll score .042 points per yard. A team that gains 250 yards and scores 21 points is more efficient: they score .084 points per yard. So in this model, efficient teams are good. Second, we’ll do the same for the defense, using yards allowed and the opponent’s score. Here, it’s inverted: Defenses that keep points off the board are good. Third, we’ll use turnover margin. Teams that give the ball away are bad, teams that take the ball away are good, and you want to take it away more than you give it away. Using logistic regression and these statistics, our model predicts that Nebraska is actually worse than they were: the Husker’s should have been 2-6. Giving the ball away three times and only scoring 28 points against Rutgers should have doomed the team to a bad loss at the end of the season. But, it didn’t. So how much of a corner would the team need to turn? With modeling, we can figure this out. What would Nebraska’s record if they had a +1 turnover margin and improves offensive production 10 percent? As played, our model gave Nebraska a 32 percent chance of beating Minnesota. If Nebraska were to have a +1 turnover margin, instead of the -2 that really happened, that jumps to a 40 percent chance. If Nebraska were to improve their offense just 10 percent – score a touchdown every 100 yards of offense – Nebraska wins the game. Nebraska wins, they’re 4-4 on the season (and they still don’t beat Iowa). So how close are they to turning the corner? That close. 1.1 Requirements and Conventions This book is all in the R statistical language. To follow along, you’ll do the following: Install the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer. Install R Studio Desktop. The free version is great. Going forward, you’ll see passages like this: install.packages(&quot;tidyverse&quot;) Don’t do it now, but that is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do. 1.2 About this book This book is the collection of class materials for the author’s Advanced Sports Data Analysis class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. There’s some things you should know about it: It is free for students. The topics will remain the same but the text is going to be constantly tinkered with. What is the work of the author is copyright Matt Waite 2021. The text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either. As such, the whole book – authored in Bookdown – is open sourced on Github. Pull requests welcomed! "],["the-modeling-process-and-linear-regression.html", "Chapter 2 The modeling process and linear regression 2.1 Feature engineering 2.2 Setting up the modeling process 2.3 Predicting based on the model 2.4 Predicting data we haven’t seen before 2.5 Looking locally", " Chapter 2 The modeling process and linear regression One of the most common – and seemingly least rigorous – parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method – looking at how a team is playing, looking at the players, consulting their gut – but rarely ever do you hear of a sports pundit using a model. We’re going to change that. Throughout this book, you’ll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not). Let’s start by looking at predicting how many points the team should score given how well they are shooting. And we’ll use this as a chance to look at linear regression modeling. If you don’t have them already installed, we’ll need the tidyverse and tidymodels for this book. As well as zoo to make rolling means and hoopR for data. install.packages(c(&quot;tidyverse&quot;, &quot;tidymodels&quot;, &quot;zoo&quot;, &quot;hoopR&quot;) After they’ve installed – and if you haven’t this will take a bit – load them. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) For this walkthrough, we’re going to use a dataset of college basketball games from the 14-15 season through current games from hoopR. You can pull those from games from the library like this: Let’s load this data and do a little work on it. The first function pulls the team box scores, then I use dplyr’s separate and mutate_at functions to reformat hoopR’s ways of recording shots made and shots attempted. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) 2.1 Feature engineering Feature engineering is the process of using what you know about something – domain knowledge – to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good. Let’s look at basketball games again. A number of basketball heads – including Ken Pomeroy of KenPom fame – have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can’t score if you don’t have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score? One problem? Possessions aren’t in typical metrics. They aren’t usually tracked. But you can estimate them from typical box scores. The way to do that is like this: Possessions = Field Goal Attempts – Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts) Since we’re trying to predict how many points a team will score, we need to know that. If you look at the data, however, you’ll see that’s not actually in the data. Which is unfortunate. But we can calculate it pretty easily. Then we’ll use the possessions estimate formula to get that, so we can then calculate points per possession. While we’re here, we’ll add true shooting percentage as well – to try and incorporate some free throw shooting into our metrics. We’ll save that to a new dataframe called teamstats. teamstats &lt;- teamgames %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100 ) Now we begin the process of creating a model. Modeling in data science has a ton of details, but the process for each model type is similar. Split your data into training and testing data sets. A common split is 80/20. Train the model on the training dataset. Evaluate the model on the training data. Apply the model to the testing data. Evaluate the model on the test data. From there, it’s how you want to use the model. We’ll walk through a simple example here, using the simplest model – a linear model. Linear models are something you’ve understood since you took middle school math and learned the equation of a line. Remember y = mx + b? It’s back. And, unlike what you complained bitterly in middle school, it’s very, very useful. What a linear model says, in words is that we can predict y if we multiply a value – a coefficient – by our x value offset with b, which is really the y-intercept, but think of it like where the line starts. Or, expressed as y = mx + b: points = true_shooting_percentage * ? + some starting point. Think of some starting point as what the score should be if the true_shooting_percentage is zero. Should be zero, right? Intuitively, yes, but it won’t always work out so easily. What we’re trying to do here is predict how many points a team should score given their shooting prowess as a team or their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats BEFORE the game – what we knew about the team going into the game in question. We can do that using zoo and rolling means. We’ll add three new columns – the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage. teamstats &lt;- teamstats %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=4, align=&quot;right&quot;, fill=NA), rolling_ppp = rollmean(lag(ppp, n=1), k=4, align=&quot;right&quot;, fill=NA), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=4, align=&quot;right&quot;, fill=NA) ) %&gt;% ungroup() %&gt;% na.omit() 2.2 Setting up the modeling process With most modeling tasks we need to start with setting a random number seed to aid our random splitting of data into training and testing. set.seed(1234) Random numbers play a large role in a lot of data science algorithms, so setting one helps our reproducibility. After that, we split our data. There’s a number of ways to do this – R has a bunch and you’ll find all kinds of examples online – but Tidymodels has made this easy. game_split &lt;- initial_split(teamstats, prop = .8) game_split ## &lt;Analysis/Assess/Total&gt; ## &lt;45549/11388/56937&gt; What does this mean? It says that initial_split divided the data into 68,000+ games in analysis (or training), 17,000+ into assess (or test), of the 86,000+ total records in the dataset. But the split object isn’t useful to us. We need to assign them to dataframes. We do so like this: game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Now we have two dataframes – game_train and game_test – that we can now use for modeling. First step to making a model is to set what type of model this will be. We’re going to name our model object – lm_model works because this is a linear model. We’ll use the linear_reg function in parsnip (the modeling library in Tidymodels) and set the engine to “lm.” lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) We can get a peek at lm_model and make sure we did everything right by just typing it and executing. lm_model ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Now, let’s fit a linear model to our data. We’ll name the fitted model fit_lm and we’ll take our model object that we just created and fit it using the fit function. What goes in the fit function can be read like this: team_score is approximately modeled by the rolling mean of shooting percentage The only thing left is to specify the dataset. fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) Let’s take a look at what the fitted model object tells us about our data. tidy(fit_lm, conf.int = TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 51.6 0.621 83.1 0 50.4 52.8 ## 2 rolling_shooting_pe… 0.462 0.0139 33.2 2.67e-239 0.435 0.489 The two most important things to see here are the terms and the estimates. Start with rolling_shooting_percentage. What that says is for every 10 percentage points of shooting percentage, a team should score 13 points. HOWEVER, the intercept has something to say about this. What the intercept says is that a team with a big fat zero for shooting percentage is going to score 13 points. Wait … how? Well, are field goals the only way to score in basketball? No. So there’s some of your non-zero intercept. Think again about y = mx + b. We have our terms here: y is team score, m is 1.3 x is the team shooting percentage and b is 13.1. Let’s pretend for a minute that you coached a team that shot 40 percent in college basketball. Our model predicts you would score about 65 points. 2.3 Predicting based on the model Now, we can take the model predictions and bind them to our dataset. This will be a common step throughout this book so we can see what the model predicted vs what the real world produced. trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) Walking through this, we’re creating a dataframe called trainresults, which is game_train with the results of the predict function bound to it. The predict function takes two arguments – the fitted model and the dataset it is being applied to, which in this case is the same dataset. What will result is our game_train dataset with a new column: .pred Our first step in evaluating a linear model is to get the r-squared value. The yardstick library (part of Tidymodels) does this nicely. We tell it to produce metrics on a dataset, and we have to tell it what the real world result is (the truth column) and what the estimate column is (.pred). metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.4 ## 2 rsq standard 0.0237 ## 3 mae standard 9.80 We’ll get more into RMSE and MAE later. For now, focus on rsq or r-squared. What that says is that changes in a four game rolling shooting percentage account for … 2.3 percent of the variation in team score. That’s … not good. A way to look at this is with a scatterplot. The geom_smooth creates its own linear model and puts the line of best fit through our dots. ggplot() + geom_point(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score)) + geom_smooth(data=teamstats, aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; As you can see, there’s a lot of dots above the line and below the line. That gap is a called a residual. The residual is the actual thing minus the predicted thing. The truth minus our guess. A positive residual – in this case – is good. It means that player is scoring more than we’d predict they would. A negative residual means they’re not scoring as much as we’d expect. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% mutate(label = case_when( residual &gt; 0 ~ &quot;Positive&quot;, residual &lt; 0 ~ &quot;Negative&quot;) ) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=team_score, color=label)) + geom_smooth(aes(x=rolling_shooting_percentage, y=team_score), method=&quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; Residuals, aside from telling us who is and isn’t playing well, can tell us if a linear model is appropriate for this data. We can use a scatterplot to reveal this. trainresults %&gt;% mutate(residual = team_score - .pred) %&gt;% ggplot() + geom_point(aes(x=rolling_shooting_percentage, y=residual)) What we’re looking for is for the dots to be randomly spaced around the plot. It should look like someone spilled Skittles on the floor. This … does. It means a linear model is appropriate here. More on that in the coming chapters. 2.4 Predicting data we haven’t seen before Now we can do the same thing, but with the test data. testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) What do these metrics look like? metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.4 ## 2 rsq standard 0.0214 ## 3 mae standard 9.85 If you look at the r-squared value, you’ll note that when we apply the same model to our test data, the amount of variance that we can explain goes down a little. It’s not much, so the model does the same job predicting the outcome that the training data did, which is the point of a model. It’s just, in our simple model here, it’s doing a terrible job of it. 2.5 Looking locally We can get clearer picture of what these predictions look like if we look at something we know – like this season’s Nebraska team. What does the model say about how they are doing? First, we can get Nebraska’s games with a filter. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) Now apply the model to the games. nupreds &lt;- nu %&gt;% bind_cols(predict(fit_lm, nu)) To really see this clearly, we’ll calculate the residual, then sort by the residual. Where did the model miss the most, for good or bad? nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 24 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 75.4 24.6 ## 2 2021-12-23 Nebraska Kennesaw State 88 68.3 19.7 ## 3 2021-11-21 Nebraska Southern 82 71.3 10.7 ## 4 2022-01-03 Nebraska Ohio State 79 70.0 8.96 ## 5 2022-02-02 Nebraska Michigan 79 70.3 8.72 ## 6 2021-11-27 Nebraska South Dakota 83 74.3 8.65 ## 7 2022-02-10 Nebraska Minnesota 78 70.7 7.35 ## 8 2022-02-26 Nebraska Iowa 78 71.6 6.42 ## 9 2021-11-24 Nebraska Tennessee State 79 73.0 6.05 ## 10 2022-02-13 Nebraska Iowa 75 71.6 3.39 ## # … with 14 more rows What does this mean? It says the model predicted the team would score 75 against NC State and they put up 100, for a 25 point miss (residual). Might have had something to do with that game going to three overtimes, but alas, our model can’t get everything right. For most games, the prediction is within a few points, with some odd games with larger misses. Linear models are incredibly important to understand — they underpin many of the more advanced methods we’ll talk about going forward — so understanding them now is critical. "],["multiple-regression.html", "Chapter 3 Multiple regression 3.1 A multiple regression speed run 3.2 Picking what moves the needle", " Chapter 3 Multiple regression As we saw in the previous chapter, we can measure how much something can be predicted by another thing. We looked at how many points a team can score based on their shooting percentage. The theory being how well you shoot the ball probably has a lot to say about how many points you score. And what did we find? It’s a part of the story, but not the whole story. But that raises the problem with simple regressions – they’re simple. Anyone who has watched a basketball game knows there’s a lot more to the outcome than just shooting prowess. Enter the multiple regression. Multiple regressions are a step toward reality – where more than one thing influences the outcome. However, the more variance we attempt to explain, the more error and uncertainty we introduce into our model. Let’s begin by loading some libraries and installing a new one: corrr library(tidyverse) library(tidymodels) library(zoo) library(hoopR) library(corrr) For this, we’ll work with our college basketball game data and we’ll continue down the road we started in the last chapter. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) 3.1 A multiple regression speed run First, let’s restore what we did in last chapter with the feature engineering we did, making the different metrics and the rolling numbers. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=4, align=&quot;right&quot;, fill=NA), rolling_ppp = rollmean(lag(ppp, n=1), k=4, align=&quot;right&quot;, fill=NA), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=4, align=&quot;right&quot;, fill=NA) ) %&gt;% ungroup() %&gt;% na.omit() Now we’ll split our data into training and testing, creating a linear model predicting score from the shooting percentage and producing the metrics for the results. set.seed(1234) game_split &lt;- initial_split(teamstats, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.4 ## 2 rsq standard 0.0237 ## 3 mae standard 9.80 Bottom line: We can predict about 2.3 percent of the difference in team scores by the rolling shooting percentage. But we know, because we’ve shot hoops in the driveway before, or went through a basketball unit in PE in the third grade, that sure, being a good shooter is important, but how many times you shoot the ball is also important. If you’re a 100 percent shooter, that’s insane, but it probably means you took one shot. Congrats, you scored two points (three if you’re gutsy). One shot is not going to win a game. In our feature engineering, we created another metric – points per posession. It’s a measure of efficiency – did you score when you had the ball? We created a rolling metric for this too. To add it to our model, it’s as simple as just adding + rolling_ppp fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_shooting_percentage + rolling_ppp, data = game_train) trainresults &lt;- game_train %&gt;% bind_cols(predict(fit_lm, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.3 ## 2 rsq standard 0.0448 ## 3 mae standard 9.69 And just like that, by acknowledging reality, we’ve jumped to … almost 5 percent of variance explained and our root mean squared error – the average amount we’re off by – has not really budged. Your temptation now is to start adding things until we get to 1 on the r-squared and 0 on the rmse. The problem with that is called “overfitting” Overfitting is where you produce a model that is too close to your training data, which makes it prone to fail with data you’ve never seen before – the model becomes unreliable when it’s not the training data anymore. A secondary problem you encounter is this: the point of this is to predict future events. In this class, we’re attempting to predict the outcome of things that have not yet happened. That means we are going to be estimating the inputs to these models, inputs that will no doubt have error. So our inputs have a range of possible outcomes, our model is not perfect, so the outcome is going to combine the two. The more elements of your model that you use as inputs, the more error – uncertainty – you are introducing. The point is you want to pick the things that really matter and ignore the rest in some vain quest to get to 100 percent. You won’t get there. 3.2 Picking what moves the needle There are multiple ways to find the right combination of inputs to your models. With multiple regressions, the most common is the correlation matrix. We’re looking to maximize r-squared by choosing inputs that are highly correlated to our target value, but not correlated with other things. Example: We can assume that field_goals_made and field_goal_pct are highly correlated to team_score, but the number of Field Goals made is also highly correlated with the field goal percentage. Using corrr, we can create a correlation matrix in a dataframe to find columns that are highly correlated with our target – team_score. To do this, we need to select the columns we’re working with – our three rolling metrics. teamstats %&gt;% select(team_score, rolling_shooting_percentage, rolling_ppp, rolling_true_shooting_percentage) %&gt;% correlate() ## # A tibble: 4 × 5 ## term team_score rolling_shooting_pe… rolling_ppp rolling_true_shooti… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 team_score NA 0.152 0.208 0.167 ## 2 rolling_shoo… 0.152 NA 0.798 0.919 ## 3 rolling_ppp 0.208 0.798 NA 0.851 ## 4 rolling_true… 0.167 0.919 0.851 NA Reading this can be a lot, and it helps to take some notes as you go. You read up and down and left and right – it’s a matrix. Follow the rolling_true_shooting_percentage row across to the rolling_shooting_percentage column and you’ll see they’re almost perfectly correlated with each other – 1 is a perfect correlation. What does that mean? It means including both is going to just add error without adding much value. They’re so similar. You pick the one that is more highly correlated with team_score – true shooting. So how does this look in a model? Since we already have the features – the columns – in our data and we have it split into training and testing, we just need to create a new fit. new_fit_lm &lt;- lm_model %&gt;% fit(team_score ~ rolling_true_shooting_percentage + rolling_ppp, data = game_train) Let’s take a peek at our model coefficients. tidy(new_fit_lm, conf.int = TRUE) ## # A tibble: 3 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 43.7 0.691 63.2 0 42.3 45.0 ## 2 rolling_true_shooti… -0.0990 0.0236 -4.20 2.70e- 5 -0.145 -0.0528 ## 3 rolling_ppp 32.6 1.18 27.7 2.74e-167 30.3 34.9 Now let’s apply. newtrainresults &lt;- game_train %&gt;% bind_cols(predict(new_fit_lm, game_train)) metrics(newtrainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.3 ## 2 rsq standard 0.0446 ## 3 mae standard 9.69 So … we’re up to an r-squared of .043 and an rmse a little lower than before. The goal here is to add to r-squared and reduce our error metrics. How well does the model do with data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(fit_lm, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.3 ## 2 rsq standard 0.0395 ## 3 mae standard 9.74 It’s remarkably stable. Our rmse is all but unchanged as is our r-squared. Until we find other metrics or other methods, this is what we’ve got. Let’s put this against a set of games we’re familiar with. nu &lt;- teamstats %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(new_fit_lm, nu)) nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(game_date, team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 24 × 6 ## game_date team_short_display_name opponent_name team_score .pred residual ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-12-02 Nebraska NC State 100 74.5 25.5 ## 2 2021-12-23 Nebraska Kennesaw State 88 66.2 21.8 ## 3 2022-01-03 Nebraska Ohio State 79 68.3 10.7 ## 4 2022-02-02 Nebraska Michigan 79 68.5 10.5 ## 5 2021-11-21 Nebraska Southern 82 72.1 9.91 ## 6 2021-11-27 Nebraska South Dakota 83 73.4 9.58 ## 7 2022-02-10 Nebraska Minnesota 78 68.4 9.55 ## 8 2022-02-26 Nebraska Iowa 78 71.0 6.96 ## 9 2021-11-24 Nebraska Tennessee State 79 72.8 6.24 ## 10 2022-02-13 Nebraska Iowa 75 69.8 5.24 ## # … with 14 more rows The question to start thinking about is this – what else could we include? "],["decision-trees-and-random-forests.html", "Chapter 4 Decision trees and random forests 4.1 An intro to pre-processing 4.2 Decision trees 4.3 Random forest", " Chapter 4 Decision trees and random forests Tree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less than x seasons of experience? Do they have more or less then y minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have on average. The upside of decision trees is that if the model is small, you can explain it to anyone. They’re very easy to understand. The trouble with decision trees is that if the model is small, they’re a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree. The most common is the random forest. Let’s implement one. We start with libraries. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) library(corrr) We’ll be using college basketball games again. Let’s load this data and add our rolling metrics right away. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_ppp = rollmean(lag(ppp, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=2, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() More often than not, we need to do more than just use the data we have. Often, with modeling, we need to pre-process our data. Pre-processing can mean a lot of things – fixing dates, creating new features, scaling numbers to be similar – but it’s all about making your models better. 4.1 An intro to pre-processing To simplify things, we’re going to first simplify our data. We want to start with a minimum of columns. We need the columns to help us identify individual records, we need our predictors and we need the outcome we’re trying to predict. modelgames &lt;- teamstats %&gt;% select(team_short_display_name, opponent_name, game_date, season, team_score, rolling_ppp, rolling_free_throw_factor, rolling_turnover_percentage) %&gt;% na.omit() Now we need to split our data into training and testing sets. set.seed(1234) game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) Going forward, we’re going to make our lives easier by using workflows. Workflows in tidymodels take in a pre-processing recipe and a model definition and executes those things to make our modeling code slimmer and our lives easier. To start, we need to define a pre-processing recipe. The recipe defines a series of steps that will be performed on your data. We’ll start simple and add our formula from previous work. score_rec &lt;- recipe(team_score ~ rolling_ppp + rolling_turnover_percentage + rolling_free_throw_factor, data = game_train) Another, more flexible way to express this, is using the . to say all predictors. In this case, all predictors is rolling_ppp, rolling_turnover_percentage and rolling_free_throw_factor. What follows is the same as above, just less typing. But we’re also going to add a role to our recipe. In this case, the role is how we’re going to identify each row – an ID. In this case, to identify a game, we need to know the Team, the Opponent, the Date and the Season. What isn’t an ID is a predictor. score_rec &lt;- recipe(team_score ~ ., data = game_train) %&gt;% update_role(team_short_display_name, opponent_name, game_date, season, new_role = &quot;ID&quot;) Now that we’ve created our pre-processing recipe, we can create our model definition. 4.2 Decision trees As discussed earlier, decision trees are essentially a series of if/else statements. Visualized, they look like branches on a tree (thus, decision trees). We’ve already defined a recipe for our data, so now we’re ready to define a model definition. First, we’ll use decision trees to prove a point. tree &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;regression&quot;) Now we’ll create the workflow. In its simplest form, the workflow defines itself as a workflow and then adds a recipe and a model definition. tree_wf &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(tree) Now we can fit the data with our model using the workflow. This applies our recipe to the data without us having to do it, then uses the model definition to do the fitting. tree_fit &lt;- tree_wf %&gt;% fit(data = game_train) What does this produce? Here’s what a basic decision tree looks like. tree_fit %&gt;% pull_workflow_fit() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. ## parsnip model object ## ## Fit time: 747ms ## n= 64401 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 64401 10648420 71.49446 ## 2) rolling_ppp&lt; 1.010671 26879 4435055 69.41717 * ## 3) rolling_ppp&gt;=1.010671 37522 6014292 72.98254 * They can be a bit tough to read, but take the bottom nodes. It says if rolling_ppp is greater than or equal to 1.01, your score is around 72 points a game. If it’s less than 1.01, you’ll score around 69 points. Easy to understand, right? The algorithm cuts branches when the splits stop reducing error, and there’s a limit But here’s where the crude instrument comes in. Let’s use our decision tree to predict some scores. treeresults &lt;- game_train %&gt;% bind_cols(predict(tree_fit, game_train)) What are the accuracy metrics we get? metrics(treeresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.7 ## 2 rsq standard 0.0187 ## 3 mae standard 10.0 Our rsquared is about .018, which is terrible. And our MAE says we’re off by 10 points on average, but our RSME says were off by 13 points on a different average, indicating there’s some big misses. We can do better. 4.3 Random forest Enter the random forest. A random forest is, as the name implies, a large number of decision trees, and they use a random set of inputs. The algorithm creates a large number of randomly selected training inputs, and randomly chooses the feature input for each branch, creating predictions. The goal is to create uncorrelated forests of trees. The trees all make predictions, and the wisdom of the crowds takes over. In the case of classification algorithm, the most common prediction is the one that gets chosen. In a regression model, the predictions get averaged together. The random part of random forest is in how the number of tree splits get created and how the samples from the data are taken to generate the splits. They’re randomized, which has the effect of limiting the influence of a particular feature and prevents overfitting – where your predictions are so tailored to your training data that they miss badly on the test data. For random forests, we change the model type to rand_forest and set the engine to “ranger.” There’s multiple implementations of the random forest algorithm, and the differences between them are beyond the scope of what we’re doing here. rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) And now we can create our workflow. We first need to define it as a workflow, then add the model and add the recipe. score_wflow &lt;- workflow() %&gt;% add_recipe(score_rec) %&gt;% add_model(rf_mod) score_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Random Forest Model Specification (regression) ## ## Computational engine: ranger With the workflow in place, we can fit our model. Note: this can make your laptop fan go wheeeeee. score_fit &lt;- score_wflow %&gt;% fit(data = game_train) Now we can use a use a new function – pull_workflow_fit, which pulls the fit stats we want to see to evaluate it. score_fit %&gt;% pull_workflow_fit() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. ## parsnip model object ## ## Fit time: 1m 17s ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 64401 ## Number of independent variables: 3 ## Mtry: 1 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 169.9385 ## R squared (OOB): -0.02776131 Similar to previous work, we can bind the prediction to our training data and evaluate the model. trainresults &lt;- game_train %&gt;% bind_cols(predict(score_fit, game_train)) metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 6.22 ## 2 rsq standard 0.935 ## 3 mae standard 4.84 Note: The RMSE for this model is down to 6.2. The R-squared is absurdly high – an indication of overfitting. It built a model perfectly tailored to the training data. But how does this model handle data it hasn’t seen before? testresults &lt;- game_test %&gt;% bind_cols(predict(score_fit, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 13.1 ## 2 rsq standard 0.0130 ## 3 mae standard 10.3 Poorly. How well does the random forest algorithm do with Nebraska’s schedule in 2022? nu &lt;- modelgames %&gt;% filter(season == 2022, team_short_display_name == &quot;Nebraska&quot;) nupreds &lt;- nu %&gt;% bind_cols(predict(score_fit, nu)) nupreds %&gt;% mutate(residual = team_score - .pred) %&gt;% arrange(desc(residual)) %&gt;% select(team_short_display_name, opponent_name, team_score, .pred, residual) ## # A tibble: 26 × 5 ## team_short_display_name opponent_name team_score .pred residual ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nebraska NC State 100 87.8 12.2 ## 2 Nebraska Kennesaw State 88 77.4 10.6 ## 3 Nebraska Ohio State 79 71.3 7.72 ## 4 Nebraska Southern 82 75.9 6.12 ## 5 Nebraska Michigan 79 73.0 6.01 ## 6 Nebraska Minnesota 78 72.2 5.83 ## 7 Nebraska South Dakota 83 78.0 5.00 ## 8 Nebraska Iowa 75 70.7 4.26 ## 9 Nebraska Tennessee State 79 74.9 4.13 ## 10 Nebraska Idaho State 78 73.9 4.12 ## # … with 16 more rows Compare this to the multiple regression of the previous chapter. Take just the NC State game as an example. The multiple regression model was off by 26 points – which is a lot, but it’s a triple overtime game. But the random forest managed to be off by 12. "],["xgboost.html", "Chapter 5 XGBoost 5.1 Hyperparameters", " Chapter 5 XGBoost As we learned in the previous chapter, random forests (and bagged methods) average together a large number of trees to get to an answer. Random forests add a wrinkle by randomly choosing features at each branch to make it so each tree is not correlated and the trees are rather deep. The idea behind averaging them together is to cut down on the variance in predictions – random forests tend to be somewhat harder to fit to unseen data because of the variance. Random forests are fairly simple to implement, and are very popular. Boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on. The path to a boosted method is complex, the results can take a lot of your computer’s time, but the models are more generalizable, meaning they handle new data better than other methods. Among data scientists, boosted methods, such as xgboost, are very popular for solving a wide variety of problems. Let’s re-implement our predictions in an XGBoost algorithm. First, we’ll load libraries and we’re going to introduce a new one here – doParallel – which handles using more of your computer’s processor cores to accomplish tasks in parallel instead of one core at a time. In other words, instead of one task, it can do X at a time in parallel and put the answers together after. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) library(doParallel) cores &lt;- parallel::detectCores(logical = FALSE) We’ll load our game data and do a spot of feature engineering that we used with random forests. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted), ppp = team_score/possessions, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, rolling_shooting_percentage = rollmean(lag(field_goal_pct, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_ppp = rollmean(lag(ppp, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=2, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), rolling_orb = rollmean(lag(orb, n=1), k=2, align=&quot;right&quot;, fill=NA), rolling_drb = rollmean(lag(drb, n=1), k=2, align=&quot;right&quot;, fill=NA) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) modelgames &lt;- newteamstats %&gt;% select(team_short_display_name, opponent_name, game_date, season, team_score, rolling_true_shooting_percentage, rolling_free_throw_factor, rolling_turnover_percentage, rolling_orb, rolling_drb) %&gt;% na.omit() Per usual, we split our data into training and testing. game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) And our recipe. game_rec &lt;- recipe(team_score ~ ., data = game_train) %&gt;% update_role(team_short_display_name, opponent_name, game_date, season, new_role = &quot;ID&quot;) summary(game_rec) ## # A tibble: 10 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 team_short_display_name nominal ID original ## 2 opponent_name nominal ID original ## 3 game_date date ID original ## 4 season numeric ID original ## 5 rolling_true_shooting_percentage numeric predictor original ## 6 rolling_free_throw_factor numeric predictor original ## 7 rolling_turnover_percentage numeric predictor original ## 8 rolling_orb numeric predictor original ## 9 rolling_drb numeric predictor original ## 10 team_score numeric outcome original To this point, everything looks like what we’ve done before. Nothing has really changed. It’s about to. 5.1 Hyperparameters The hyperparameters are the inputs into the algorithm that make the fit. To find the ideal hyperparameters, you need to tune them. But first, let’s talk about the hyperparameters: Number of trees – this is the total number of trees in the sequence. A gradient boosting algorithm will minimize residuals forever, so you need to tell it where to stop. That stopping point is different for every problem. Learn rate – this controls how fast the algorithm goes down the gradient descent – how fast it learns. Too fast and you’ll overshoot the optimal stopping point and start going up the error curve. Too slow and you’ll never get to the optimal stopping point. Tree depth – controls the depth of each individual tree. Too short and you’ll need a lot of them to get good results. Too deep and you risk overfitting. Minimum number of observations in the terminal node – controls the complexity of each tree. Typical values range from 5-15, and higher values keep a model from figuring out relationships that are unique to that training set (ie overfitting). Other settings: Loss reduction – this is the minimum loss reduction to make a new tree split. If the improvement hits this minimum, a split occurs. A low value and you get a complex tree. High value and you get a tree more robust to new data, but it’s more conservative. Sample size – The fraction of the total training set that can be used for each boosting round. Low values may lead to underfitting, high to overfitting. mtry – the number of predictors that will be randomly sampled at each split when making trees. All of these combine to make the model, and each has their own specific ideal. How do we find it? Tuning. First, we make a mode and label each parameter as tune() xg_mod &lt;- boost_tree( trees = tune(), learn_rate = tune(), tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), ) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;xgboost&quot;, nthread = cores) Let’s make a workflow now that we have our recipe and our model. game_wflow &lt;- workflow() %&gt;% add_model(xg_mod) %&gt;% add_recipe(game_rec) Now, to tune the model, we have to create a grid. The grid is essentially a random sample of parameters to try. The latin hypercube is a method of creating a near-random sample of parameter values in multidimentional distributions (ie there’s more than one predictor). The latin hypercube is near-random because there has to be one sample in each row and column of the hypercube. Essentially, it removes the possibility of totally empty spaces in the cube. What follows is what parameters the hypercube will tune. xgb_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), game_train), learn_rate(), size = 30 ) xgb_grid ## # A tibble: 30 × 7 ## trees tree_depth min_n loss_reduction sample_size mtry learn_rate ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1749 13 5 1.18e-3 0.581 4 0.0000000175 ## 2 262 10 34 2.21e-7 0.626 2 0.000000158 ## 3 1837 11 37 7.42e-9 0.825 4 0.00204 ## 4 524 12 28 3.06e-9 0.282 7 0.00000390 ## 5 981 15 29 9.62e-3 0.331 8 0.00000000421 ## 6 1674 3 9 2.90e+1 0.517 9 0.000000400 ## 7 1210 8 31 2.12e-5 0.408 6 0.000266 ## 8 1912 12 25 8.83e-9 0.930 3 0.00961 ## 9 403 4 10 2.41e-8 0.439 9 0.00145 ## 10 170 8 38 6.28e-7 0.658 6 0.000138 ## # … with 20 more rows How do we tune it? Using something called cross fold validation. Cross fold validation takes our grid, applies it to a set of subsets (in our case 10 subsets) and compares. When it’s done, each validation set will have a set of tuned values and outcomes that we can evaluate and pick the optimal set to get a result. game_folds &lt;- vfold_cv(game_train) game_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [57960/6441]&gt; Fold01 ## 2 &lt;split [57961/6440]&gt; Fold02 ## 3 &lt;split [57961/6440]&gt; Fold03 ## 4 &lt;split [57961/6440]&gt; Fold04 ## 5 &lt;split [57961/6440]&gt; Fold05 ## 6 &lt;split [57961/6440]&gt; Fold06 ## 7 &lt;split [57961/6440]&gt; Fold07 ## 8 &lt;split [57961/6440]&gt; Fold08 ## 9 &lt;split [57961/6440]&gt; Fold09 ## 10 &lt;split [57961/6440]&gt; Fold10 This part takes about 25-30 minutes on my machine and it will saturate all of your processors, so your computer just needs to sit there. No texting, no YouTube, nothing. Let it burn. doParallel::registerDoParallel(cores = cores) xgb_res &lt;- tune_grid( game_wflow, resamples = game_folds, grid = xgb_grid, control = control_grid(save_pred = TRUE) ) doParallel::stopImplicitCluster() xgb_res ## Warning: This tuning result has notes. Example notes on model fitting include: ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## internal: A correlation computation is required, but `estimate` is constant and has 0 standard deviation, resulting in a divide by 0 error. `NA` will be returned. ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [57960/6441]&gt; Fold01 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 2 &lt;split [57961/6440]&gt; Fold02 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 3 &lt;split [57961/6440]&gt; Fold03 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 4 &lt;split [57961/6440]&gt; Fold04 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 5 &lt;split [57961/6440]&gt; Fold05 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 6 &lt;split [57961/6440]&gt; Fold06 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 7 &lt;split [57961/6440]&gt; Fold07 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 8 &lt;split [57961/6440]&gt; Fold08 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 9 &lt;split [57961/6440]&gt; Fold09 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… ## 10 &lt;split [57961/6440]&gt; Fold10 &lt;tibble [60 × 11]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble [193… So our grid has run on all of our validation samples, and what do we see? collect_metrics(xgb_res) ## # A tibble: 60 × 13 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 555 13 1 0.00000138 0.110 0.575 rmse ## 2 9 555 13 1 0.00000138 0.110 0.575 rsq ## 3 5 103 15 2 0.00517 0.899 0.538 rmse ## 4 5 103 15 2 0.00517 0.899 0.538 rsq ## 5 1 683 39 2 0.00000263 0.000358 0.852 rmse ## 6 1 683 39 2 0.00000263 0.000358 0.852 rsq ## 7 9 1674 9 3 0.000000400 29.0 0.517 rmse ## 8 9 1674 9 3 0.000000400 29.0 0.517 rsq ## 9 2 1947 23 3 0.0000000107 4.30 0.947 rmse ## 10 2 1947 23 3 0.0000000107 4.30 0.947 rsq ## # … with 50 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, ## # n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt; Well we see 60 combinations and the metrics from them. But that doesn’t mean much to us just eyeballing it. We want to see the best combination. show_best(xgb_res, &quot;rmse&quot;) ## # A tibble: 5 × 13 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 908 7 6 0.0246 0.0646 0.721 rmse ## 2 3 1912 25 12 0.00961 0.00000000883 0.930 rmse ## 3 4 1837 37 11 0.00204 0.00000000742 0.825 rmse ## 4 2 665 12 9 0.0520 2.06 0.350 rmse ## 5 9 1633 21 13 0.0454 0.00000134 0.756 rmse ## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt; The best combination as of this data update comes up with an RMSE of 9.93. Second is 9.97. Let’s capture our best set of hyperparameters. best_rmse &lt;- select_best(xgb_res, &quot;rmse&quot;) And now put that into a final workflow. Pay attention to the main arguments in the output below. final_xgb &lt;- finalize_workflow( game_wflow, best_rmse ) final_xgb ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = 3 ## trees = 908 ## min_n = 7 ## tree_depth = 6 ## learn_rate = 0.0245913955268473 ## loss_reduction = 0.0646470652479078 ## sample_size = 0.72087624378968 ## ## Engine-Specific Arguments: ## nthread = cores ## ## Computational engine: xgboost There’s our best set of hyperparameters. We’ve tuned this model to give the best possible set of results in those settings. Now we apply it like we have been doing all along. We create a fit. xg_fit &lt;- final_xgb %&gt;% fit(data = game_train) We can see something things about that fit, including all the iterations of our XGBoost model. Note: our tuned number of trees is 1,743 – and in the workflow fit, you can see 1,743 iterations. Remember: Boosted models work sequentially. One after the other. So you can see it at work. The RMSE goes down with each iteration as we go down the gradient desent. xg_fit %&gt;% pull_workflow_fit() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. ## parsnip model object ## ## Fit time: 31.4s ## ##### xgb.Booster ## raw: 4.1 Mb ## call: ## xgboost::xgb.train(params = list(eta = 0.0245913955268473, max_depth = 6L, ## gamma = 0.0646470652479078, colsample_bytree = 1, colsample_bynode = 0.6, ## min_child_weight = 7L, subsample = 0.72087624378968, objective = &quot;reg:squarederror&quot;), ## data = x$data, nrounds = 908L, watchlist = x$watchlist, verbose = 0, ## nthread = 6L) ## params (as set within xgb.train): ## eta = &quot;0.0245913955268473&quot;, max_depth = &quot;6&quot;, gamma = &quot;0.0646470652479078&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.6&quot;, min_child_weight = &quot;7&quot;, subsample = &quot;0.72087624378968&quot;, objective = &quot;reg:squarederror&quot;, nthread = &quot;6&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 5 ## niter: 908 ## nfeatures : 5 ## evaluation_log: ## iter training_rmse ## 1 70.42922 ## 2 68.75217 ## --- ## 907 11.62990 ## 908 11.62942 Now, like before, we can bind our predictions using our xg_fit to the game_train data. trainresults &lt;- game_train %&gt;% bind_cols(predict(xg_fit, game_train)) And now see how we did. metrics(trainresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.6 ## 2 rsq standard 0.202 ## 3 mae standard 9.18 How about the test data? testresults &lt;- game_test %&gt;% bind_cols(predict(xg_fit, game_test)) metrics(testresults, truth = team_score, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 12.5 ## 2 rsq standard 0.0766 ## 3 mae standard 9.85 Unlike the random forest, not nearly the drop in metrics between train and test. "],["logistic-regression.html", "Chapter 6 Logistic Regression 6.1 Visualizing the decision boundary 6.2 The logistic regression 6.3 Evaluating the fit 6.4 Comparing it to test data 6.5 How well did it do with Nebraska?", " Chapter 6 Logistic Regression Up to this point, we’ve been dealing with problems that lead to a quantitative answer: We want a number. How many points? How many possessions? But there are lots of problems in the world where the answer is a classification: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not? These are problems of classification and they use many of the same algorithms we’ve used to try and predict those classes. Ultimately, the algorithms will predict the probability that this row is X or Y and make a decision based on that probability. That probability will be somewhere between 0 and 1, with 0 being no chance and 1 being a sure thing. Where this gets interesting is in the middle. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) What we need to do here is get both sides of the game. We’ll start with getting the box scores. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) Now we’ll create the team side of the game. teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=&quot;right&quot;, fill=NA) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) team_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() Now we’ll use use the same dataframe and rename some columns to create the opponent side of the game. opponent_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() %&gt;% rename( opponent_id = team_id, opponent_short_display_name = team_short_display_name, opponent_score = team_score, opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage, opponent_rolling_free_throw_factor = team_rolling_free_throw_factor, opponent_rolling_turnover_percentage = team_rolling_turnover_percentage, opponent_rolling_orb = team_rolling_orb, opponent_rolling_drb = team_rolling_drb ) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) Now we’ll join them together. games &lt;- team_side %&gt;% inner_join(opponent_side) ## Joining, by = c(&quot;game_id&quot;, &quot;opponent_id&quot;) The last problem to solve? Who won? We can add this with conditional logic. The other thing we’re doing here is we’re going to is we’re going to convert our new TeamResult column into a factor. What is a factor? A factor is a type of data in R that stores categorical values that have a limited number of differences. So wins and losses are a perfect factor. Modeling libraries are looking for factors so it can treat the differences in the data as categories, so that’s why we’re converting it here. games &lt;- games %&gt;% mutate( TeamResult = as.factor(case_when( team_score &gt; opponent_score ~ &quot;W&quot;, opponent_score &gt; team_score ~ &quot;L&quot; ))) %&gt;% na.omit() Now that we’ve done that, we need to look at the order of our factors. levels(games$TeamResult) ## [1] &quot;L&quot; &quot;W&quot; The order listed here is the order they are in. What this means is that our predictions will be done through the lens of losses. That doesn’t make intuitive sense to us, so we can reorder the factors with relevel. games$TeamResult &lt;- relevel(games$TeamResult, ref=&quot;W&quot;) levels(games$TeamResult) ## [1] &quot;W&quot; &quot;L&quot; For simplicity, let’s limit the number of columns we’re going to feed our model. modelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, team_rolling_true_shooting_percentage, opponent_rolling_true_shooting_percentage, team_rolling_turnover_percentage, opponent_rolling_turnover_percentage, TeamResult) 6.1 Visualizing the decision boundary This is just one dimension of the data, but it can illustrate how this works. You can almost see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. You can read it like this: If this team shoots this well and the opponent shoots this well, most of the time this team wins. Or loses. It just depends on where the dot ends up. That neatly captures the probabilities we’re looking at here. ggplot() + geom_point(data=games, aes(x=team_rolling_true_shooting_percentage, y=opponent_rolling_true_shooting_percentage, color=TeamResult)) 6.2 The logistic regression Much of implementing classification algorithms should look familiar by now. The steps we’ve been using are steps we will use again. First, we split into training and testing. log_split &lt;- initial_split(modelgames, prop = .8) log_train &lt;- training(log_split) log_test &lt;- testing(log_split) We create a recipe. In this case, we need to normalize our predictors so scale differences don’t create undue influences. This will turn all of our numbers into zscores. log_recipe &lt;- recipe(TeamResult ~ ., data = log_train) %&gt;% update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = &quot;ID&quot;) %&gt;% step_normalize(all_predictors()) summary(log_recipe) ## # A tibble: 10 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 game_id numeric ID original ## 2 game_date date ID original ## 3 team_short_display_name nominal ID original ## 4 opponent_short_display_name nominal ID original ## 5 season numeric ID original ## 6 team_rolling_true_shooting_percentage numeric predictor original ## 7 opponent_rolling_true_shooting_percentage numeric predictor original ## 8 team_rolling_turnover_percentage numeric predictor original ## 9 opponent_rolling_turnover_percentage numeric predictor original ## 10 TeamResult nominal outcome original We have four predictors – how well each team shot, and how much each team turned the ball over. Now we define the model. Note the set_mode. log_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) Now we have enough for a workflow. log_workflow &lt;- workflow() %&gt;% add_model(log_mod) %&gt;% add_recipe(log_recipe) And now we fit our model (this can take a few minutes). log_fit &lt;- log_workflow %&gt;% fit(data = log_train) 6.3 Evaluating the fit With logistic regression, there’s two things we’re looking at: The prediction and the probabilities. We can get those with two different fits and combine them together. trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train) %&gt;% bind_cols(log_train) trainpredict ## # A tibble: 44,006 × 11 ## .pred_class game_id game_date team_short_displa… opponent_short_di… season ## &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 L 401171532 2020-01-31 Presbyterian SC Upstate 2020 ## 2 L 401175909 2020-01-31 FAU Western KY 2020 ## 3 W 401168419 2020-02-13 Villanova Marquette 2020 ## 4 W 400916553 2016-12-23 Presbyterian East Carolina 2017 ## 5 W 401083261 2019-02-06 Marquette St. John&#39;s 2019 ## 6 L 401086619 2019-02-28 Maine UMass Lowell 2019 ## 7 L 401377774 2022-02-26 East Carolina Tulsa 2022 ## 8 L 400916936 2017-01-21 Samford Chattanooga 2017 ## 9 W 400915795 2016-12-23 BYU CSU Bakersfield 2017 ## 10 L 400917170 2017-02-15 N Illinois Ball State 2017 ## # … with 43,996 more rows, and 5 more variables: ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train, type=&quot;prob&quot;) %&gt;% bind_cols(trainpredict) trainpredict ## # A tibble: 44,006 × 13 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.447 0.553 L 401171532 2020-01-31 Presbyterian ## 2 0.346 0.654 L 401175909 2020-01-31 FAU ## 3 0.533 0.467 W 401168419 2020-02-13 Villanova ## 4 0.607 0.393 W 400916553 2016-12-23 Presbyterian ## 5 0.535 0.465 W 401083261 2019-02-06 Marquette ## 6 0.356 0.644 L 401086619 2019-02-28 Maine ## 7 0.346 0.654 L 401377774 2022-02-26 East Carolina ## 8 0.464 0.536 L 400916936 2017-01-21 Samford ## 9 0.642 0.358 W 400915795 2016-12-23 BYU ## 10 0.260 0.740 L 400917170 2017-02-15 N Illinois ## # … with 43,996 more rows, and 7 more variables: ## # opponent_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; There’s several metrics to look at, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. metrics(trainpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.624 ## 2 kap binary 0.248 Another way to look at that is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy. trainpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 13554 8207 ## L 8343 13902 The roc_auc metric is largely a graphical representation of how well the classifier did. The higher the roc_auc, the better, but too high and you’ve likely overfit the data. We can look at the roc_auc metric for both sides of our prediction. roc_auc(trainpredict, truth = TeamResult, .pred_W) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.672 But is quite confident on Loses. roc_auc(trainpredict, truth = TeamResult, .pred_L) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.328 The advantage of the roc_auc curve is that you can visualize it. roc_data &lt;- roc_curve(trainpredict, truth = TeamResult, .pred_W) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.4 Comparing it to test data Now we can apply our fit to the test data to see how robust it is. Short version: Pretty good. Our numbers don’t dip all that much. testpredict &lt;- log_fit %&gt;% predict(new_data = log_test) %&gt;% bind_cols(log_test) testpredict ## # A tibble: 11,002 × 11 ## .pred_class game_id game_date team_short_displa… opponent_short_di… season ## &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 L 400588682 2014-12-13 SC Upstate Maryland 2015 ## 2 L 400586182 2014-12-14 Rider Hartford 2015 ## 3 W 400586182 2014-12-14 Hartford Rider 2015 ## 4 W 400589312 2014-12-18 Gardner-Webb Jacksonville 2015 ## 5 W 400585975 2014-12-20 Norfolk State N Arizona 2015 ## 6 L 400586931 2014-12-20 Denver Colorado State 2015 ## 7 W 400587857 2014-12-20 Ohio State North Carolina 2015 ## 8 L 400587858 2014-12-20 Wake Forest Florida 2015 ## 9 W 400588379 2014-12-20 Providence UMass 2015 ## 10 W 400588692 2014-12-20 Princeton Lipscomb 2015 ## # … with 10,992 more rows, and 5 more variables: ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; testpredict &lt;- log_fit %&gt;% predict(new_data = log_test, type=&quot;prob&quot;) %&gt;% bind_cols(testpredict) testpredict ## # A tibble: 11,002 × 13 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.371 0.629 L 400588682 2014-12-13 SC Upstate ## 2 0.494 0.506 L 400586182 2014-12-14 Rider ## 3 0.501 0.499 W 400586182 2014-12-14 Hartford ## 4 0.733 0.267 W 400589312 2014-12-18 Gardner-Webb ## 5 0.633 0.367 W 400585975 2014-12-20 Norfolk State ## 6 0.471 0.529 L 400586931 2014-12-20 Denver ## 7 0.746 0.254 W 400587857 2014-12-20 Ohio State ## 8 0.383 0.617 L 400587858 2014-12-20 Wake Forest ## 9 0.536 0.464 W 400588379 2014-12-20 Providence ## 10 0.598 0.402 W 400588692 2014-12-20 Princeton ## # … with 10,992 more rows, and 7 more variables: ## # opponent_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt;, TeamResult &lt;fct&gt; metrics(testpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.622 ## 2 kap binary 0.244 testpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 3472 2022 ## L 2135 3373 roc_auc(testpredict, truth = TeamResult, .pred_W) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.668 roc_auc(testpredict, truth = TeamResult, .pred_L) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.332 roc_data &lt;- roc_curve(testpredict, truth = TeamResult, .pred_W) roc_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() 6.5 How well did it do with Nebraska? Let’s grab predictions for Nebraska from both our test and train data and take a look. nutrain &lt;- trainpredict %&gt;% filter(team_short_display_name == &quot;Nebraska&quot;, season == 2022) nutest &lt;- testpredict %&gt;% filter(team_short_display_name == &quot;Nebraska&quot;, season == 2022) bind_rows(nutrain, nutest) %&gt;% arrange(game_date) %&gt;% select(.pred_W, .pred_L, .pred_class, TeamResult, everything()) ## # A tibble: 17 × 13 ## .pred_W .pred_L .pred_class TeamResult game_id game_date team_short_displ… ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.431 0.569 L W 401369220 2021-12-23 Nebraska ## 2 0.339 0.661 L L 401364358 2022-01-03 Nebraska ## 3 0.432 0.568 L L 401364362 2022-01-06 Nebraska ## 4 0.429 0.571 L L 401364368 2022-01-08 Nebraska ## 5 0.197 0.803 L L 401364373 2022-01-12 Nebraska ## 6 0.200 0.800 L L 401364381 2022-01-14 Nebraska ## 7 0.362 0.638 L L 401364386 2022-01-17 Nebraska ## 8 0.282 0.718 L L 401364402 2022-01-27 Nebraska ## 9 0.454 0.546 L L 401364410 2022-01-29 Nebraska ## 10 0.289 0.711 L L 401364415 2022-02-02 Nebraska ## 11 0.470 0.530 L L 401364423 2022-02-05 Nebraska ## 12 0.356 0.644 L W 401364430 2022-02-10 Nebraska ## 13 0.309 0.691 L L 401364439 2022-02-13 Nebraska ## 14 0.441 0.559 L L 401364447 2022-02-19 Nebraska ## 15 0.468 0.532 L L 401364455 2022-02-23 Nebraska ## 16 0.312 0.688 L L 401364460 2022-02-26 Nebraska ## 17 0.560 0.440 W W 401364467 2022-02-28 Nebraska ## # … with 6 more variables: opponent_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; By our rolling metrics, we shouldn’t have beaten Minnesota, when we did. How could you improve this? "],["using-random-forests-to-predict-winners.html", "Chapter 7 Using random forests to predict winners 7.1 Setup 7.2 Predicting a future game", " Chapter 7 Using random forests to predict winners And now we return to decision trees and random forests. Recall that tree-based algorithms are based on decision trees, which are very easy to understand. A random forest is, as the name implies, a large number of decision trees, and they use a random choice of inputs at each fork in the tree. The algorithm creates a large number of randomly selected training inputs, and randomly chooses the feature input for each branch, creating predictions. The goal is to create uncorrelated forests of trees. The trees all make predictions, and the wisdom of the crowds takes over. Now, instead of a regression problem where we predict a number, we’re predicting a classification – win or loss. Along with that is the probability that it’s a win or a loss. Let’s get our libraries. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) And we’ll restore our feature engineering from logistic regression to compare. teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=&quot;right&quot;, fill=NA) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) team_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() opponent_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() %&gt;% rename( opponent_id = team_id, opponent_short_display_name = team_short_display_name, opponent_score = team_score, opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage, opponent_rolling_free_throw_factor = team_rolling_free_throw_factor, opponent_rolling_turnover_percentage = team_rolling_turnover_percentage, opponent_rolling_orb = team_rolling_orb, opponent_rolling_drb = team_rolling_drb ) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) games &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate( TeamResult = as.factor(case_when( team_score &gt; opponent_score ~ &quot;W&quot;, opponent_score &gt; team_score ~ &quot;L&quot; ))) %&gt;% na.omit() ## Joining, by = c(&quot;game_id&quot;, &quot;opponent_id&quot;) games$TeamResult &lt;- relevel(games$TeamResult, ref=&quot;W&quot;) modelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, team_rolling_true_shooting_percentage, opponent_rolling_true_shooting_percentage, team_rolling_turnover_percentage, opponent_rolling_turnover_percentage, TeamResult) With modelgames now with TeamResult and it being releveled, we’re ready to start the process. 7.1 Setup game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) For this walkthrough, we’re going to do both a logistic regression and a random forest side by side to show the value of workflows. The recipe we’ll create is the same for both, so we’ll use it twice. game_recipe &lt;- recipe(TeamResult ~ ., data = game_train) %&gt;% update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = &quot;ID&quot;) %&gt;% step_normalize(all_predictors()) summary(game_recipe) ## # A tibble: 10 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 game_id numeric ID original ## 2 game_date date ID original ## 3 team_short_display_name nominal ID original ## 4 opponent_short_display_name nominal ID original ## 5 season numeric ID original ## 6 team_rolling_true_shooting_percentage numeric predictor original ## 7 opponent_rolling_true_shooting_percentage numeric predictor original ## 8 team_rolling_turnover_percentage numeric predictor original ## 9 opponent_rolling_turnover_percentage numeric predictor original ## 10 TeamResult nominal outcome original Now, we’re going to create two different model specifications. The first will be the logistic regression model definintion and the second will be the random forest. log_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) Now we have enough for our workflows. We have two models and one recipe. log_workflow &lt;- workflow() %&gt;% add_model(log_mod) %&gt;% add_recipe(game_recipe) rf_workflow &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_recipe(game_recipe) Now we can fit our models to the data. log_fit &lt;- log_workflow %&gt;% fit(data = game_train) The random forest fit is going to take a beat or two. rf_fit &lt;- rf_workflow %&gt;% fit(data = game_train) Now we can bind our predictions to the training data and see how we did. logpredict &lt;- log_fit %&gt;% predict(new_data = game_train) %&gt;% bind_cols(game_train) logpredict &lt;- log_fit %&gt;% predict(new_data = game_train, type=&quot;prob&quot;) %&gt;% bind_cols(logpredict) rfpredict &lt;- rf_fit %&gt;% predict(new_data = game_train) %&gt;% bind_cols(game_train) rfpredict &lt;- rf_fit %&gt;% predict(new_data = game_train, type=&quot;prob&quot;) %&gt;% bind_cols(rfpredict) Now, how did we do? First, let’s look at the logistic regression. metrics(logpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.624 ## 2 kap binary 0.248 Same as last time, the logistic regression model comes in at 62 percent accuracy, and when we expose it to testing data, it remains pretty stable. This is a gigantic hint about what is to come. How about the random forest? metrics(rfpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.994 ## 2 kap binary 0.989 Holy buckets! We made a model that’s 99 percent accurate? GET ME TO VEGAS. Remember: Where a model makes its money is in data that it has never seen before. First, we look at logistic regression. logtestpredict &lt;- log_fit %&gt;% predict(new_data = game_test) %&gt;% bind_cols(game_test) logtestpredict &lt;- log_fit %&gt;% predict(new_data = game_test, type=&quot;prob&quot;) %&gt;% bind_cols(logtestpredict) metrics(logtestpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.622 ## 2 kap binary 0.244 Just about the same. That’s a robust model. Now, the inevitable crash with random forests. rftestpredict &lt;- rf_fit %&gt;% predict(new_data = game_test) %&gt;% bind_cols(game_test) rftestpredict &lt;- rf_fit %&gt;% predict(new_data = game_test, type=&quot;prob&quot;) %&gt;% bind_cols(rftestpredict) metrics(rftestpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.596 ## 2 kap binary 0.192 Right at 59 percent. A little bit lower than logistic regression. But did they come to the same answers to get those numbers? No. logtestpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 3472 2022 ## L 2135 3373 rftestpredict %&gt;% conf_mat(TeamResult, .pred_class) ## Truth ## Prediction W L ## W 3383 2221 ## L 2224 3174 Our two models, based on our feature engineering, are only slightly better than flipping a coin. But while that doesn’t sound great, some circumstances make it look uncanny. Let’s use our fits to see what’s happened in Nebraska’s season so far, to see differences in the models. Here’s the logistic regression. nu &lt;- modelgames %&gt;% filter(team_short_display_name == &quot;Nebraska&quot; &amp; season == 2022) %&gt;% arrange(game_date) nulogpredict &lt;- log_fit %&gt;% predict(new_data = nu) %&gt;% bind_cols(nu) nulogpredict &lt;- log_fit %&gt;% predict(new_data = nu, type=&quot;prob&quot;) %&gt;% bind_cols(nulogpredict) nulogpredict %&gt;% select(.pred_class, TeamResult, opponent_short_display_name, everything()) ## # A tibble: 17 × 13 ## .pred_class TeamResult opponent_short_dis… .pred_W .pred_L game_id game_date ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 L W Kennesaw State 0.431 0.569 4.01e8 2021-12-23 ## 2 L L Ohio State 0.339 0.661 4.01e8 2022-01-03 ## 3 L L Michigan State 0.432 0.568 4.01e8 2022-01-06 ## 4 L L Rutgers 0.429 0.571 4.01e8 2022-01-08 ## 5 L L Illinois 0.197 0.803 4.01e8 2022-01-12 ## 6 L L Purdue 0.200 0.800 4.01e8 2022-01-14 ## 7 L L Indiana 0.362 0.638 4.01e8 2022-01-17 ## 8 L L Wisconsin 0.282 0.718 4.01e8 2022-01-27 ## 9 L L Rutgers 0.454 0.546 4.01e8 2022-01-29 ## 10 L L Michigan 0.289 0.711 4.01e8 2022-02-02 ## 11 L L Northwestern 0.470 0.530 4.01e8 2022-02-05 ## 12 L W Minnesota 0.356 0.644 4.01e8 2022-02-10 ## 13 L L Iowa 0.309 0.691 4.01e8 2022-02-13 ## 14 L L Maryland 0.441 0.559 4.01e8 2022-02-19 ## 15 L L Northwestern 0.468 0.532 4.01e8 2022-02-23 ## 16 L L Iowa 0.312 0.688 4.01e8 2022-02-26 ## 17 W W Penn State 0.560 0.440 4.01e8 2022-02-28 ## # … with 6 more variables: team_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; It gets the Kennesaw State game wrong – the 11th game of the season – and thought we’d lose to Minnesota. Those are the two misses. Now, the random forest. nurfpredict &lt;- rf_fit %&gt;% predict(new_data = nu) %&gt;% bind_cols(nu) nurfpredict &lt;- rf_fit %&gt;% predict(new_data = nu, type=&quot;prob&quot;) %&gt;% bind_cols(nurfpredict) nurfpredict %&gt;% select(.pred_class, TeamResult, opponent_short_display_name, everything()) ## # A tibble: 17 × 13 ## .pred_class TeamResult opponent_short_dis… .pred_W .pred_L game_id game_date ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 W W Kennesaw State 0.734 0.266 4.01e8 2021-12-23 ## 2 W L Ohio State 0.538 0.462 4.01e8 2022-01-03 ## 3 L L Michigan State 0.191 0.809 4.01e8 2022-01-06 ## 4 L L Rutgers 0.239 0.761 4.01e8 2022-01-08 ## 5 L L Illinois 0.218 0.782 4.01e8 2022-01-12 ## 6 L L Purdue 0.304 0.696 4.01e8 2022-01-14 ## 7 L L Indiana 0.129 0.871 4.01e8 2022-01-17 ## 8 L L Wisconsin 0.166 0.834 4.01e8 2022-01-27 ## 9 L L Rutgers 0.143 0.857 4.01e8 2022-01-29 ## 10 L L Michigan 0.153 0.847 4.01e8 2022-02-02 ## 11 W L Northwestern 0.568 0.432 4.01e8 2022-02-05 ## 12 W W Minnesota 0.679 0.321 4.01e8 2022-02-10 ## 13 L L Iowa 0.114 0.886 4.01e8 2022-02-13 ## 14 L L Maryland 0.193 0.807 4.01e8 2022-02-19 ## 15 L L Northwestern 0.398 0.602 4.01e8 2022-02-23 ## 16 L L Iowa 0.242 0.758 4.01e8 2022-02-26 ## 17 W W Penn State 0.788 0.212 4.01e8 2022-02-28 ## # … with 6 more variables: team_short_display_name &lt;chr&gt;, season &lt;int&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; Flawless. But is it? No. More luck than perfection. More Nebraska being worse than the teams they were playing, so not a lot of close calls. 7.2 Predicting a future game This is being written on Monday, Feb. 21 and Nebraska plays Northwestern on Tuesday the 22nd. What do these models have to say about that game? We can get what we need by reworking what we do above and removing the lag. We use the lag to simulate knowing the stats before the game – not after. So by removing it, we return to our state of not knowing, since this game hasn’t been played yet. We can create a tibble – a dataframe – that matches our modelgames data that our two fits are expecting. It looks like this. win_prediction &lt;- tibble( game_id = 1, game_date = as.Date(&quot;2022-02-21&quot;), team_short_display_name=&quot;Nebraska&quot;, opponent_short_display_name=&quot;Northwestern&quot;, season = 2022, team_rolling_true_shooting_percentage = 53.73109, opponent_rolling_true_shooting_percentage = 51.20706, team_rolling_turnover_percentage = 0.1690204, opponent_rolling_turnover_percentage = 0.1345149 ) First we’ll try the logistic regression. nulogpredict &lt;- log_fit %&gt;% predict(new_data = win_prediction) %&gt;% bind_cols(win_prediction) nulogpredict &lt;- log_fit %&gt;% predict(new_data = nulogpredict, type=&quot;prob&quot;) %&gt;% bind_cols(nulogpredict) nulogpredict ## # A tibble: 1 × 12 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.468 0.532 L 1 2022-02-21 Nebraska ## # … with 6 more variables: opponent_short_display_name &lt;chr&gt;, season &lt;dbl&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; Logistic regression gives Nebraska a 47 percent chance of winning against Northwestern. Now, random forest. nurfpredict &lt;- rf_fit %&gt;% predict(new_data = win_prediction) %&gt;% bind_cols(win_prediction) nurfpredict &lt;- rf_fit %&gt;% predict(new_data = nurfpredict, type=&quot;prob&quot;) %&gt;% bind_cols(nurfpredict) nurfpredict ## # A tibble: 1 × 12 ## .pred_W .pred_L .pred_class game_id game_date team_short_display_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; ## 1 0.398 0.602 L 1 2022-02-21 Nebraska ## # … with 6 more variables: opponent_short_display_name &lt;chr&gt;, season &lt;dbl&gt;, ## # team_rolling_true_shooting_percentage &lt;dbl&gt;, ## # opponent_rolling_true_shooting_percentage &lt;dbl&gt;, ## # team_rolling_turnover_percentage &lt;dbl&gt;, ## # opponent_rolling_turnover_percentage &lt;dbl&gt; Random forest is even less bullish on Nebraksa’s chances. It gives the Huskers an 38 percent chance. Spoiler Alert: They lost. "],["xgboost-to-make-classifications.html", "Chapter 8 XGBoost to make classifications", " Chapter 8 XGBoost to make classifications And now we return to XGBoost, but now for classifications. Recall that boosting methods are another wrinkle in the tree based methods. Instead of deep trees, boosting methods intentionally pick shallow trees – called stumps – that, at least initially, do a poor job of predicting the outcome. Then, each subsequent stump takes the job the previous one did, optimizes to reduce the residuals – the gap between prediction and reality – and makes a prediction. And then the next one does the same, and so on and so on. So far, our logistic regression and random forest methods are just better than coin flipping (which would be 50 percent accurate). Does XGBoost, with it’s method of optimizing for reduced error, fare any better? Let’s try, and we’ll add in a little wrinkle – assigning a dummy value to home or away. The home team gets 1, the away team gets 0. The truth is XGboost is going to be similarly predictive. Let’s see what we can do with an additional value. We’ll restore our libraries and feature engineering, now with a dummy value for home_away. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) require(doParallel) cores &lt;- parallel::detectCores(logical = FALSE) teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_dummy_home_away = case_when( home_away == &quot;HOME&quot; ~ 1, home_away == &quot;AWAY&quot; ~ 0 ) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) team_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_dummy_home_away) %&gt;% na.omit() opponent_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() %&gt;% rename( opponent_id = team_id, opponent_short_display_name = team_short_display_name, opponent_score = team_score, opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage, opponent_rolling_free_throw_factor = team_rolling_free_throw_factor, opponent_rolling_turnover_percentage = team_rolling_turnover_percentage, opponent_rolling_orb = team_rolling_orb, opponent_rolling_drb = team_rolling_drb ) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) games &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate( TeamResult = as.factor(case_when( team_score &gt; opponent_score ~ &quot;W&quot;, opponent_score &gt; team_score ~ &quot;L&quot; ))) %&gt;% na.omit() ## Joining, by = c(&quot;game_id&quot;, &quot;opponent_id&quot;) games$TeamResult &lt;- relevel(games$TeamResult, ref=&quot;W&quot;) modelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, team_rolling_true_shooting_percentage, opponent_rolling_true_shooting_percentage, team_rolling_turnover_percentage, opponent_rolling_turnover_percentage, team_dummy_home_away, TeamResult) Per usual, we split our data into training and testing. game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) For this walkthrough, we’ll just do XGboost because of the time needed to complete it. We can refer to the previous chapter for comparison. Our recipe is the same as the previous chapter. game_recipe &lt;- recipe(TeamResult ~ ., data = game_train) %&gt;% update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = &quot;ID&quot;) %&gt;% step_normalize(all_predictors()) summary(game_recipe) ## # A tibble: 11 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 game_id numeric ID original ## 2 game_date date ID original ## 3 team_short_display_name nominal ID original ## 4 opponent_short_display_name nominal ID original ## 5 season numeric ID original ## 6 team_rolling_true_shooting_percentage numeric predictor original ## 7 opponent_rolling_true_shooting_percentage numeric predictor original ## 8 team_rolling_turnover_percentage numeric predictor original ## 9 opponent_rolling_turnover_percentage numeric predictor original ## 10 team_dummy_home_away numeric predictor original ## 11 TeamResult nominal outcome original Similar to last time, we’re going to use a boosted tree function, tune the parameters but this time we’ll set the mode to classification. xg_mod &lt;- boost_tree( trees = tune(), learn_rate = tune(), tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;xgboost&quot;, nthread = cores) We’ll make our workflow. xg_wflow &lt;- workflow() %&gt;% add_model(xg_mod) %&gt;% add_recipe(game_recipe) Establish our grid to start the process of cross validating our model tuning. xgb_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), game_train), learn_rate(), size = 30 ) xgb_grid ## # A tibble: 30 × 7 ## trees tree_depth min_n loss_reduction sample_size mtry learn_rate ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 842 13 36 1.83e- 9 0.687 4 1.14e- 8 ## 2 25 10 34 1.08e-10 0.837 8 5.58e-10 ## 3 1870 9 8 8.15e- 6 0.149 6 2.37e- 3 ## 4 171 14 13 7.49e- 5 0.259 3 2.51e- 8 ## 5 1974 1 30 3.36e- 4 0.609 11 2.12e- 5 ## 6 1242 11 36 5.86e- 1 0.787 7 3.78e- 8 ## 7 435 3 17 3.10e- 3 0.936 2 4.65e- 7 ## 8 786 9 28 1.71e- 5 0.701 5 9.97e- 8 ## 9 316 4 22 7.11e- 9 0.454 11 1.21e- 4 ## 10 1802 10 15 1.90e- 7 0.521 5 7.96e- 2 ## # … with 20 more rows Speaking of cross-validation … game_folds &lt;- vfold_cv(game_train) game_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [39605/4401]&gt; Fold01 ## 2 &lt;split [39605/4401]&gt; Fold02 ## 3 &lt;split [39605/4401]&gt; Fold03 ## 4 &lt;split [39605/4401]&gt; Fold04 ## 5 &lt;split [39605/4401]&gt; Fold05 ## 6 &lt;split [39605/4401]&gt; Fold06 ## 7 &lt;split [39606/4400]&gt; Fold07 ## 8 &lt;split [39606/4400]&gt; Fold08 ## 9 &lt;split [39606/4400]&gt; Fold09 ## 10 &lt;split [39606/4400]&gt; Fold10 And now the part that makes laptop fans go wheeee – tuning the model. doParallel::registerDoParallel(cores = cores) xgb_res &lt;- tune_grid( xg_wflow, resamples = game_folds, grid = xgb_grid, control = control_grid(save_pred = TRUE) ) doParallel::stopImplicitCluster() xgb_res ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [39605/4401]&gt; Fold01 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 2 &lt;split [39605/4401]&gt; Fold02 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 3 &lt;split [39605/4401]&gt; Fold03 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 4 &lt;split [39605/4401]&gt; Fold04 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 5 &lt;split [39605/4401]&gt; Fold05 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 6 &lt;split [39605/4401]&gt; Fold06 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 7 &lt;split [39606/4400]&gt; Fold07 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 8 &lt;split [39606/4400]&gt; Fold08 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 9 &lt;split [39606/4400]&gt; Fold09 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… ## 10 &lt;split [39606/4400]&gt; Fold10 &lt;tibble [60 × 11]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [132… We’ll cut to the chase and go straight to the best by accuracy. best_acc &lt;- select_best(xgb_res, &quot;accuracy&quot;) We’ll create our final workflow and fit the model. final_xgb &lt;- finalize_workflow( xg_wflow, best_acc ) xg_fit &lt;- final_xgb %&gt;% fit(data = game_train) ## [23:04:54] WARNING: amalgamation/../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. Now to see how our model fared against training data. trainresults &lt;- game_train %&gt;% bind_cols(predict(xg_fit, game_train)) metrics(trainresults, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.665 ## 2 kap binary 0.330 We’re up pretty significantly against the random forest without the home and away dummy. But is it stable? testresults &lt;- game_test %&gt;% bind_cols(predict(xg_fit, game_test)) metrics(testresults, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.648 ## 2 kap binary 0.296 Say this for XGBoost: It’s remarkably stable. "],["support-vector-machines.html", "Chapter 9 Support Vector Machines 9.1 Logistic regression 9.2 Random forest 9.3 Support Vector Machine", " Chapter 9 Support Vector Machines The last method of statistical learning that we’ll use is the Support Vector Machine. The concept to understand about the support vector machine is the concept of a hyperplane. First think of a hyperplane as a line – a means to separate wins and losses along an axis where a certain stat says this way is a win and that way is a loss. When you have just two stats like that, it’s pretty easy. Where support vector machines get hard is when you have many predictors that create the hyperplane. Then, instead of a line, it becomes a multidimensional shape in a multidimensional space. We’re goingt to implement a support vector machine along side a logistic regression and a random forest. We’ll use our models and feature engineering from XGBoost to compare. library(tidyverse) library(tidymodels) library(zoo) library(hoopR) set.seed(1234) teamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;% separate(field_goals_made_field_goals_attempted, into = c(&quot;field_goals_made&quot;,&quot;field_goals_attempted&quot;)) %&gt;% separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(&quot;three_point_field_goals_made&quot;,&quot;three_point_field_goals_attempted&quot;)) %&gt;% separate(free_throws_made_free_throws_attempted, into = c(&quot;free_throws_made&quot;,&quot;free_throws_attempted&quot;)) %&gt;% mutate_at(12:34, as.numeric) teamstats &lt;- teamgames %&gt;% group_by(team_short_display_name, season) %&gt;% arrange(game_date) %&gt;% mutate( team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made, true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100, turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers), free_throw_factor = free_throws_made/field_goals_attempted, team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=&quot;right&quot;, fill=NA), ) %&gt;% ungroup() opponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) newteamstats &lt;- teamstats %&gt;% inner_join(opponent) %&gt;% mutate( orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds), drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds), team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=&quot;right&quot;, fill=NA), team_dummy_home_away = case_when( home_away == &quot;HOME&quot; ~ 1, home_away == &quot;AWAY&quot; ~ 0 ) ) ## Joining, by = c(&quot;opponent_id&quot;, &quot;game_id&quot;) team_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_dummy_home_away) %&gt;% na.omit() opponent_side &lt;- newteamstats %&gt;% select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb) %&gt;% na.omit() %&gt;% rename( opponent_id = team_id, opponent_short_display_name = team_short_display_name, opponent_score = team_score, opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage, opponent_rolling_free_throw_factor = team_rolling_free_throw_factor, opponent_rolling_turnover_percentage = team_rolling_turnover_percentage, opponent_rolling_orb = team_rolling_orb, opponent_rolling_drb = team_rolling_drb ) %&gt;% mutate(opponent_id = as.numeric(opponent_id)) games &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate( TeamResult = as.factor(case_when( team_score &gt; opponent_score ~ &quot;W&quot;, opponent_score &gt; team_score ~ &quot;L&quot; ))) %&gt;% na.omit() ## Joining, by = c(&quot;game_id&quot;, &quot;opponent_id&quot;) games$TeamResult &lt;- relevel(games$TeamResult, ref=&quot;W&quot;) modelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, team_rolling_true_shooting_percentage, opponent_rolling_true_shooting_percentage, team_rolling_turnover_percentage, opponent_rolling_turnover_percentage, team_dummy_home_away, TeamResult) Now to split and define our recipe. game_split &lt;- initial_split(modelgames, prop = .8) game_train &lt;- training(game_split) game_test &lt;- testing(game_split) game_recipe &lt;- recipe(TeamResult ~ ., data = game_train) %&gt;% update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = &quot;ID&quot;) %&gt;% step_normalize(all_predictors()) summary(game_recipe) ## # A tibble: 11 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 game_id numeric ID original ## 2 game_date date ID original ## 3 team_short_display_name nominal ID original ## 4 opponent_short_display_name nominal ID original ## 5 season numeric ID original ## 6 team_rolling_true_shooting_percentage numeric predictor original ## 7 opponent_rolling_true_shooting_percentage numeric predictor original ## 8 team_rolling_turnover_percentage numeric predictor original ## 9 opponent_rolling_turnover_percentage numeric predictor original ## 10 team_dummy_home_away numeric predictor original ## 11 TeamResult nominal outcome original Now, to define our models. log_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) svm_mod &lt;- svm_poly() %&gt;% set_engine(&quot;kernlab&quot;) %&gt;% set_mode(&quot;classification&quot;) And our workflows. log_workflow &lt;- workflow() %&gt;% add_model(log_mod) %&gt;% add_recipe(game_recipe) rf_wflow &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_recipe(game_recipe) svm_wflow &lt;- workflow() %&gt;% add_model(svm_mod) %&gt;% add_recipe(game_recipe) We’ll take them one by one. 9.1 Logistic regression log_fit &lt;- log_workflow %&gt;% fit(data = game_train) logpredict &lt;- log_fit %&gt;% predict(new_data = game_train) %&gt;% bind_cols(game_train) logpredict &lt;- log_fit %&gt;% predict(new_data = game_train, type=&quot;prob&quot;) %&gt;% bind_cols(logpredict) metrics(logpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.655 ## 2 kap binary 0.309 logtest &lt;- game_test %&gt;% bind_cols(predict(log_fit, game_test)) metrics(logtest, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.648 ## 2 kap binary 0.297 9.2 Random forest rf_fit &lt;- rf_wflow %&gt;% fit(data = game_train) rfpredict &lt;- rf_fit %&gt;% predict(new_data = game_train) %&gt;% bind_cols(game_train) rfpredict &lt;- rf_fit %&gt;% predict(new_data = game_train, type=&quot;prob&quot;) %&gt;% bind_cols(rfpredict) metrics(rfpredict, TeamResult, .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.988 ## 2 kap binary 0.976 But we all know we can’t take a random forest training metrics seriously, so here’s test. rftestresults &lt;- game_test %&gt;% bind_cols(predict(rf_fit, game_test)) metrics(rftestresults, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.630 ## 2 kap binary 0.260 9.3 Support Vector Machine The inputs of the support vector machine are very similar to others. There are parameters you can tune, but similar to XGBoost, this can take time and in my experience doesn’t result in major improvements. The defaults work well enough for us. We can just jump right into the fit. svm_fit &lt;- svm_wflow %&gt;% fit(data = game_train) ## Setting default kernel parameters Training. svmtrainresults &lt;- game_train %&gt;% bind_cols(predict(svm_fit, game_train)) metrics(svmtrainresults, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.653 ## 2 kap binary 0.307 And now testing. svmtestresults &lt;- game_test %&gt;% bind_cols(predict(svm_fit, game_test)) metrics(svmtestresults, truth = TeamResult, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.649 ## 2 kap binary 0.298 By the barest fractions it’s done worse than logistic regression and slightly better than random forest. And, as a bonus, it’s very robust to new data. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
