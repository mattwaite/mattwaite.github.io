[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Matt Waite is a professor of practice in the College of Journalism and Mass Communications at the University of Nebraska-Lincoln.\nSince he joined the faculty in 2011, he founded the Drone Journalism Lab, the first of its kind at a journalism school. His students have used drones to report news in six countries on three continents. He regularly speaks about the legal and ethical complexities of using drones at conferences around the world and is frequently consulted by media organizations about their potential. He also teaches courses in data journalism, web development and the intersection of storytelling and technology. He created an open learning lab for students called Maker Hours, helped develop an interdisciplinary minor in Informatics and serves on the Publications Board of the Daily Nebraskan. In 2016, he received the University of Nebraska’s Innovation, Development and Entrepreneurship Award and was inducted into the Nebraska Press Association’s Hall of Fame in October.\nFrom 2007-2011, he was a hybrid programmer/journalist for the St. Petersburg Times where he developed PolitiFact, a website that fact checks what politicians say. The site became the first website awarded the Pulitzer Prize in 2009.\nBefore becoming a web developer, he was an award-winning investigative reporter. He began his journalism career at the Arkansas Democrat-Gazette in Little Rock, covering police and breaking news, including the crash of American Airlines flight 1420 in 1999. In 2000, he moved to the St. Petersburg Times. From 2005-2007, he co-authored a series of award-winning stories about Florida’s vanishing wetlands. That work was later expanded into a book, “Paving Paradise: Florida’s Vanishing Wetlands and the Failure of No Net Loss,” published in 2009 by the University Press of Florida."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Nebraska-Lincoln | Lincoln, NE Masters of Science in Business Analytics | Dec 2020\nUniversity of Nebraska-Lincoln | Lincoln, NE B.J in Journalism | Dec 1997"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nProfessor of practice | University of Nebraska-Lincoln | August 2011 - Present\nReporter, Technologist | Tampa Bay Times, St. Petersburg, FL | March 2000 - Feb. 2011\nReporter | Arkansas Democrat Gazette, Little Rock, AR | January 1998 - Feb. 2000"
  },
  {
    "objectID": "posts/another-year-another-attempt-another-bracket-disaster/index.html",
    "href": "posts/another-year-another-attempt-another-bracket-disaster/index.html",
    "title": "Another year, another attempt, another bracket disaster",
    "section": "",
    "text": "Once again, I attempted to predict the outcome of the NCAA tournament using machine learning, and I had a class-load of students try the same.\nIf you like the madness part of March Madness, this year is for you.\nIt is not for machine learning algorithms based on regular season performance. At least not mine.\nOf the 14 brackets I and my students produced, using 14 different methods, we came up with 7 unique national title winners.\nZero of them are right.\nThe best brackets had two of the four Final Four teams, but none picked a team still playing now to win it all with the Final Four set.\nI had hope for my bracket picking algorithm this year. It had what felt like a good mix of upsets and favorites, some madness and some method.\nWhat follows is a post mortem and an attempt to figure out what went wrong. If anything went wrong. How do you predict St. Peter’s? You don’t. It’s what makes this fun.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(zoo)\nlibrary(hoopR)\nlibrary(gt)\n\nset.seed(1234)\n\nkenpom &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/ratings.csv\")\nnamekey &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/nametable.csv\")\nsimplestats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/simplestats.csv\")\n\nteamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;%\n  filter(game_date &lt; as.Date(\"2022-03-17\")) %&gt;%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %&gt;%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %&gt;%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %&gt;%\n  mutate_at(12:34, as.numeric) %&gt;% \n  mutate(team_id = as.numeric(team_id))\n\nteamgames &lt;- teamgames %&gt;% left_join(namekey) %&gt;% left_join(kenpom, by=c(\"team\" = \"team\", \"season\"=\"year\")) %&gt;% left_join(simplestats, by=c(\"School\" = \"School\", \"season\" = \"Season\"))\n\nteamstats &lt;- teamgames %&gt;% \n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    team_offensive_efficiency = (team_score/possessions)*100,\n    true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100,\n    turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    free_throw_factor = free_throws_made/field_goals_attempted,\n    team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=\"right\", fill=NA), \n    team_cumulative_mean_true_shooting = lag(cummean(true_shooting_percentage), n=1, default=0),\n    team_cumulative_mean_turnover_percentage = lag(cummean(turnover_pct), n=1, default=0),\n    team_cumulative_mean_free_throw_factor = lag(cummean(free_throw_factor), n=1, default=0),\n    team_cumulative_o_eff = lag(cummean(team_offensive_efficiency), n=1, default=0),\n    team_rolling_o_eff = rollmean(lag(team_offensive_efficiency, n=1), k=10, align=\"right\", fill=NA)\n  ) %&gt;% ungroup() %&gt;%\n  rename(\n    team_sos = OverallSOS,\n    team_srs = OverallSRS,\n    team_luck = luck\n  )\n\nteamstats &lt;- teamstats %&gt;% \n  select(game_id, team_id, team_offensive_efficiency) %&gt;%\n  mutate(team_id = as.numeric(team_id)) %&gt;% \n  rename(opponent_id = team_id, opponent_offensive_efficiency=team_offensive_efficiency) %&gt;% \n  left_join(teamstats) %&gt;%\n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_cumulative_d_eff = lag(cummean(opponent_offensive_efficiency), n=1, default=0),\n    team_rolling_d_eff = rollmean(lag(opponent_offensive_efficiency, n=1), k=10, align=\"right\", fill=NA)\n    ) %&gt;% ungroup()\n\nopponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id))\n\nnewteamstats &lt;- teamstats %&gt;% \n  inner_join(opponent) %&gt;% \n  mutate(\n    orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds),\n    drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds),\n    team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=\"right\", fill=NA),\n    team_cumulative_mean_orb = lag(cummean(orb), n=1, default=0),\n    team_cumulative_mean_drb = lag(cummean(drb), n=1, default=0),\n    team_efficiency_margin = team_cumulative_o_eff - team_cumulative_d_eff,\n    team_recent_efficiency_margin = team_rolling_o_eff - team_rolling_d_eff,\n    team_recency = team_recent_efficiency_margin - team_efficiency_margin\n    ) \n\nteam_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit()\n\nopponent_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit() %&gt;%\n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage,\n    opponent_rolling_free_throw_factor = team_rolling_free_throw_factor,\n    opponent_rolling_turnover_percentage = team_rolling_turnover_percentage,\n    opponent_rolling_orb = team_rolling_orb,\n    opponent_rolling_drb = team_rolling_drb,\n    opponent_cumulative_mean_true_shooting = team_cumulative_mean_true_shooting,\n    opponent_cumulative_mean_turnover_percentage = team_cumulative_mean_turnover_percentage,\n    opponent_cumulative_mean_free_throw_factor = team_cumulative_mean_free_throw_factor,\n    opponent_cumulative_mean_orb = team_cumulative_mean_orb,\n    opponent_cumulative_mean_drb = team_cumulative_mean_drb,\n    opponent_cumulative_o_eff = team_cumulative_o_eff,\n    opponent_cumulative_d_eff = team_cumulative_d_eff,\n    opponent_efficiency_margin = team_efficiency_margin,\n    opponent_srs = team_srs,\n    opponent_sos = team_sos,\n    opponent_luck = team_luck,\n    opponent_recency = team_recency\n  ) %&gt;%\n  mutate(\n    opponent_id = as.numeric(opponent_id)\n    )\n\ngames &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate(\n  TeamResult = as.factor(case_when(\n    team_score &gt; opponent_score ~ \"W\",\n    opponent_score &gt; team_score ~ \"L\"\n))) %&gt;% na.omit()\n\ngames$TeamResult &lt;- relevel(games$TeamResult, ref=\"W\")\n\ncumulativesimplemodelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, opponent_efficiency_margin, team_efficiency_margin, team_sos, team_srs, opponent_sos, opponent_srs, opponent_luck, team_luck, opponent_recency, team_recency, TeamResult) %&gt;% na.omit()\n\ncumulative_split &lt;- initial_split(cumulativesimplemodelgames, prop = .8)\ncumulative_train &lt;- training(cumulative_split)\ncumulative_test &lt;- testing(cumulative_split)\n\ncumulative_recipe &lt;- \n  recipe(TeamResult ~ ., data = cumulative_train) %&gt;% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %&gt;%\n  step_normalize(all_predictors())\n\nlog_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nlog_workflow &lt;- \n  workflow() %&gt;% \n  add_model(log_mod) %&gt;% \n  add_recipe(cumulative_recipe)\n\nlog_fit &lt;- \n  log_workflow %&gt;% \n  fit(data = cumulative_train)\n\nteamstats &lt;- teamgames %&gt;% \n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    team_offensive_efficiency = (team_score/possessions)*100,\n    true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100,\n    turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    free_throw_factor = free_throws_made/field_goals_attempted,\n    team_rolling_true_shooting_percentage = rollmean(true_shooting_percentage, k=5, align=\"right\", fill=NA),\n    team_rolling_turnover_percentage = rollmean(turnover_pct, k=5, align=\"right\", fill=NA),\n    team_rolling_free_throw_factor = rollmean(free_throw_factor, k=4, align=\"right\", fill=NA), \n    team_cumulative_mean_true_shooting = cummean(true_shooting_percentage),\n    team_cumulative_mean_turnover_percentage = cummean(turnover_pct),\n    team_cumulative_mean_free_throw_factor = cummean(free_throw_factor),\n    team_cumulative_o_eff = cummean(team_offensive_efficiency),\n    team_rolling_o_eff = rollmean(team_offensive_efficiency, k=10, align=\"right\", fill=NA)\n  ) %&gt;% ungroup() \n\nteamstats &lt;- teamstats %&gt;% \n  select(game_id, team_id, team_offensive_efficiency) %&gt;%\n  mutate(team_id = as.numeric(team_id)) %&gt;% \n  rename(opponent_id = team_id, opponent_offensive_efficiency=team_offensive_efficiency) %&gt;% \n  left_join(teamstats) %&gt;%\n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_cumulative_d_eff = cummean(opponent_offensive_efficiency),\n    team_rolling_d_eff = rollmean(opponent_offensive_efficiency, k=10, align=\"right\", fill=NA)\n    ) %&gt;% ungroup() %&gt;%\n  rename(\n    team_sos = OverallSOS,\n    team_srs = OverallSRS, \n    team_luck = luck\n  )\n\nopponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id))\n\nnewteamstats &lt;- teamstats %&gt;% \n  inner_join(opponent) %&gt;% \n  mutate(\n    orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds),\n    drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds),\n    team_rolling_orb = rollmean(orb, k=5, align=\"right\", fill=NA),\n    team_rolling_drb = rollmean(drb, k=5, align=\"right\", fill=NA),\n    team_cumulative_mean_orb = cummean(orb),\n    team_cumulative_mean_drb = cummean(drb),\n    team_efficiency_margin = team_cumulative_o_eff - team_cumulative_d_eff,\n    team_recent_efficiency_margin = team_rolling_o_eff - team_rolling_d_eff,\n    team_recency = team_recent_efficiency_margin - team_efficiency_margin\n    )\n\nteam_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_rolling_o_eff, team_rolling_d_eff, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit()\n\nopponent_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_rolling_o_eff, team_rolling_d_eff, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit() %&gt;%\n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage,\n    opponent_rolling_free_throw_factor = team_rolling_free_throw_factor,\n    opponent_rolling_turnover_percentage = team_rolling_turnover_percentage,\n    opponent_rolling_orb = team_rolling_orb,\n    opponent_rolling_drb = team_rolling_drb,\n    opponent_cumulative_mean_true_shooting = team_cumulative_mean_true_shooting,\n    opponent_cumulative_mean_turnover_percentage = team_cumulative_mean_turnover_percentage,\n    opponent_cumulative_mean_free_throw_factor = team_cumulative_mean_free_throw_factor,\n    opponent_cumulative_mean_orb = team_cumulative_mean_orb,\n    opponent_cumulative_mean_drb = team_cumulative_mean_drb,\n    opponent_cumulative_o_eff = team_cumulative_o_eff,\n    opponent_cumulative_d_eff = team_cumulative_d_eff,\n    opponent_efficiency_margin = team_efficiency_margin,\n    opponent_rolling_o_eff = team_rolling_o_eff, \n    opponent_rolling_d_eff = team_rolling_d_eff,\n    opponent_srs = team_srs,\n    opponent_sos = team_sos,\n    opponent_luck = team_luck,\n    opponent_recency = team_recency\n  ) %&gt;%\n  mutate(\n    opponent_id = as.numeric(opponent_id)\n    )\n\ngames &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate(\n  TeamResult = as.factor(case_when(\n    team_score &gt; opponent_score ~ \"W\",\n    opponent_score &gt; team_score ~ \"L\"\n))) %&gt;% na.omit()\n\ngames$TeamResult &lt;- relevel(games$TeamResult, ref=\"W\")\n\ncumulativesimplemodelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, opponent_efficiency_margin, team_efficiency_margin, team_sos, team_srs, opponent_sos, opponent_srs, team_luck, opponent_luck, team_recency, opponent_recency, TeamResult) \n\nwestround1games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Georgia State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Boise State\",\n  opponent_short_display_name=\"Memphis\"\n) %&gt;% add_row(\n  team_short_display_name=\"UConn\",\n  opponent_short_display_name=\"New Mexico St\"\n) %&gt;% add_row(\n  team_short_display_name=\"Arkansas\",\n  opponent_short_display_name=\"Vermont\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Notre Dame\"\n) %&gt;% add_row(\n  team_short_display_name=\"Texas Tech\",\n  opponent_short_display_name=\"Montana State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan State\",\n  opponent_short_display_name=\"Davidson\"\n) %&gt;% add_row(\n  team_short_display_name=\"Duke\",\n  opponent_short_display_name=\"CSU Fullerton\"\n)\n\nwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround1games)\n\nwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround1games) \n\nwestround1log &lt;- log_fit %&gt;% predict(new_data = westround1games) %&gt;%\n  bind_cols(westround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround1log &lt;- log_fit %&gt;% predict(new_data = westround1log, type=\"prob\") %&gt;%\n  bind_cols(westround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround2games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Memphis\"\n) %&gt;% add_row(\n  team_short_display_name=\"UConn\",\n  opponent_short_display_name=\"Arkansas\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Texas Tech\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan State\",\n  opponent_short_display_name=\"Duke\"\n)\n\nwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround2games)\n\nwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround2games) \n\nwestround2log &lt;- log_fit %&gt;% predict(new_data = westround2games) %&gt;%\n  bind_cols(westround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround2log &lt;- log_fit %&gt;% predict(new_data = westround2log, type=\"prob\") %&gt;%\n  bind_cols(westround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround3games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Arkansas\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Duke\"\n) \n\nwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround3games)\n\nwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround3games) \n\nwestround3log &lt;- log_fit %&gt;% predict(new_data = westround3games) %&gt;%\n  bind_cols(westround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround3log &lt;- log_fit %&gt;% predict(new_data = westround3log, type=\"prob\") %&gt;%\n  bind_cols(westround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround4games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Alabama\"\n) \n\nwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround4games)\n\nwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround4games) \n\nwestround4log &lt;- log_fit %&gt;% predict(new_data = westround4games) %&gt;%\n  bind_cols(westround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround4log &lt;- log_fit %&gt;% predict(new_data = westround4log, type=\"prob\") %&gt;%\n  bind_cols(westround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround1games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Norfolk State\"\n) %&gt;% add_row(\n  team_short_display_name=\"North Carolina\",\n  opponent_short_display_name=\"Marquette\"\n) %&gt;% add_row(\n  team_short_display_name=\"Saint Mary's\",\n  opponent_short_display_name=\"Indiana\"\n) %&gt;% add_row(\n  team_short_display_name=\"UCLA\",\n  opponent_short_display_name=\"Akron\"\n) %&gt;% add_row(\n  team_short_display_name=\"Texas\",\n  opponent_short_display_name=\"Virginia Tech\"\n) %&gt;% add_row(\n  team_short_display_name=\"Purdue\",\n  opponent_short_display_name=\"Yale\"\n) %&gt;% add_row(\n  team_short_display_name=\"Murray State\",\n  opponent_short_display_name=\"San Francisco\"\n) %&gt;% add_row(\n  team_short_display_name=\"Kentucky\",\n  opponent_short_display_name=\"Saint Peter's\"\n)\n\neastround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround1games)\n\neastround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround1games) \n\neastround1log &lt;- log_fit %&gt;% predict(new_data = eastround1games) %&gt;%\n  bind_cols(eastround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround1log &lt;- log_fit %&gt;% predict(new_data = eastround1log, type=\"prob\") %&gt;%\n  bind_cols(eastround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround2games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"North Carolina\"\n) %&gt;% add_row(\n  team_short_display_name=\"Indiana\",\n  opponent_short_display_name=\"UCLA\"\n)  %&gt;% add_row(\n  team_short_display_name=\"Texas\",\n  opponent_short_display_name=\"Purdue\"\n) %&gt;% add_row(\n  team_short_display_name=\"San Francisco\",\n  opponent_short_display_name=\"Kentucky\"\n) \n\neastround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround2games)\n\neastround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround2games) \n\neastround2log &lt;- log_fit %&gt;% predict(new_data = eastround2games) %&gt;%\n  bind_cols(eastround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround2log &lt;- log_fit %&gt;% predict(new_data = eastround2log, type=\"prob\") %&gt;%\n  bind_cols(eastround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround3games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"UCLA\"\n) %&gt;% add_row(\n  team_short_display_name=\"Purdue\",\n  opponent_short_display_name=\"Kentucky\"\n) \n\neastround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround3games)\n\neastround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround3games) \n\neastround3log &lt;- log_fit %&gt;% predict(new_data = eastround3games) %&gt;%\n  bind_cols(eastround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround3log &lt;- log_fit %&gt;% predict(new_data = eastround3log, type=\"prob\") %&gt;%\n  bind_cols(eastround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround4games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Purdue\"\n) \n\neastround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround4games)\n\neastround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround4games) \n\neastround4log &lt;- log_fit %&gt;% predict(new_data = eastround4games) %&gt;%\n  bind_cols(eastround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround4log &lt;- log_fit %&gt;% predict(new_data = eastround4log, type=\"prob\") %&gt;%\n  bind_cols(eastround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround1games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Wright State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Seton Hall\",\n  opponent_short_display_name=\"TCU\"\n) %&gt;% add_row(\n  team_short_display_name=\"Houston\",\n  opponent_short_display_name=\"UAB\"\n) %&gt;% add_row(\n  team_short_display_name=\"Illinois\",\n  opponent_short_display_name=\"Chattanooga\"\n) %&gt;% add_row(\n  team_short_display_name=\"Colorado State\",\n  opponent_short_display_name=\"Michigan\"\n) %&gt;% add_row(\n  team_short_display_name=\"Tennessee\",\n  opponent_short_display_name=\"Longwood\"\n) %&gt;% add_row(\n  team_short_display_name=\"Ohio State\",\n  opponent_short_display_name=\"Loyola Chicago\"\n) %&gt;% add_row(\n  team_short_display_name=\"Villanova\",\n  opponent_short_display_name=\"Delaware\"\n)\n\nsouthround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround1games)\n\nsouthround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround1games) \n\nsouthround1log &lt;- log_fit %&gt;% predict(new_data = southround1games) %&gt;%\n  bind_cols(southround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround1log &lt;- log_fit %&gt;% predict(new_data = southround1log, type=\"prob\") %&gt;%\n  bind_cols(southround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround2games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Seton Hall\"\n) %&gt;% add_row(\n  team_short_display_name=\"Houston\",\n  opponent_short_display_name=\"Illinois\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Tennessee\"\n) %&gt;% add_row(\n  team_short_display_name=\"Ohio State\",\n  opponent_short_display_name=\"Villanova\"\n)\n\nsouthround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround2games)\n\nsouthround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround2games) \n\nsouthround2log &lt;- log_fit %&gt;% predict(new_data = southround2games) %&gt;%\n  bind_cols(southround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround2log &lt;- log_fit %&gt;% predict(new_data = southround2log, type=\"prob\") %&gt;%\n  bind_cols(southround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround3games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Illinois\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Villanova\"\n)\n\nsouthround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround3games)\n\nsouthround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround3games) \n\nsouthround3log &lt;- log_fit %&gt;% predict(new_data = southround3games) %&gt;%\n  bind_cols(southround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround3log &lt;- log_fit %&gt;% predict(new_data = southround3log, type=\"prob\") %&gt;%\n  bind_cols(southround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround4games &lt;- tibble(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Illinois\"\n) \n\nsouthround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround4games)\n\nsouthround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround4games) \n\nsouthround4log &lt;- log_fit %&gt;% predict(new_data = southround4games) %&gt;%\n  bind_cols(southround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround4log &lt;- log_fit %&gt;% predict(new_data = southround4log, type=\"prob\") %&gt;%\n  bind_cols(southround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround1games &lt;- tibble(\n  team_short_display_name=\"Kansas\",\n  opponent_short_display_name=\"Texas Southern\"\n) %&gt;% add_row(\n  team_short_display_name=\"San Diego State\",\n  opponent_short_display_name=\"Creighton\"\n) %&gt;% add_row(\n  team_short_display_name=\"Iowa\",\n  opponent_short_display_name=\"Richmond\"\n) %&gt;% add_row(\n  team_short_display_name=\"Providence\",\n  opponent_short_display_name=\"S Dakota St\"\n) %&gt;% add_row(\n  team_short_display_name=\"LSU\",\n  opponent_short_display_name=\"Iowa State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Wisconsin\",\n  opponent_short_display_name=\"Colgate\"\n) %&gt;% add_row(\n  team_short_display_name=\"USC\",\n  opponent_short_display_name=\"Miami\"\n) %&gt;% add_row(\n  team_short_display_name=\"Auburn\",\n  opponent_short_display_name=\"J'Ville St\"\n)\n\nmidwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround1games)\n\nmidwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround1games) \n\nmidwestround1log &lt;- log_fit %&gt;% predict(new_data = midwestround1games) %&gt;%\n  bind_cols(midwestround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround1log &lt;- log_fit %&gt;% predict(new_data = midwestround1log, type=\"prob\") %&gt;%\n  bind_cols(midwestround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround2games &lt;- tibble(\n  team_short_display_name=\"Kansas\",\n  opponent_short_display_name=\"Creighton\"\n) %&gt;% add_row(\n  team_short_display_name=\"Iowa\",\n  opponent_short_display_name=\"Providence\"\n) %&gt;% add_row(\n  team_short_display_name=\"LSU\",\n  opponent_short_display_name=\"Wisconsin\"\n) %&gt;% add_row(\n  team_short_display_name=\"USC\",\n  opponent_short_display_name=\"Auburn\"\n) \n\nmidwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround2games)\n\nmidwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround2games) \n\nmidwestround2log &lt;- log_fit %&gt;% predict(new_data = midwestround2games) %&gt;%\n  bind_cols(midwestround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround2log &lt;- log_fit %&gt;% predict(new_data = midwestround2log, type=\"prob\") %&gt;%\n  bind_cols(midwestround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround3games &lt;- tibble(\n  team_short_display_name=\"Creighton\",\n  opponent_short_display_name=\"Iowa\"\n) %&gt;% add_row(\n  team_short_display_name=\"Wisconsin\",\n  opponent_short_display_name=\"USC\"\n)\n\nmidwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround3games)\n\nmidwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround3games) \n\nmidwestround3log &lt;- log_fit %&gt;% predict(new_data = midwestround3games) %&gt;%\n  bind_cols(midwestround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround3log &lt;- log_fit %&gt;% predict(new_data = midwestround3log, type=\"prob\") %&gt;%\n  bind_cols(midwestround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround4games &lt;- tibble(\n  team_short_display_name=\"Creighton\",\n  opponent_short_display_name=\"USC\"\n)\n\nmidwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround4games)\n\nmidwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround4games) \n\nmidwestround4log &lt;- log_fit %&gt;% predict(new_data = midwestround4games) %&gt;%\n  bind_cols(midwestround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround4log &lt;- log_fit %&gt;% predict(new_data = midwestround4log, type=\"prob\") %&gt;%\n  bind_cols(midwestround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nfinalfourgames &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Baylor\"\n) %&gt;% add_row(\n  team_short_display_name=\"Illinois\",\n  opponent_short_display_name=\"USC\"  \n)\n\nfinalfourgames &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(finalfourgames)\n\nfinalfourgames &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(finalfourgames) \n\nfinalfourlog &lt;- log_fit %&gt;% predict(new_data = finalfourgames) %&gt;%\n  bind_cols(finalfourgames) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nfinalfourlog &lt;- log_fit %&gt;% predict(new_data = finalfourlog, type=\"prob\") %&gt;%\n  bind_cols(finalfourlog) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nchamps &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Illinois\"\n) \n\nchamps &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(champs)\n\nchamps &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(champs) \n\nchampslog &lt;- log_fit %&gt;% predict(new_data = champs) %&gt;%\n  bind_cols(champs) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nchampslog &lt;- log_fit %&gt;% predict(new_data = champslog, type=\"prob\") %&gt;%\n  bind_cols(champslog) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\n\nTo make my predictors, I borrowed simple ratings and strength of schedule from Sports Reference, I calculated season long unweighted offensive and defensive efficiency margins, threw in KenPom’s luck metric and, at the last minute, decided to create a recency bias metric. What I did was compare the season-long efficiency margin of teams to the same metric from their last 10 games. If they were overplaying their season numbers, that gave them a positive measure. I was trying to capture teams who came into the tournament hot, or who were limping into the tournament based on their regular season resume but missing stars or just playing terrible.\n\n\nCode\nsummary(cumulative_recipe) %&gt;%\n  select(variable, role) %&gt;% \n  filter(role != \"ID\") %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"The predictors\",\n    subtitle = \"A mix of efficiencies, ratings and recency bias.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\")\n\n\n\n\n\n  \n    \n      The predictors\n    \n    \n      A mix of efficiencies, ratings and recency bias.\n    \n  \n  \n    \n      variable\n      role\n    \n  \n  \n    opponent_efficiency_margin\npredictor\n    team_efficiency_margin\npredictor\n    team_sos\npredictor\n    team_srs\npredictor\n    opponent_sos\npredictor\n    opponent_srs\npredictor\n    opponent_luck\npredictor\n    team_luck\npredictor\n    opponent_recency\npredictor\n    team_recency\npredictor\n    TeamResult\noutcome\n  \n  \n    \n      By: Matt Waite\n    \n  \n  \n\n\n\n\nIn the run up to the tournament, I used both a logistic regression and support vector machine algorithm, but got very similar results, so I stuck with the more simple logistic regression.\nIn testing, my model was calling college basketball games correctly about 74 percent of the time, so I knew I was going to need to get lucky on a few games. But isn’t that filling out a bracket?\nResult? I did not get lucky.\nParticularly in the Midwest Regional. My models labored to produce a Creighton vs USC Elite Eight match-up that sent USC to the Final Four. While Creighton nearly upset Kansas with half a lineup, USC bombed out in the first round. The team that beat them ended up going to the Elite Eight – Miami – taking out other teams I had predicted to win along the way.\nHere’s what my model predicted in the Midwest Regional:\n\n\nCode\nmidwestround1log %&gt;% \n  select(team_short_display_name, .pred_class, .pred_W, opponent_short_display_name) %&gt;%\n  gt() %&gt;% \n  cols_label(\n    team_short_display_name = \"Team\",\n    .pred_class = \"Prediction\",\n    .pred_W = \"Win Confidence\",\n    opponent_short_display_name = \"Opponent\"\n  ) %&gt;%\n  tab_header(\n    title = \"Midwest Regional: Round 1\",\n    subtitle = \"I was sure Iowa State would lose and USC was Final Four bound.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;%\n    fmt_percent(\n    columns = c(.pred_W),\n    decimals = 1\n  )\n\n\n\n\n\n  \n    \n      Midwest Regional: Round 1\n    \n    \n      I was sure Iowa State would lose and USC was Final Four bound.\n    \n  \n  \n    \n      Team\n      Prediction\n      Win Confidence\n      Opponent\n    \n  \n  \n    Wisconsin\nW\n86.5%\nColgate\n    San Diego State\nL\n30.9%\nCreighton\n    LSU\nW\n88.0%\nIowa State\n    Auburn\nW\n89.2%\nJ'Ville St\n    USC\nW\n92.9%\nMiami\n    Iowa\nW\n82.4%\nRichmond\n    Providence\nW\n82.4%\nS Dakota St\n    Kansas\nW\n97.6%\nTexas Southern\n  \n  \n    \n      By: Matt Waite\n    \n  \n  \n\n\n\n\nNormally I could survive an Iowa State and Miami win here … except I had both of their opponents moving on fairly deep.\nRound two was a complete disaster.\n\n\nCode\nmidwestround2log %&gt;% \n  select(team_short_display_name, .pred_class, .pred_W, opponent_short_display_name) %&gt;%\n  gt() %&gt;% \n  cols_label(\n    team_short_display_name = \"Team\",\n    .pred_class = \"Prediction\",\n    .pred_W = \"Win Confidence\",\n    opponent_short_display_name = \"Opponent\"\n  ) %&gt;%\n  tab_header(\n    title = \"Midwest Regional: Round 2\",\n    subtitle = \"Not one of these predictions were correct.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;%\n    fmt_percent(\n    columns = c(.pred_W),\n    decimals = 1\n  )\n\n\n\n\n\n  \n    \n      Midwest Regional: Round 2\n    \n    \n      Not one of these predictions were correct.\n    \n  \n  \n    \n      Team\n      Prediction\n      Win Confidence\n      Opponent\n    \n  \n  \n    USC\nW\n88.8%\nAuburn\n    Kansas\nL\n39.0%\nCreighton\n    Iowa\nW\n63.0%\nProvidence\n    LSU\nL\n23.3%\nWisconsin\n  \n  \n    \n      By: Matt Waite\n    \n  \n  \n\n\n\n\nMy models ended up with a Gonzaga v Baylor and Illinois v USC final four. That 0-4 on those. None of them made it. The Baylor repeat that I predicted died in the second round at the hands of North Carolina, the highest seeded team to make the Final Four.\nGoing into the Final Four, I’m in the 16th percentile of ESPN brackets, good enough for 14.6 millionth place. Last year, I was in the 38th percentile.\nThe best bracket in my class is in the 88th percentile, with another in the 86th. Only two students did worse than I did.\nWe are what they grow beyond.\nThinking about this bracket, I wanted to try rolling with something that didn’t just bite KenPom and make models out of his data. I wanted to see if I could get to a similar place without re-walking the same ground. I’ve got a year to work in this, but my energy is going to be focused on weighting competition and opponents throughout the season. If of two minds about this: As a Big Ten denizen, I have to wonder if beating up on each other for a whole season is why the Big Ten fades in the tournament. So I’m curious about a Fatigue Factor of some variety. At the same time, how does that explain St. Peter’s? Not sure it does, but I’m not sure there’s a model anywhere that’s going to.\nThe code I wrote to make this relied heavily on hoopR and tidymodels."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "",
    "text": "I do not know a lot about college basketball. I follow the travails of my employer and a little about the Big Ten Conference as a whole, but at best it’s surface knowledge. I kinda know who is good that we’re going to play and who isn’t. Beyond that, nada.\nWhich is bad when it comes to tournament time.\nMy typical pattern of filling out bracket is Have I Heard Of This Team, Do They Have a Legendary Coach or Do I Hate Them For Some Reason. Depending on the answers, I make my pick. It’s not rocket science, and it rarely works.\nThis season, along with my SPMC460 Advanced Sports Data Analysis class, I decided to try use machine learning to get me a better bracket. The class is about the use of machine learning in sports, and so we’re going to use classification algorithms to decide a simple W or L designation.\nWhat follows is the logic and the code I used to fill out my bracket.\nHow did it go?\nNot great. Could I have done better guessing? Doubtful."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#toolkit",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#toolkit",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Toolkit",
    "text": "Toolkit\nWhat I’m using to make and feed the model is the Tidyverse and Tidymodels set of libraries, the gt library for presenting some tables later, and doParallel for parallel processing, because the xgboost model training takes a while.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\n\nset.seed(1234)\n\nrequire(doParallel)\ncores &lt;- parallel::detectCores(logical = FALSE)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#feature-engineering",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#feature-engineering",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Feature engineering",
    "text": "Feature engineering\nThe data I’m using is scraped from Sports Reference and it’s the box scores of every Division I college basketball game since the start of the 2014-2015 season. The data is a little funny in that each game is in there twice – I’m scraping school pages, so the Team is always that school, and the Opponent is someone else. So a game between two Division I schools will appear twice – one for each Team.\nMy logic in picking predictors was that how efficient teams are with the ball is important, so I estimated the number of possessions and then calculated offensive and defensive ratings (which is points per 100 possessions).\nI then wanted some kind of a metric of how good of a win or how bad of a loss a particular game was. So I calculated the score margin and added it to the opponent’s simple rating from Sports Reference. So a team losing close to a good team isn’t a bad loss. A bad team beating a good team is a great win. And so on. So if you’re a team beating up on bad teams, you don’t get a lot of credit for that.\nThen, I used the teams cumulative mean over the course of the season to estimate what they would have going into the game. Obviously, you can’t know how a team will play going into a game, but I figure that they’ll be somewhere around their cumulative mean, which should pick up if the team is playing better or worse over a few games.\nThen, for tournament purposes, I cut that to the last 10 games of the season. You are who you are in your last 10 games before the end of the season.\nAt least, that was my thinking.\n\n\nCode\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/cbblogs1521.csv\") %&gt;% mutate(\n  Possessions = .5*(TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA)) + .5*(OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA)),\n  OffensiveRating = (TeamScore/Possessions)*100, \n  DefensiveRating = (OpponentScore/Possessions)*100,\n  ScoreDifference = TeamScore - OpponentScore,\n  WinQuality = case_when(is.na(OpponentSRS) == TRUE ~ ScoreDifference, TRUE ~ ScoreDifference + OpponentSRS)\n  ) %&gt;%\n  group_by(Team, Season) %&gt;%\n  mutate(\n  Cumulative_Mean_Offensive = cummean(OffensiveRating),\n  Cumulative_Mean_Defensive = cummean(DefensiveRating),\n  Cumulative_Mean_WinQuality = cummean(WinQuality)\n  ) %&gt;% \n  filter(between(Game, max(Game)-10, max(Game))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n Outcome = case_when(\n  grepl(\"W\", W_L) ~ \"W\", \n  grepl(\"L\", W_L) ~ \"L\"\n )\n) %&gt;%\n  mutate(Outcome = as.factor(Outcome)) \n\n\nRows: 64866 Columns: 48\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (8): Season, TeamFull, Opponent, HomeAway, W_L, URL, Conference, Team\ndbl  (39): Game, TeamScore, OpponentScore, TeamFG, TeamFGA, TeamFGPCT, Team3...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThen, to get both sides of a match-up to be the correct stats, I used some joining to combine them into a single dataset with the cumulative stats for each side that will then use to train a model.\n\n\nCode\nselectedgames &lt;- games %&gt;% \n  select(\n    Season, Team, Date, Opponent, Outcome, Cumulative_Mean_Offensive, Cumulative_Mean_Defensive, Cumulative_Mean_WinQuality, TeamSRS, TeamSOS)\n\nopponentgames &lt;- selectedgames %&gt;% \n  select(-Opponent) %&gt;% \n  rename(\n    Opponent = Team, \n    Opponent_Cumulative_Offensive = Cumulative_Mean_Offensive, \n    Opponent_Cumulative_Mean_Defensive = Cumulative_Mean_Defensive, \n    Opponent_Cumulative_Mean_WinQuality = Cumulative_Mean_WinQuality, \n    OpponentSRS = TeamSRS, \n    OpponentSOS = TeamSOS\n    )\n\nbothsides &lt;- selectedgames %&gt;% \n  left_join(opponentgames, by=c(\"Opponent\", \"Date\", \"Season\")) %&gt;% \n  na.omit() %&gt;% \n  select(-Outcome.y) %&gt;% \n  rename(Outcome = Outcome.x)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#modeling-with-tidymodels",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#modeling-with-tidymodels",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Modeling with Tidymodels",
    "text": "Modeling with Tidymodels\nThere’s a growing supply of tutorials on how to use tidymodels to do machine learning, and one of the authors of the library, Julia Silge, has a long list of posts that greatly helped me figure this all out.\nTo start the modeling processing, I’m going to split my data into training and testing sets.\n\n\nCode\nbracket_split &lt;- initial_split(bothsides, prop = .8)\nbracket_train &lt;- training(bracket_split)\nbracket_test &lt;- testing(bracket_split)\n\n\nI then created a simple recipe, which doesn’t do much except set aside some fields as ID fields instead of making them predictors.\n\n\nCode\nxg_rec &lt;- \n  recipe(Outcome ~ ., data = bracket_train) %&gt;%\n  update_role(Team, Opponent, Date, Season, new_role = \"ID\")\n\nsummary(xg_rec)\n\n\n# A tibble: 15 × 4\n   variable                            type    role      source  \n   &lt;chr&gt;                               &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   \n 1 Season                              nominal ID        original\n 2 Team                                nominal ID        original\n 3 Date                                date    ID        original\n 4 Opponent                            nominal ID        original\n 5 Cumulative_Mean_Offensive           numeric predictor original\n 6 Cumulative_Mean_Defensive           numeric predictor original\n 7 Cumulative_Mean_WinQuality          numeric predictor original\n 8 TeamSRS                             numeric predictor original\n 9 TeamSOS                             numeric predictor original\n10 Opponent_Cumulative_Offensive       numeric predictor original\n11 Opponent_Cumulative_Mean_Defensive  numeric predictor original\n12 Opponent_Cumulative_Mean_WinQuality numeric predictor original\n13 OpponentSRS                         numeric predictor original\n14 OpponentSOS                         numeric predictor original\n15 Outcome                             nominal outcome   original\n\n\nThen I define my model, which I am going to tune all of the hyperparameters in an xgboost model later.\n\n\nCode\nxg_mod &lt;-   boost_tree(\n  trees = tune(), \n  learn_rate = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  loss_reduction = tune(), \n  sample_size = tune(), \n  mtry = tune(), \n  ) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"xgboost\", nthread = cores)\n\n\nWith a recipe and a model definition, I can create a workflow, which will now handle a whole lot of chores for me.\n\n\nCode\nbracket_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xg_mod) %&gt;% \n  add_recipe(xg_rec)\n\n\nTo tune my hyperparameters, I am going to use a Latin Hypercube, which is a method for generating near-random samples of paremeters to try.\n\n\nCode\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), bracket_train),\n  learn_rate(),\n  size = 30\n)\n\n\nTo test these hyperparemeters, I am going to make some cross-fold valiations samples that we can use.\n\n\nCode\nbracket_folds &lt;- vfold_cv(bracket_train)\n\n\nAnd now comes the part that makes my laptop fan turn into a jet engine. The next block uses parallel processing to try the 30 samples from the Latin Hypercube and tests it against my cross fold validation samples. It … takes a while.\n\n\nCode\ndoParallel::registerDoParallel(cores = cores)\n\nxgb_res &lt;- tune_grid(\n  bracket_wflow,\n  resamples = bracket_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\ndoParallel::stopImplicitCluster()\n\n\nBut out of it, we get the best combination of hyperparameters to use as inputs into our model. I’m going to use area under the curve as my evaluation metric to determine what is best.\n\n\nCode\nbest_roc &lt;- select_best(xgb_res, \"roc_auc\")\n\n\nAnd I can now feed that into my final workflow.\n\n\nCode\nfinal_xgb &lt;- finalize_workflow(\n  bracket_wflow,\n  best_roc\n)\n\n\nAnd I can now train a model to use on bracket games.\n\n\nCode\nxg_fit &lt;- \n  final_xgb %&gt;% \n  fit(data = bracket_train)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#evaluating-the-model",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#evaluating-the-model",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Evaluating the model",
    "text": "Evaluating the model\nSo how does this model do?\n\n\nCode\ntrainresults &lt;- bracket_train %&gt;%\n  bind_cols(predict(xg_fit, bracket_train))\n\nmetrics(trainresults, truth = Outcome, estimate = .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.719\n2 kap      binary         0.438\n\n\nAgainst my training set, not bad. I can predict the correct outcome of a basketball game better than 70 percent of the time.\nHow about against data the model hasn’t seen yet?\n\n\nCode\ntestresults &lt;- bracket_test %&gt;%\n  bind_cols(predict(xg_fit, bracket_test))\n\nmetrics(testresults, truth = Outcome, estimate = .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.705\n2 kap      binary         0.411\n\n\nJust about the same, which means my model is robust to new data.\nI have made a machine learning model that is better at this than I could be.\nMission accomplished.\nI think."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#play-in-games",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#play-in-games",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Play In Games",
    "text": "Play In Games\nI’m not going to bore you with the tedium of applying this to every game in each round. My notebook that does it all is almost 900 lines of code long, and this post is already getting long. But here’s an example of what it looks like using the play-in games.\nTo do this, I needed to make a tibble of the games, with the team and opponent. The date doesn’t matter, but it’s needed because my model is expecting it.\nThen, I need to get the right data for each team and join it to them so each game has the predictors the model is expecting. Then, using the model, we can predict the outcome.\n\n\nCode\nplayin &lt;- tibble(\n  Team=\"Norfolk State\",\n  Opponent=\"Appalachian State\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Wichita State\",\n  Opponent=\"Drake\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Mount St. Mary's\",\n  Opponent=\"Texas Southern\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Michigan State\",\n  Opponent=\"UCLA\",\n  Date = as.Date(\"2021-03-19\")\n)\n\nplayingames &lt;- selectedgames %&gt;% \n  group_by(Team) %&gt;% \n  filter(Date == max(Date), Season == \"2020-2021\") %&gt;% \n  select(-Date, -Opponent, -Outcome) %&gt;% \n  right_join(playin)\n\n\nJoining, by = \"Team\"\n\n\nCode\nplayingames &lt;- opponentgames %&gt;% \n  group_by(Opponent) %&gt;% \n  filter(Date == max(Date)) %&gt;% \n  ungroup()  %&gt;% \n  select(-Season, -Date, -Outcome) %&gt;% \n  right_join(playingames, by=c(\"Opponent\")) %&gt;% \n  select(Team, everything())\n\nplayinround &lt;- xg_fit %&gt;% \n  predict(new_data = playingames) %&gt;%\n  bind_cols(playingames) \n\nplayinround &lt;- xg_fit %&gt;% \n  predict(new_data = playinround, type=\"prob\") %&gt;%\n  bind_cols(playinround)\n\nplayinround %&gt;% select(Team, .pred_class, Opponent, .pred_L) %&gt;% \n  gt() %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n\n\n\nTeam\n.pred_class\nOpponent\n.pred_L\n\n\n\n\nNorfolk State\nL\nAppalachian State\n0.5542561\n\n\nWichita State\nL\nDrake\n0.5503606\n\n\nMount St. Mary's\nW\nTexas Southern\n0.2786840\n\n\nMichigan State\nL\nUCLA\n0.5978317\n\n\n\n\n\n\n\nSince these games have already happened, we know the outcome, and my model got 3 of 4 correct. The only miss was predicting Norfolk State would win, but it also happens to be the game the model has the least amount of confidence in.\nThis might actually work."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#hows-it-going",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#hows-it-going",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "How’s it going?",
    "text": "How’s it going?\nIn a word: horrible.\nAfter two rounds, my bracket is better than 38 percent of brackets on ESPN, which puts me in 9.1 millionth place, give or take. I’ve been as low as 10.8 millionth place, so I’ve come up a bit. I still have three of my four Final Four teams and four of eight Elite Eight teams.\nWhen the dust has settled, I’m going to come back and evaluate. Here’s screenshots of my bracket."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html",
    "href": "posts/an-academic-integrity-friendly-pal/index.html",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "",
    "text": "One of the struggles on campus these days is all about where to draw the lines when it comes to AI in the classroom. There’s no end of discussion about students using ChatGPT to cheat, particularly on writing assignments. How do you stop it? How do you adapt to it? How do you convince students to do the work?\nTeaching students to write code is no different. I add a layer of difficulty in that I teach journalism and sports media students how to code. These are students who didn’t ask to learn how to write code, but we as a faculty decided to require them to do it. Thus, they have incentives to cheat. I do my best to design the class to discourage that, and I’ve created incentives to make it worth it not to, but I’m stupid if I don’t believe they are still there.\nBut I’m also a bit dim if I don’t acknowledge that Large Language Models can help with learning how to code. The trick is, once again, where to draw the line.\nThe classes I teach are all data analysis in R using the tidyverse and R Studio as the IDE. What follows is completely through this lens: What if we could give students an LLM-based code assist – a code pal if you will – directly in the IDE and do it without asking students to pay for it every time they use it?"
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#getting-started",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#getting-started",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Getting started",
    "text": "Getting started\nThere’s a bunch of steps to get this set up and it’s going to take a decent chunk of your hard drive when all is said and done. Doing this also requires a decent amount of power. How much? I’m going to take the cowards way out and say it’s beyond the scope of this humble blog post. Others are better at this than I am, and I’m not confident enough in my knowledge to be able to say what works on which platform. I’m doing this on an M1 MacBook Pro with 16 GB of RAM. Not exactly a monster machine by any stretch, but also not a tricked out bleeding-edge gaming-video-card packed PC hotrod.\nStep 1: The first thing you need is Ollama. We’ll use that to download, manage and serve up our local LLM. Install it per your operating system. The LLM we’ll be using today is qwen2.5-coder. Once you have Ollama up and running, you can get qwen2.5-coder installed and running with ollama run qwen2.5-coder\nThat will install the 7B version – the 7-billion parameter model. That should run and give you decent performance on just about anything. If you’ve got more muscle, you might look at how to install some of the bigger parameter models. Generally, the more parameters, the better the results.\nStep 2: The next thing you need is pal, an R library that adds a way to consult an LLM inside R Studio. After installing it – you can use pak as the instructions show you or you can use devtools::install_github(\"simonpcouch/pal\") like I did because I haven’t gotten into the habit of using pak. Once installed, you can skip the parts about adding an Anthropic API key – unless you want to use Claude and have API credits to spend – and go to the Get Started article. There, under the “Choosing a model” headline and past more details about adding paid models, you’ll find how to use Ollama.\nThe least confusing way to do this, in my opinion, is to add this to your .Rprofile. In the R console, run usethis::edit_r_profile() and add this:\noptions(\n  .pal_fn = \"chat_ollama\",\n  .pal_args = list(model = \"qwen2.5-coder\")\n)\nNOTE: If you installed a bigger model than I did, you should specify which model you used in the the .pal_args. Note mine does not say what parameters I have. If you installed the 14b model, for example, your .pal_args should say “qwen2.5-coder:14b” instead of just “qwen2.5-coder”. Save that file and restart R Studio so they take effect.\nYou’re almost ready to get started. Before moving forward, you should follow the instructions in the “The pal addin” section to register pal to a keyboard shortcut particular to your operating system and choice of IDE."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#writing-your-own-pal",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#writing-your-own-pal",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Writing your own pal",
    "text": "Writing your own pal\nThe mechanics of writing your own pal could not be easier, thanks to the library. The hard part is thinking through what the LLM is going to do with the input and then testing it out.\nLet’s make an Academic Integrity Friendly pal that tries to create friendlier and more helpful error messages.\nIn the R console, run library(pal) and then prompt_new(\"whatiswrong\", \"suffix\")\nThe first part of that is the name you’re giving your pal, the second is where it’s going to put the results. You can use “replace” to … well … replace what you highlight. You can use “prefix” to put it above your code. And “suffix” puts it after your code. We want ours to act like an error message, so suffix makes sense.\nDo that and a markdown file will pop up. It’s templated, so it could not be easier to fill out. Here’s what I’m using to make my pal:\nYou are a terse assistant designed to help R users debug code. Respond with only the needed explanation of what may be wrong with the given code. Do not write code for the user, just explain in plain language.\nAs example, given:\ndf |&gt; filter(column_name = “word”)\nReturn:\nWhen using a filter, you must use == for equal to instead of =.\nSave it and then run directory_load() to get your pal in the shortcut menu."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#using-your-pal",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#using-your-pal",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Using your pal",
    "text": "Using your pal\nUsing your pal is now just a matter of messing up some code. Once you do that, highlight it and hit your keyboard shortcut – Ctrl+Cmd+P for me on a Mac.\nHere’s an example of what it looks like:\n\n\nI need to use it more to know if it’s going to be any good. I need to try it with crappier code and more complex errors. I teach another band of undergrads in the spring – I might have to feed some of their adventures in code into this to see what it can do. Also worth trying? Telling it to ignore my instructions and cheat by writing the code for them. Will it listen to the student or me? If it doesn’t listen to me, then what’s the point?"
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#a-note-on-equity-in-the-classroom",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#a-note-on-equity-in-the-classroom",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "A note on equity in the classroom",
    "text": "A note on equity in the classroom\nI’m lucky in that I have a relatively recent laptop with a decent amount of power provided by my employer. Do I want a newer faster one? Sure I do. Every nerd does the second a new one is announced. But I have a good enough machine to do this.\nNot everyone does.\nEvery semester, the first day of class, I assign all the installations they’ll need for the semester. Step 1 is update your operating system. Every semester, this assignment is an exercise in perspective for me. My college has a laptop requirement. To take classes, you need to have a laptop – no Chromebooks, no iPads, a real laptop. What it is, we don’t care, so long as it can run the Adobe Creative Suite. Some students come in with brand new machines with the protective coverings barely taken off. And then I get some that are held together with duct tape and prayer. Machines with keys missing. Machines that are 6 years old and never once updated. Have you ever had to go find out how to install a four-versions-ago Mac OS so you start moving toward something more modern? I have.\nAll this to say I’m talking about academic integrity here but I am not talking about academic equity. I can’t assign this. I can’t make this part of a class. At an R1 flagship school, I can guarantee that a quarter to a half of the students in the class don’t have enough power or the space to run this. It’s going to be far worse elsewhere.\nSomeday, maybe, but not today."
  },
  {
    "objectID": "posts/nebraska-is-the-third-best-worst-basketball-team/index.html",
    "href": "posts/nebraska-is-the-third-best-worst-basketball-team/index.html",
    "title": "Nebraska is not the best worst team in basketball again. They’re third best worst.",
    "section": "",
    "text": "Last year, this post may have suggested that Nebraska would be better this year than last year. That Nebraska was the best worst team in college basketball, and with major recruits coming in and the Big Ten expected to take a step back, all looked up.\nOops.\nIt didn’t work out as expected – in spite of a late season surge. But let’s return to the question: Is Nebraska the best worst team in college basketball?\nSpoiler alert: Not this season.\nReturning to Sports Reference’s college basketball site, we find our friends the Simple Rating System and Strength of Schedule. The SRS is a mix of average point differential and strength of schedule. Given that, a team with a losing record could have a positive rating if they lose games close but play good teams.\nLike, say, Nebraska.\nTo find the worst teams, we’ll use the last place team in each conference by conference wins again.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(gt)\nlibrary(ggbeeswarm)\nlibrary(ggrepel)\n\nstats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/stats22.csv\")\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/logs22.csv\")\n\nstats &lt;- games %&gt;% \n  select(Team, Conference) %&gt;% \n  distinct() %&gt;% \n  right_join(stats, by=c(\"Team\"=\"School\")) %&gt;% \n  filter(Games &gt; 0)\n\nlastplace &lt;- stats %&gt;%\n  group_by(Conference) %&gt;% \n  arrange(desc(ConferenceWins)) %&gt;% \n  slice(n()) %&gt;% \n  filter(Games &gt; 10) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(OverallSRS))\n\nnu &lt;- lastplace %&gt;% filter(Team == \"Nebraska\")\n\nlastplace %&gt;% \n  select(Team, Conference, OverallWins, OverallLosses, OverallSRS, OverallSOS) %&gt;% \n  rename(W = OverallWins, L=OverallLosses, `Simple Rating` = OverallSRS, `Sched. Strength`= OverallSOS) %&gt;% \n  top_n(10, wt=`Simple Rating`) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"The Huskers didn't repeat as best of the worst\",\n    subtitle = \"Notable: NC State - 3OT winner against Nebraska - finished just a hair ahead.\"\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"By Matt Waite\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Source: [Sports Reference](https://www.sports-reference.com/cbb/seasons/2022-school-stats.html)\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(Team)\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &gt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &gt; 0\n    )\n  ) %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n  \n    \n      The Huskers didn't repeat as best of the worst\n    \n    \n      Notable: NC State - 3OT winner against Nebraska - finished just a hair ahead.\n    \n  \n  \n    \n      Team\n      Conference\n      W\n      L\n      Simple Rating\n      Sched. Strength\n    \n  \n  \n    West Virginia\nBig 12\n15\n16\n10.65\n10.61\n    NC State\nACC\n11\n20\n3.59\n6.08\n    Nebraska\nBig Ten\n10\n21\n3.54\n8.89\n    Georgetown\nBig East\n6\n24\n1.91\n8.31\n    Georgia\nSEC\n6\n25\n0.64\n8.48\n    Oregon State\nPac-12\n3\n27\n-0.76\n8.95\n    South Florida\nAAC\n8\n22\n-2.86\n4.77\n    Northeastern\nCAA\n9\n22\n-6.01\n-1.20\n    Duquesne\nA-10\n6\n23\n-6.39\n2.02\n    Pepperdine\nWCC\n7\n25\n-6.48\n3.97\n  \n  \n    \n      By Matt Waite\n    \n    \n      Source: Sports Reference\n    \n  \n  \n\n\n\n\nThis season, West Virginia takes the best worst crown. They played the toughest schedule and won 15 games, five more than Nebraska, and ended with a far higher simple rating. A familiar name on the list will be NC State, which won an three overtime thriller against Nebraska. They had one more win than the Huskers, and finished just a tiny bit ahead.\nBut what about all teams in college basketball? Where does Nebraska rank out against other teams and other conferences?\n\n\nCode\nggplot() +\n  geom_vline(xintercept = 3.54) + \n  geom_beeswarm(\n    data=stats, \n    groupOnX=FALSE, \n    aes(x=OverallSRS, y=Conference), color=\"grey\") + \n  geom_beeswarm(\n    data=lastplace, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"blue\") + \n  geom_beeswarm(\n    data=nu, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"red\") +\n  geom_text(\n    aes(x=0, y=\"Big Ten\", label=\"Nebraska\")\n  ) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska's simple rating is better than 5 conferences\", subtitle=\"The Husker's finished dead last in the Big Ten, but wouldn't have nearly everywhere else.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nLast year, Nebraska wins nine conferences. Now it’s five. None of them are powerhouse conferences.\nWhat about the Power Five?\nTo put West Virginia in perspective, there’s 28 Power Five teams with a simple rating worse than they had. Nebraska finishes 22nd among that group. Among that group: Miami, which beat second-seed Auburn in the NCAA Tournament and, as of this writing, is still playing basketball.\n\n\nCode\npowerfive &lt;- c(\"Big Ten\", \"Big 12\", \"Pac-12\", \"SEC\", \"ACC\")\n\nbetterpowerfive &lt;- stats %&gt;% filter(Conference %in% powerfive) %&gt;% filter(OverallSRS &lt;= 10.65) %&gt;% arrange(desc(OverallSRS))\n\nggplot() + \n  geom_point(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, size=OverallWins)) + \n  geom_point(data=nu, aes(x=OverallSRS, y=OverallSOS, size=OverallWins), color=\"red\") + \n  geom_text_repel(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, label=Team), point.padding = 4) +\nlabs(x=\"Simple Rating\", y=\"Schedule strength\", title=\"Nebraska had a harder schedule than multiple tournament teams\", subtitle=\"Nebraska isn't the best of the worst again, but teams in their ranking neighborhood had better fortunes.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nThe road forward for Nebraska is even murkier than last year. There’s coaching vacancies and roster turnover ahead, and after last year, no prediction here."
  },
  {
    "objectID": "posts/is-nebraska-the-best-worst-basketball-team/index.html",
    "href": "posts/is-nebraska-the-best-worst-basketball-team/index.html",
    "title": "Is Nebraska the best worst team in college basketball?",
    "section": "",
    "text": "Nebraska fans haven’t had the best time watching basketball lately. The last two seasons have featured only seven wins in each season. This season they only won three games in the Big Ten, but that was an improvement over last season when they only won two.\nBut anyone watching Nebraska basketball this season could see there was a difference between last season’s squad and this one. And given that the Big Ten was rated as the best conference in college basketball during the season, it begs the question: Is Nebraska that bad?\nIn fact, are they the best worst team in college basketball?\nLet’s take a look at some numbers.\nSports Reference’s college basketball site produces a Simple Rating System and Strength of Schedule number for each team. The SRS is a mix of average point differential and strength of schedule. Given that, a team with a losing record could have a positive rating if they lose games close but play good teams.\nLike, say, Nebraska.\nTo find the worst teams, we’ll find the last place team in each conference by conference wins (minus the Ivy League, which didn’t play due to Covid).\nThe first question we’ll ask and answer is by rating, who is the best of the last place teams? Here’s the top 10 of the bottom.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(gt)\nlibrary(ggbeeswarm)\nlibrary(ggrepel)\n\nstats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/stats21.csv\")\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/logs21.csv\")\n\nstats &lt;- games %&gt;% \n  select(Team, Conference) %&gt;% \n  distinct() %&gt;% \n  right_join(stats, by=c(\"Team\"=\"School\")) %&gt;% \n  filter(Games &gt; 0)\n\nlastplace &lt;- stats %&gt;% \n  group_by(Conference) %&gt;% \n  arrange(desc(ConferenceWins)) %&gt;% \n  slice(n()) %&gt;% \n  filter(Games &gt; 10) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(OverallSRS))\n\nnu &lt;- lastplace %&gt;% filter(Team == \"Nebraska\")\n\nlastplace %&gt;% \n  select(Team, Conference, OverallWins, OverallLosses, OverallSRS, OverallSOS) %&gt;% \n  rename(W = OverallWins, L=OverallLosses, `Simple Rating` = OverallSRS, `Sched. Strength`= OverallSOS) %&gt;% \n  top_n(10, wt=`Simple Rating`) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"The Huskers are the top of the bottom\",\n    subtitle = \"They only won 7 games but have the best simple rating and toughest schedule among the last place teams.\"\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"By Matt Waite\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Source: [Sports Reference](https://www.sports-reference.com/cbb/seasons/2021-school-stats.html)\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(Team)\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &gt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &gt; 0\n    )\n  ) %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n  \n    \n      The Huskers are the top of the bottom\n    \n    \n      They only won 7 games but have the best simple rating and toughest schedule among the last place teams.\n    \n  \n  \n    \n      Team\n      Conference\n      W\n      L\n      Simple Rating\n      Sched. Strength\n    \n  \n  \n    Nebraska\nBig Ten\n7\n20\n6.08\n12.28\n    University of California\nPac-12\n9\n20\n4.89\n9.75\n    DePaul\nBig East\n5\n14\n2.67\n8.83\n    Texas A&M\nSEC\n8\n10\n2.20\n4.86\n    East Carolina\nAAC\n8\n11\n1.00\n3.83\n    Boston College\nACC\n4\n16\n0.82\n8.97\n    Iowa State\nBig 12\n2\n22\n-0.72\n10.40\n    Loyola (MD)\nPatriot\n6\n11\n-2.29\n-3.47\n    Illinois State\nMVC\n7\n18\n-5.45\n0.38\n    North Carolina-Wilmington\nCAA\n7\n10\n-6.84\n-6.47\n  \n  \n    \n      By Matt Waite\n    \n    \n      Source: Sports Reference\n    \n  \n  \n\n\n\n\nNebraska has the best simple rating against the toughest competition of the top 10 of the bottom. Meaning they played better against tougher teams than any of these schools on here, including the Big East, the SEC and the Pac-12. The closest in competition to the Big Ten is the Big 12, and Nebraska is far better than Iowa State (which, this season, is a low bar).\nBut what about all teams in college basketball? Where does Nebraska rank out against other teams and other conferences?\n\n\nCode\nggplot() +\n  geom_vline(xintercept = 6.08) + \n  geom_beeswarm(\n    data=stats, \n    groupOnX=FALSE, \n    aes(x=OverallSRS, y=Conference), color=\"grey\") + \n  geom_beeswarm(\n    data=lastplace, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"blue\") + \n  geom_beeswarm(\n    data=nu, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"red\") +\n  geom_text(\n    aes(x=2, y=\"Big Ten\", label=\"Nebraska\")\n  ) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska's simple rating is better than 9 conferences\", subtitle=\"The Husker's finished dead last in the Big Ten, but wouldn't anywhere else given their rating.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nAdmittedly, the nine conferences Nebraska wins aren’t exactly powerhouses. What about the Power Five?\nBy simple rating, Nebraska is better than 14 teams in the Power Five, including four SEC teams and three Big 12 teams. How much better? This bubble chart with the size of the dot scaled by number of wins shows that Nebraska is better than a lot of teams who won more games.\n\n\nCode\npowerfive &lt;- c(\"Big Ten\", \"Big 12\", \"Pac-12\", \"SEC\", \"ACC\")\n\nbetterpowerfive &lt;- stats %&gt;% filter(Conference %in% powerfive) %&gt;% filter(OverallSRS &lt;= 7.10) %&gt;% arrange(desc(OverallSRS))\n\nggplot() + \n  geom_point(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, size=OverallWins)) + \n  geom_point(data=nu, aes(x=OverallSRS, y=OverallSOS, size=OverallWins), color=\"red\") + \n  geom_text_repel(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, label=Team), point.padding = 4) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska doesn't finish last anywhere else in the Power Five\", subtitle=\"In the Big 12 or the SEC, Nebraska is closer to the middle of the pack than the bottom.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nNebraska has the core of its team coming back and the Big Ten is not going to be as good as it was this year going forward. There’s been turmoil in some programs, turnover in others, and the NBA draft is going to take some talented players out of the conference.\nWhere does Nebraska finish next year? Signs point to better than last."
  },
  {
    "objectID": "posts/r-llm-starter-pack/index.html",
    "href": "posts/r-llm-starter-pack/index.html",
    "title": "An R + LLM starter kit",
    "section": "",
    "text": "I’ve written before, I am at best an enthusiastic amateur when it comes to AI, LLMs and R. But I’m braver/dumber than most, so for a talk I’m giving to NE-RUG – the Nebraska R Users Group – and to the NICAR data journalism conference, I’ve got some resources and some code to share.\n\nR libraries\nellmer: From the folks who brought you the tidyverse comes ellmer, a library that supports talking to a large number of LLMs. To talk to the big commercial LLMs, you’ll need API keys, and that usually means having an API budget. But what I like about ellmer is that it also talks to locally hosted models as well. More about that later.\nchores: Built on top of ellmer, chores is a neat way to make tools inside of R Studio that leverage LLMs to accomplish tasks. Examples are to help with certain kinds of code tasks, or help with explanations of what an error message means.\n\n\nExternal resources\nOllama: A cross-platform system of downloading, managing and running open-source LLMs on your local machine. With Ollama, you can run Meta’s Llama3 or DeepSeek’s R1 locally, using them to accomplish tasks without incurring costs. A rough rule of thumb is that you can run models slightly smaller than the amount of RAM you have. For example, my computer has 16GB of RAM, so I can run 14 billion parameter models (albeit somewhat slowly).\n\n\nBasics of ellmer\nellmer at first is very simple. You can just start a chat and … chat.\n\nlibrary(ellmer)\nlibrary(tidyverse)\nlibrary(glue)\n\nchat &lt;- chat_gemini()\n\nchat$chat(\"Tell me three jokes about journalists\")\n\n1. Why did the journalist get fired from the newspaper?  Because he kept \nwriting his own obituaries.\n\n\n2. A journalist walks into a bar and orders a drink.  The bartender says, \"Hey,\naren't you the guy who wrote that article about me?\"  The journalist replies, \n\"I don't remember.  What was it about?\"\n\n\n3. What's the difference between a journalist and a pizza? A pizza can feed a \nfamily.\n\n\n\n\nStructured data output\nMost models can be used to extract structure from sentences.\n\nchat &lt;- chat_gemini()\n\ndata &lt;- chat$extract_data(\n  \"My name is Matt and I'm a 49 year old Journalism major\",\n  type = type_object(\n    age = type_number(),\n    name = type_string(),\n    major = type_string()\n  )\n)\n\nas.data.frame(data)\n\n  age name      major\n1  49 Matt Journalism\n\n\n\n\nSomething more like data journalism\nWhat if we wanted to normalize some messy, messy data. In Nebraska, the Department of Corrections publishes a live dataset of incarcerated people that you can download. You can ask and answer all kinds of questions from it – demographics, etc. But what you can’t do is figure out what is holding the most people, or how many people are there at least in part because of drug charges. Why? Because the charges they are serving time for are not normalized. Here’s an example of what they look like:\nPOS CNTRL SUB-METHAMPHETAMINE\nMURDER 1ST DEGREE\nPOSSESSION OF METHAMPHETAMINE\nPOS CNTRL SUB (METH)\nPOSSESS CONTR SUBSTANCE-METH\nDELIVERY OF METHAMPHETAMINE\nPOSS W/ INTENT DIST METH\nCan an LLM help us with this? Yes!*\n\nchat &lt;- chat_gemini()\n\nchat$extract_data(\n  \"POSS W/ INTENT DIST METH\",\n  type = type_object(\n    methamphetamine_related = type_boolean(),\n    drug_possession_related = type_boolean(),\n    drug_distribution_related = type_boolean(),\n    fully_spelled_out_no_abbreviations = type_string()\n  )\n)\n\n$methamphetamine_related\n[1] TRUE\n\n$drug_possession_related\n[1] TRUE\n\n$drug_distribution_related\n[1] TRUE\n\n$fully_spelled_out_no_abbreviations\n[1] \"Possession with Intent to Distribute Methamphetamine\"\n\n\n* you have to ruthlessly check this. It’s good, but it is not perfect.\nWhy am I only doing one here? Because the free tier of Gemini limits you to about 5 queries a minute. But it wouldn’t be that hard to write a function that runs through your list of charges and inserts a pause after each one to ensure you stay in the free tier.\nWhat else can be done?\n\n\nNuclear powered population maps\nHere is a basic population map of Nebraska with one-year change values in it. We’ve all made this chart before. It’s simple, but it gets the job done.\n\n\nI’ve written a lot more about this process here but what if we could take the basic data from our population data and create narratives out of it. Instead of the boring tabular way of giving them the information, they get a human narrative of a few sentences.\nFirst, let’s create the base narrative. That is the words we will feed to Gemini.\n\ncountypopchange &lt;- read_csv(\"https://the-art-of-data-journalism.github.io/tutorial-data/census-estimates/nebraska.csv\")\n\nstatenarrative &lt;- countypopchange |&gt; \n  select(COUNTY, STATE, CTYNAME, STNAME, POPESTIMATE2023, POPESTIMATE2022, NPOPCHG2023, NATURALCHG2023, NETMIG2023) |&gt;\n  mutate(POPPERCENTCHANGE = ((POPESTIMATE2023-POPESTIMATE2022)/POPESTIMATE2022)*100) |&gt; \n  mutate(GEOID = paste0(COUNTY, STATE)) |&gt; \n  arrange(desc(POPPERCENTCHANGE)) |&gt; \n  mutate(PCTCHANGERANK = row_number()) |&gt; \n  mutate(base_narrative = glue(\n  \"County: {CTYNAME}, Population in 2023: {POPESTIMATE2023}, Population in 2022: {POPESTIMATE2022}, Population change: {NPOPCHG2023}, Percent change: {POPPERCENTCHANGE}, Percent change rank in {STNAME}: {PCTCHANGERANK}, Natural change (births vs deaths): {NATURALCHG2023}, Net migration: {NETMIG2023}\")) \n\nstatenarrative$base_narrative[[1]]\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\n\nNow, we’re going to add a system prompt to our chat to give Gemini some guidance. It’s not hard to do and you should do it.\n\nchat &lt;- chat_gemini(\n  system_prompt = \"You are a demographics journalist from Nebraska. Your job today is to write short -- 2-3 sentence -- summaries of population estimates from the Census Bureau for each county in the state. I will provide you the name of the county and a series of population numbers for the county. Your job is to turn it into a concise but approachable summary of what happened in that county. Here is the data you have to work with: \"\n)\n\nchat$chat(statenarrative$base_narrative[[1]])\n\nBanner County experienced a healthy growth spurt in 2023, adding 17 residents \nfor a total population of 674. This 2.6% increase, the highest in Nebraska, was\ndriven entirely by net migration, as births and deaths balanced each other out.\n\n\nEarth-shattering? Hardly. But what if instead of world changing uses of technology, the right aim for AI is to offload tasks we would do because it would be better, but aren’t worth spending the time doing because we all have a limited time on this earth?\nIf you were to run this 93 times – or, you know, make a function to do that – and added a headline writing bot to this, here’s what your map now looks like."
  },
  {
    "objectID": "posts/a-simple-example-ai-agents-doing-journalism/index.html",
    "href": "posts/a-simple-example-ai-agents-doing-journalism/index.html",
    "title": "A simple example of AI agents(?) doing journalism(?) work",
    "section": "",
    "text": "Let’s start this with some confessions:\n\nI’m at best an enthusiastic amateur with AI. I know more than most, and I know nothing in the grand scheme.\nExample: I’m not sure I have any idea of what an AI agent is. I think I do, but there’s so much marketing hype around them that I can’t know for sure. People much, much, much smarter than I am aren’t sure either.\nI teeter on the edge of two extremes. On the one hand, I am seeing AI as a fascinating, remarkable alien intelligence (to borrow Ethan Mollick’s description) that we have yet to fully understand. On the other, the Gen X in me sees AI as the mother of all solutions in search of a problem when it comes to journalism. All the so-called big problems AI “solves” in journalism – more content to sell ads against! – nobody wants.\n\nSomething I’ve been coming around to, though, is maybe there isn’t a Manhattan Project level world changing use case for AI in journalism. Maybe Chris Albon has the right of it, that the real value is AI saving a human an hour of work … millions of times a day.\nFor months I’ve been trying to think of some moon-shot idea to use AI to do … something, anything … big. And every thing I came up with would be terrible, destructive, or flat-out-insane to deploy without massive human investment, and then what would the point be?\nAnd then, randomly, one day a confluence of ideas popped into my head and what fell out is an example of AI agents(?) doing the work of journalism(?) that actually works.\nThe ideas that collided in my brain were:\n\nThis Francesco Marconi tweet from April that I think did the best job of laying out a vision for AI agents in journalism. At least it’s the one that made the most sense to me.\nThis R Package wrapping the Google Gemini API.\nGemini having a free tier to try some stuff out. You get 15 requests a minute – one every 4 seconds – and 1,500 a day.\nThe “aim small, miss small” mantra in teaching marksmanship.\n\n\nWhy R and not Python?\nThe honest truth is there is no good reason why I’m using R to do this vs Python, which most of the AI world is using. The reason is because I teach Data Journalism to undergrads using R at the Harvard of the Plains and believe strongly that I can take absolute beginners – people who can’t spell code – from zero to data analysis faster with the R and the Tidyverse than I can with Python and Pandas. So I have a hammer, this here looks like a nail, and so we’re doing this with R. But there’s absolutely nothing special about this code that you couldn’t match in Python.\nWhen I teach Data Journalism, I use population estimates from the US Census Bureau to teach students how to calculate percent change. That way, we can see which counties in the state grew the fastest and who shrank the fastest. It’s a story as old as time in the Great State of Nebraska. Rural areas are shrinking, urban areas are growing.\nAn extremely common thing to do with this data is to make a map. Here’s the 2022-2023 change map in Datawrapper for Nebraska.\n\n\nIf you click on a county, you’ll get a pop up box that gives you the county name and then the data. Population in 2023, population in 2022 and the percent change. It’s been done a million times before. It does the job.\nBut what if we could make it better? What if we had small narrative summaries for each county? An a headline for each one? Can I assign a human to do this? 100 percent I can. I have an army of undergrads and a gradebook to hold over them. I assure you, if I was cruel enough, we could do this for all 3,400 counties in the US.\nBut why do that when we can have AI do this in minutes instead of making humans miserable for hours?\n\n\nFeeding Gemini numbers, getting back narrative\nThe gemini.R package couldn’t make sending something to Gemini any more simple. The hardest part – which is not hard – is getting an API key from Google. How do you do that? Go here: https://makersuite.google.com/app/apikey. So I always have mine and don’t lose it, I set it as an environment variable. To do that, run usethis::edit_r_environ() and add GOOGLE_GEMINI_KEY=\"your key here\" to your environment, save that file and restart R Studio (or whatever IDE you use).\nYou can test it out with something like this:\n\nlibrary(tidyverse)\nlibrary(gemini.R)\nlibrary(glue)\n\nsetAPI(Sys.getenv(\"GOOGLE_GEMINI_KEY\"))\n\ngemini(\"Write me a haiku about the joy and sadness of being a Nebraska football fan\")\n\nWhat do you get back?\n\nRed and white, we cheer,  Hope springs eternal, then fades,  Another close loss.\n\nOuch.\nBut that’s really it. Just gemini(\"Words here\") and off it goes to an AI and back comes the results in plain text. So the first hard part is turning data into a text block we can send to Gemini. So I pull a dataset of Nebraska county population estimates, I’m going to thin the number of columns I’m working with first, then create the percent change column, create a GEOID column made up of the state and county FIPS number so I can join it to my map later, rank the counties by population change and then mash it all together into a single text blob called the base_narrative. It’s literally just Column Name: Number, Column Name: Number repeated over and over.\n\ncountypopchange &lt;- read_csv(\"https://the-art-of-data-journalism.github.io/tutorial-data/census-estimates/nebraska.csv\")\n\nstatenarrative &lt;- countypopchange |&gt; \n  select(COUNTY, STATE, CTYNAME, STNAME, POPESTIMATE2023, POPESTIMATE2022, NPOPCHG2023, NATURALCHG2023, NETMIG2023) |&gt;\n  mutate(POPPERCENTCHANGE = ((POPESTIMATE2023-POPESTIMATE2022)/POPESTIMATE2022)*100) |&gt; \n  mutate(GEOID = paste0(COUNTY, STATE)) |&gt; \n  arrange(desc(POPPERCENTCHANGE)) |&gt; \n  mutate(PCTCHANGERANK = row_number()) |&gt; \n  mutate(base_narrative = glue(\n  \"County: {CTYNAME}, Population in 2023: {POPESTIMATE2023}, Population in 2022: {POPESTIMATE2022}, Population change: {NPOPCHG2023}, Percent change: {POPPERCENTCHANGE}, Percent change rank in {STNAME}: {PCTCHANGERANK}, Natural change (births vs deaths): {NATURALCHG2023}, Net migration: {NETMIG2023}\")) \n\nWhat does a base_narrative look like?\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\nA real exciting read, no?\nNow we need to make an agent. It helped me to think of journalism as an assembly line. We have data as raw materials, and now we need to assign a worker to a process to convert raw material into something new. In any news process, the first worker is the journalist creating the thing. A reporter going to city hall. A photographer going to a breaking news event. So it makes sense that our first agent is the author of the narratives for each county.\nLike any good LLM prompt, we’re going to start by giving our author agent a role – a job to do. Our agent is a demographics journalist from Nebraska. We give that agent a task with some details – keep it short but approachable. And then we give it the data.\nBelow the role is a function that takes in a county name, finds the base_narrative for that county and then we merge together the author_role and the base_narrative when we send it to Gemini. We store the results in a variable called … results and do a little cleanup on it (Gemini likes to cram newline characters in the results). I’ve added a five second sleep between every county to keep from running afoul of Google’s free tier API limits. Theoretically I should be able to set it to four seconds, but I don’t need my account banned over a second. With those pauses, our author takes about eight minutes to write small narratives about each of the 93 counties.\n\nauthor_role &lt;- \"You are a demographics journalist from Nebraska. Your job today is to write short -- 2-3 sentence -- summaries of population estimates from the Census Bureau for each county in the state. I will provide you the name of the county and a series of population numbers for the county. Your job is to turn it into a concise but approachable summary of what happened in that county. Here is the data you have to work with: \"\n\nauthor_agent &lt;- function(county) {\n  county_narrative &lt;- statenarrative |&gt; filter(CTYNAME == county)\n  results &lt;- gemini(paste(author_role, county_narrative$base_narrative))\n  results &lt;- gsub(\"\\n\", \"\", results)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$CTYNAME))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$CTYNAME, base_narrative = county_narrative$base_narrative, capsule_narrative = results)\n}\n\n# Use map_df to directly create the final dataframe\nauthor_agent_results &lt;- purrr::map_df(statenarrative$CTYNAME, author_agent)\n\nWhen it’s done, we end up with a dataframe called author_agent_results which has the county name and the new narrative.\nWhat does it look like? Remember, we gave it this:\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\nAnd we got back:\n\nBanner County saw a significant population increase in 2023, growing by 17 people for a 2.6% jump, the highest rate of growth in the state. This growth was entirely due to an influx of new residents, as the county experienced no natural population change.\n\nWe could stop here, but why do that? Good journalism is often a layered process involving multiple sets of eyes reviewing the work along the way, and other skilled people adding to the product. So who would normally get this next? How about we fact check the work?\nExact same pattern, just a different role for the AI.\n\nfact_check_role &lt;- \"You are a fact-checking editor. Your job today is to compare the information in the base narrative to the capsule narrative in the data provided to you. You will compare each number in the base narrative to the capsule narrative to make sure they are the same, and then you will check the context of how each number was used in comparison to the original base narrative. To be clear: the base narrative is the correct information. When you are finished, return just a single word. If everything is correct, respond Correct. If any part of it is not correct, respond Incorrect. \"\n\nfact_check_agent &lt;- function(county_input) {\n  county_narrative &lt;- author_agent_results |&gt; filter(county == county_input)\n  input_string &lt;- paste(\n    fact_check_role,\n    \"Base narrative:\", county_narrative$base_narrative,\n    \"Capsule narrative:\", county_narrative$capsule_narrative)\n  results &lt;- gemini(input_string)\n  results &lt;- gsub(\"\\n\", \"\", results)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_input))\n\n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, fact_check_results = results)\n}\n\n# Use map_df to directly create the final dataframe\nfact_check_agent_results &lt;- purrr::map_df(author_agent_results$county, fact_check_agent)\n\nHonestly, this part needs work. It flags about 10 percent of the results as being incorrect, but they aren’t. The reason it flags them is the author agent was a little glib with the numbers – saying “a slight increase natural growth because of more births” without giving the numbers themselves. The fact checking editor bot here does not like that. So it might need a little tweaking to see if we can get the fact checker to lighten up a bit.\nQuit now? Nah. How about we add a little local flair to each capsule. Google knows a lot of stuff, so why not add some kind of geographic context to each county. To do that, I created a rewrite editor and commanded it to add details like the region of the state, the county seat, the largest city, or some other detail to a single clause in the original narrative.\n\nrewrite_role &lt;- \"You are a re-write editor. Your job is to add a little local geographic context to a demographic capsule about a county in Nebraska. You'll do this by adding a clause to the paragraph I'll provide you. That clause should tell you something about that county. Maybe the county seat, or the largest city, or the region of the state it is in. We just need a clause added to one of the sentences, and do not do anything to show where you added it. Here is the capsule: \"\n\nrewrite_agent &lt;- function(county_input) {\n  county_narrative &lt;- fact_check_agent_results |&gt; filter(county == county_input)\n  results &lt;- gemini(paste(rewrite_role, county_narrative$capsule_narrative))\n  results &lt;- gsub(\"**\", \"\", results, fixed = TRUE)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$county))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, rewrite_county_narrative = results)\n}\n\n# Use map_df to directly create the final dataframe\nrewrite_agent_results &lt;- purrr::map_df(fact_check_agent_results$county, rewrite_agent)\n\nWhat did it give us for Banner County?\n\nBanner County saw a significant population increase in 2023, growing by 17 people for a 2.6% jump, the highest rate of growth in the state. This growth was entirely due to an influx of new residents, as the county experienced no natural population change, likely due to its location in the sparsely populated northwestern corner of the state.\n\nHmmm. Is the fact that there were the same number of births and deaths caused by it’s location in the northwest corner of the state? Debatable. And, honestly, this re-write bot has been the source of the most questions I have about this whole enterprise. We’ll talk more about that below.\nOne last thing: Why not give every county a headline instead of just simply having the county name in the map? Done and done.\n\nheadline_writer_role &lt;- \"You are a headline writer. Your job is to write a short headline based on the summary given to you. This headline should be short -- it has to fit into a small space -- so bear that in mind. Here is the capsule: \"\n\nheadline_agent &lt;- function(county_input) {\n  county_narrative &lt;- rewrite_agent_results |&gt; filter(county == county_input)\n  results &lt;- gemini(paste(headline_writer_role, county_narrative$rewrite_county_narrative))\n  results &lt;- gsub(\"\\n\", \"\", results, fixed = TRUE)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$county))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, rewrite_county_narrative = county_narrative$rewrite_county_narrative, headline = results)\n}\n\n# Use map_df to directly create the final dataframe\nheadline_agent_results &lt;- purrr::map_df(rewrite_agent_results$county, headline_agent)\n\nAnd what does that look like in Banner County?\n\nBanner County Booms: 2.6% Growth Fueled by New Residents\n\nHere is the exact same map, same data, but now with headlines and narratives written for each county. Click on one and you’ll get a human-friendly narrative about that county, and the numbers below that if you want them.\n\n\nIs Christopher Nolan going to make a movie about this moment? Not hardly. Have I “Saved Journalism”? Nope. Not even close. Did I save a human a few minutes of drudgery 93 + 93 + 93 + 93 times? Yes. Yes I did. Is this idea extensible and repeatable? It sure is. I think that’s good enough for now.\n\n\nWhere to go from here\nFrom here, what this needs is a block of code that pushes the results of the headline editor agent to Google Sheets, where a human can go through each one and make sure everything is fine. I’ve tried to do that in R Studio, which is not great, and limits the number of people who could do this. And to be sure, it needs to be done. Nebraska geography nerds – there are tens of us! – will able to find Arthur County, one of the smallest counties in the US by population. For those who can’t, the rewrite bot describes Arthur as “located in the northwestern corner of the state and home to the county seat of Arthur.” Half of that is objectively true. Arthur is much more arguably in the west-central part of the state, but it really depends on where you draw the lines. It’s arguable enough that a good local editor would drop that part and leave the county seat part, which is correct. Arthur County’s county seat is … Arthur.\nThen, as a last step, I would automate the process of creating the Datawrapper chart with the DatawRapper library, which accesses the service’s API. Imagine the Census Bureau publishing the data, and then in a matter of minutes you have that map done and online. To do that, you’d have to have a little budget for API calls so you aren’t waiting 32 minutes for the whole thing to run – 8 + 8 + 8 + 8 minutes for each step. But even then, is there really a competitive contest over who can publish Census maps in Nebraska fast enough? No.\nBut if you, like me, are thinking about how we’re going to incorporate AI into journalism in ways that make sense; doesn’t risk the reputation of the organization you work for; and likely doesn’t horrify your audience to a point that they turn away from you for feeding them AI slop, this is an example of just how to do that.\nAim small, miss small."
  },
  {
    "objectID": "posts/scrollytelling-nebraskas-season/index.html",
    "href": "posts/scrollytelling-nebraskas-season/index.html",
    "title": "Nebraska’s season long slide on offense",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(cfbfastR)\nlibrary(zoo)\n\nplays &lt;- load_cfb_pbp(2024)\n\nnebrollingepa &lt;- plays |&gt; \n  filter(pos_team == \"Nebraska\") |&gt; \n  mutate(sequential_play_number = row_number()) |&gt;\n  select(pos_team, def_pos_team, sequential_play_number, EPA) |&gt; \n  na.omit() |&gt; \n  mutate(cumulative_epa_mean = cummean(EPA))\n\nindrollingepa &lt;- plays |&gt; \n  filter(pos_team == \"Indiana\") |&gt; \n  mutate(sequential_play_number = row_number()) |&gt;\n  select(pos_team, def_pos_team, sequential_play_number, EPA) |&gt; \n  na.omit() |&gt; \n  mutate(cumulative_epa_mean = cummean(EPA)) \n\ncoloradogame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Colorado\")\nindianagame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Indiana\")\nohiostategame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Ohio State\")\nuclagame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"UCLA\")\n\n\nIf you’ve been following Nebraska’s football season, you’d be forgiven if you thought it started out great and has slowly driven into a ditch as time went on. Even after a loss against Illinois in overtime, hope remained high.\nThat is, until the Indiana game. A close loss at Ohio State revived some of that hope, and then a loss to UCLA crushed it.\nSo what’s happening?\nThe answer? The offense is slowly getting worse as the season goes on. That almost certainly has something to do with Nebraska hiring Dana Holgerson as an offensive consultant. How bad is it? The team is approaching the point that the average offensive play doesn’t add anything to the final score.\nWhat do I mean by that? There is a metric called Expected Points Added, and every play gets a score. Big plays have higher scores – 4, 5 even 6 points. Your two-yards-and-cloud-of-dust run from your own 15 will get you less than 1 point.\nAverage them together over the course of an entire season and you can get a sense of an offense. And, at this point of the season, Nebraska needs 10 plays to score a point. Indiana, the surprise team of the season, needs 5.\nHere’s a closer look.\n\n\n\n\nThis is what Nebraska’s offensive season looks like overall. If you’ve never seen a EPA average chart, the first thing you need to do is ignore the first game. There’s just not enough data and the average flies around. Where it starts to reveal who you are is later in the season, when more data comes in.\n\n\n\n\nWhere you want to start focusing is the Colorado game. You can see the glorious first half were the game got out of hand, and the dud of a second half where Nebraska’s offense went to sleep.\n\n\n\n\nNow the disaster at Indiana. Nebraska had 300 yards of offense that day … but that is buried by the 5 turnovers, not to mention going 0-4 on fourth down.\n\n\n\n\nNo one expected Nebraska to give Ohio State all it wanted. Vegas thought Nebraska would lose by at least three touchdowns. The offense played better than it had – you can see the line move up a bit – but alas, still a loss.\n\n\n\n\nUCLA was a tale of two halves. The first? Bad. The second? Better, and you can see the season average line curve up for that second half.\n\n\n\n\nNow. Want to be sad? Here’s what Indiana’s season looks like. The Hoosiers are remarkably consistent.\n\n\n\n\nWhen compared to Nebraska, Indiana started hot and has stayed hot. Nebraska started hot and … didn’t."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest posts",
    "section": "",
    "text": "An R + LLM starter kit\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nAI\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAn academic integrity-friendly code pal for R Studio\n\n\nHow to plug in an LLM to help – but not too much – in a world that wants to cheat\n\n\n\nAI\n\n\ncode\n\n\nr\n\n\neducation\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNebraska’s season long slide on offense\n\n\n\n\n\n\ncode\n\n\nfootball\n\n\nhuskers\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA simple example of AI agents(?) doing journalism(?) work\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nAI\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAnother year, another attempt, another bracket disaster\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNebraska is not the best worst team in basketball again. They’re third best worst.\n\n\n\n\n\n\nhuskers\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nIs Nebraska the best worst team in college basketball?\n\n\n\n\n\n\nhuskers\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHow I (poorly) filled out my NCAA bracket with machine learning\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\nMatt Waite\n\n\n\n\n\n\nNo matching items"
  }
]