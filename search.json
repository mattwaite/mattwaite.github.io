[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Parsing PDFs with Antigravity\n\n\nIn a word: Gobsmacked.\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nNov 24, 2025\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAn R + LLM starter kit\n\n\n\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAn academic integrity-friendly code pal for R Studio\n\n\nHow to plug in an LLM to help – but not too much – in a world that wants to cheat\n\n\n\nAI\n\ncode\n\nr\n\neducation\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNebraska’s season long slide on offense\n\n\n\n\n\n\ncode\n\nfootball\n\nhuskers\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA simple example of AI agents(?) doing journalism(?) work\n\n\n\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAnother year, another attempt, another bracket disaster\n\n\n\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNebraska is not the best worst team in basketball again. They’re third best worst.\n\n\n\n\n\n\nhuskers\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nIs Nebraska the best worst team in college basketball?\n\n\n\n\n\n\nhuskers\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHow I (poorly) filled out my NCAA bracket with machine learning\n\n\n\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing a World Cup Team to root for in each match: An algorithmic approach.\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nI wrote a data journalism manual for my college in 1997. They never used it, but they kept it.\n\n\n\n\n\n\ndata journalism\n\nretro\n\n\n\n\n\n\n\n\n\nMar 24, 2016\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Lego to teach data visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2015\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nEverything I Know About Data I Learned From 70s Album Rock…\n\n\n\n\n\n\ndata journalism\n\ntalks\n\n\n\n\n\n\n\n\n\nDec 9, 2014\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA classroom experiment in Twitter Bots and creativity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2014\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nUNL CoJMC students: Two data/programming/future courses for the fall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2014\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA 5-step NICAR recovery plan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2014\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA small step toward solving the our curriculum is too full problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2014\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nUnconference panel pitch I will someday make\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nThe Minimum Viable Participant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nReplacing Reporters With Robots I Might Be\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHere Is John Keefe And I Talking About The Near\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA new way for data journalists to thwart newsroom IT: the Raspberry Pi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAdventures in prototyping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nWriting stories with code, part 2: conditional leads from trends\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nWriting stories with code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2013\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nDrones Soldering Irons Micro Controllers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNumeracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHelp me plan a hacker space/drone lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nIf you were teaching a course in data visualization…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nToward a solution to the more tech in J-school problem\n\n\n\n\n\n\njournalism\n\neducation\n\nmanifesto\n\nrant\n\n\n\n\n\n\n\n\n\nMar 14, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Python to access tweets from the command line\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA completely arbitrary list of takeaways from two unconferences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nA word of advice for Code Year participants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nData journalism class description: your thoughts?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2012\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nThinking out loud: The management wisdom of Battlestar Galactica\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHelp me refine an incomplete idea\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nJournalism students vs. tech-focused students\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nNews nerd rage: the trouble with conferences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nMe vs. 130 Journalism 101 students: The epic Q&A\n\n\n\n\n\n\njournalism\n\nfuture\n\nmanifesto\n\npolitifact\n\n\n\n\n\n\n\n\n\nSep 27, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nSome very smart posts about killing your CMS\n\n\n\n\n\n\njournalism\n\ndata\n\n\n\n\n\n\n\n\n\nJul 15, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nTake A Few Minutes And Watch This And 1 Tell Me\n\n\n\n\n\n\nresearch\n\njournalism\n\nfuture\n\n\n\n\n\n\n\n\n\nJul 15, 2011\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nWhat would you want out of a class taught by a journalist-programmer?\n\n\n\n\n\n\njournalism\n\neducation\n\n\n\n\n\n\n\n\n\nNov 23, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nThe key lesson I learned building PolitiFact: Demos, not memos\n\n\n\n\n\n\njournalism\n\ndjango\n\npolitifact\n\nnewsapps\n\n\n\n\n\n\n\n\n\nApr 27, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nTelling the Google Bot no\n\n\n\n\n\n\nnewsapps\n\n\n\n\n\n\n\n\n\nApr 12, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nBuild something or STFU\n\n\n\n\n\n\njournalism\n\nnewsapps\n\n\n\n\n\n\n\n\n\nMar 3, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter, marketing and the devil\n\n\n\n\n\n\nnewsapps\n\ntwitter\n\nmarketing\n\n\n\n\n\n\n\n\n\nJan 25, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nData = Content: Content = Data\n\n\n\n\n\n\njournalism\n\ndata\n\ncontent\n\nplatforms\n\nnewsapps\n\n\n\n\n\n\n\n\n\nJan 20, 2009\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nHow not to be a Wordpress hero\n\n\n\n\n\n\njournalism\n\n\n\n\n\n\n\n\n\nJul 18, 2008\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on Everyblock and context\n\n\n\n\n\n\ndata\n\njournalism\n\nnewsapps\n\n\n\n\n\n\n\n\n\nJan 28, 2008\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nMolten content, data ghettos and why your CMS problems are an excuse, not a reason\n\n\n\n\n\n\njournalism\n\ndata\n\nnewsapps\n\n\n\n\n\n\n\n\n\nJan 12, 2008\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nData ghettos\n\n\n\n\n\n\ndata\n\njournalism\n\nnewsapps\n\nrant\n\n\n\n\n\n\n\n\n\nJan 4, 2008\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the journalist in programmer/journalist matters\n\n\n\n\n\n\njournalism\n\nnewsapps\n\n\n\n\n\n\n\n\n\nSep 11, 2007\n\n\nMatt Waite\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing PolitiFact\n\n\n\n\n\n\npolitifact\n\njournalism\n\ndata\n\nnewsapps\n\n\n\n\n\n\n\n\n\nAug 22, 2007\n\n\nMatt Waite\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My career merges practical experience in investigative journalism and news technology with my current role as an academic leader in journalism education. Currently serving as a Professor of Practice in Journalism at the University of Nebraska-Lincoln, my teaching encompasses a range of contemporary skills, including data journalism, sports data analysis and visualization, reporting, and various technology-related courses that explore the applications of drones and artificial intelligence within the field."
  },
  {
    "objectID": "about.html#biography",
    "href": "about.html#biography",
    "title": "About",
    "section": "Biography",
    "text": "Biography\n\nEarly Career\nBefore transitioning to academia, I spent nearly 15 years in daily newspapers in both Arkansas and Florida. My tenure at the Tampa Bay Times in Florida included awards for a co-authored series of stories that meticulously examined and exposed the state and federal efforts to protect wetlands. This work was later expanded into the book, “Paving Paradise: Florida’s Vanishing Wetlands and the Failure of No Net Loss,” published in 2009.\n\n\nPolitiFact\nA pivotal moment in my career occurred in 2007 when I took on the role of principal developer for PolitiFact.com. This project showcased my ability to blend journalistic insight with technical proficiency. In a landmark achievement for digital journalism, PolitiFact.com became the first website to be awarded a Pulitzer Prize in 2009, specifically for national reporting.\n\n\nTransition to Academia\nIn 2011, I joined the faculty at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. This transition provided me with an opportunity to share my knowledge and practical experience with aspiring journalists. As a Professor of Practice, my teaching portfolio encompasses a range of courses that reflect the evolving skills and technologies crucial for modern journalism."
  },
  {
    "objectID": "about.html#the-drone-journalism-lab",
    "href": "about.html#the-drone-journalism-lab",
    "title": "About",
    "section": "The Drone Journalism Lab",
    "text": "The Drone Journalism Lab\nI founded the Drone Journalism Lab at the University of Nebraska-Lincoln, recognizing the potential of drone technology for newsgathering colliding with federal regulations and journalistic ethics. The lab’s primary mission revolved around exploring the use of drones in news reporting and providing students with comprehensive training on how to operate these devices responsibly and ethically. During the lab’s time, students actively utilized drones to cover news events across six countries on three continents."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nProfessor of Practice, University of Nebraska-Lincoln (2011-Present)\nCo-founder, Hot Type Consulting (2009-2021)\nSenior News Technologist, St. Petersburg Times (2006-2011)\nStaff Writer, St. Petersburg Times (2000-2006)\nStaff Writer, Arkansas Democrat-Gazette (1998-2000)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nMS, Business Analytics, University of Nebraska-Lincoln, 2020\nBA, Journalism, University of Nebraska-Lincoln, 1997"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nData Analysis & Visualization: R, Python, Excel, SQL\nWeb Development: HTML, CSS, JavaScript, Python\nGIS & Spatial Analysis: ArcGIS, QGIS\nDrone Piloting: FAA Remote Pilot Certificate"
  },
  {
    "objectID": "about.html#teaching",
    "href": "about.html#teaching",
    "title": "About",
    "section": "Teaching",
    "text": "Teaching\nI have developed and taught 27 unique courses, across three academic colleges, including:\n\nData Journalism\nSports Data Analysis and Visualization\nDrone Journalism\nAI for Journalism\nNews Applications\nSensors for Journalism"
  },
  {
    "objectID": "about.html#selected-awards",
    "href": "about.html#selected-awards",
    "title": "About",
    "section": "Selected Awards",
    "text": "Selected Awards\n\nPulitzer Prize for National Reporting (2009)\nNebraska Journalism Hall of Fame (2016)\nDaily Nebraskan Hall of Fame (2022)\nUniversity of Nebraska Innovation, Development, and Engagement Award (2016)"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "About",
    "section": "Talks",
    "text": "Talks\nFor a collection of my talks, please visit the Talks page."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Here is a collection of talks I’ve given over the years."
  },
  {
    "objectID": "talks.html#the-drone-between-us-matt-waite-tedxomaha",
    "href": "talks.html#the-drone-between-us-matt-waite-tedxomaha",
    "title": "Talks",
    "section": "The drone between us | Matt Waite | TEDxOmaha",
    "text": "The drone between us | Matt Waite | TEDxOmaha"
  },
  {
    "objectID": "talks.html#what-military-history-tells-us-about-the-future-of-media-matt-waite-at-tedxunl",
    "href": "talks.html#what-military-history-tells-us-about-the-future-of-media-matt-waite-at-tedxunl",
    "title": "Talks",
    "section": "What Military History Tells Us About the Future of Media: Matt Waite at TEDxUNL",
    "text": "What Military History Tells Us About the Future of Media: Matt Waite at TEDxUNL"
  },
  {
    "objectID": "talks.html#the-next-thing-for-drones-matt-waite---mobile-me-you-2022",
    "href": "talks.html#the-next-thing-for-drones-matt-waite---mobile-me-you-2022",
    "title": "Talks",
    "section": "The NEXT THING for DRONES? | Matt Waite - Mobile Me & You 2022",
    "text": "The NEXT THING for DRONES? | Matt Waite - Mobile Me & You 2022"
  },
  {
    "objectID": "talks.html#reporters-robotic-replacements-matt-waite-at-tedxpoynterinstitute",
    "href": "talks.html#reporters-robotic-replacements-matt-waite-at-tedxpoynterinstitute",
    "title": "Talks",
    "section": "Reporter’s Robotic Replacements: Matt Waite at TEDxPoynterInstitute",
    "text": "Reporter’s Robotic Replacements: Matt Waite at TEDxPoynterInstitute"
  },
  {
    "objectID": "talks.html#calming-down-about-drones-matt-waite-tedxlincoln",
    "href": "talks.html#calming-down-about-drones-matt-waite-tedxlincoln",
    "title": "Talks",
    "section": "Calming Down About Drones | Matt Waite | TEDxLincoln",
    "text": "Calming Down About Drones | Matt Waite | TEDxLincoln"
  },
  {
    "objectID": "talks.html#newsgeist-2014-matt-waite",
    "href": "talks.html#newsgeist-2014-matt-waite",
    "title": "Talks",
    "section": "Newsgeist 2014 Matt Waite",
    "text": "Newsgeist 2014 Matt Waite"
  },
  {
    "objectID": "talks.html#mobilemeyou-conference-2018---matt-waite",
    "href": "talks.html#mobilemeyou-conference-2018---matt-waite",
    "title": "Talks",
    "section": "MobileMe&You Conference 2018 - Matt Waite",
    "text": "MobileMe&You Conference 2018 - Matt Waite"
  },
  {
    "objectID": "talks.html#drone-journalism-demo-and-overview-journalism-interactive-2014",
    "href": "talks.html#drone-journalism-demo-and-overview-journalism-interactive-2014",
    "title": "Talks",
    "section": "Drone Journalism Demo and Overview: Journalism Interactive 2014",
    "text": "Drone Journalism Demo and Overview: Journalism Interactive 2014"
  },
  {
    "objectID": "posts/2012-05-16-help-me-plan-a-hacker-spacedrone-lab/index.html",
    "href": "posts/2012-05-16-help-me-plan-a-hacker-spacedrone-lab/index.html",
    "title": "Help me plan a hacker space/drone lab",
    "section": "",
    "text": "I’ve got an opportunity to build a hacker/maker space + drone lab in a journalism college. I’ve been asked to come up with requirements for the room. Square footage, furniture, gear, storage, you name it. \nDoes your university have a hacker space? What does it look like? Are there pictures of it online? Got a URL? \nHere’s what I’m thinking, broadly. Not going to get all this, but you don’t get if you don’t ask:\n\nCountertop space for working on drones or Arduino projects.\nLocking storage for the same.\nCouches? Long desks and chairs (like this)? Something different (like this)? Combinations? Need seating space for hacker/software projects.\nProjector for screen sharing or demos.\nInternet enabled TV on the wall.\nWhiteboards. Whiteboards everywhere!\nIf possible, high ceilings and a netted cage for indoor multi-copter testing (example)\n\nHere’s a Storify of tweets I got asking this question on Twitter.\n\nWhat am I missing? What would you include?"
  },
  {
    "objectID": "posts/2012-12-05-drones-soldering-irons-micro-controllers/index.html",
    "href": "posts/2012-12-05-drones-soldering-irons-micro-controllers/index.html",
    "title": "Drones Soldering Irons Micro Controllers",
    "section": "",
    "text": "Drones, soldering irons, micro controllers, multimeters, code … I reject your notions of what a journalism education must be. How journalists gather information, how it is collected, how it is stored, how it is processed, how it is analyzed are all open for wild experimentation. And there has never been a better time."
  },
  {
    "objectID": "posts/2013-01-28-writing-stories-with-code-part-2-conditional-leads-from-trends/index.html",
    "href": "posts/2013-01-28-writing-stories-with-code-part-2-conditional-leads-from-trends/index.html",
    "title": "Writing stories with code, part 2: conditional leads from trends",
    "section": "",
    "text": "See part one here. Get the code, such as it is, here.\nWhen we last left off, we had a script that would loop through a list of data and write a news lead out of it. All that the script did was look at two numbers and decide if the crime rate went up or down and then wrote an appropriate sentence. Something like this:\n\nLexington police reported more violent crime in 2010 compared to 2009, according to federal statistics.\n\nBut, sometimes, just one year isn’t enough. Sometimes a city gets on a roll, crime goes down for several years in a row, and that’s noteworthy enough to change the lead. So, lets do that. How?\nWell, not to continue a trend here, but’s it’s really simple. \nHere’s the code:\n    # determine the duration of the trend\n    if city[4] &gt; city[3] &gt; city[2]:         trend_length_clause = “, the second year in a row crime has increased”     elif city[4] &lt; city[3] &lt; city[2]:         trend_length_clause = “, the second year in a row crime has declined”     else:         trend_length_clause = ““\nWhat does that say? It’s simple greater than, less than logic. If 2010 (or city[4] in our loop of data) is greater than 2009, which is greater than 2008, then crime has gone up two straight years. If you flip the sign, you get that crime has declined two straight years. So, if those conditions are true, then lets set a variable called trend_length_clause to some words that work in our lead. Then, add that to our lead generating code:\n    lead = “%s police reported %s violent crime in 2010 compared to 2009%s, according to federal statistics.” % (clean_city, direction, trend_length_clause) \nOur lead now says insert the city where the first %s is, the direction of the trend where the next %s is and finally jam that clause in. Since we set it to blank if crime didn’t go up or down for two consecutive years, jamming it in there will do nothing if those trends don’t exist. When you run the script, now you see leads like this:\n\nLincoln police reported more violent crime in 2010 compared to 2009, according to federal statistics.\n\nNorfolk police reported more violent crime in 2010 compared to 2009, the second year in a row crime has increased, according to federal statistics.\nThat’s a little better. Still not award winning, but at least it varies it up based on the news. \nLet’s call it a lead for now. We need a second paragraph, one that starts putting some numbers to this trend. So that’s what we’ll do in the next post. We’ll write a second paragraph that does some percent change math and spells out the data a little further."
  },
  {
    "objectID": "posts/scrollytelling-nebraskas-season/index.html",
    "href": "posts/scrollytelling-nebraskas-season/index.html",
    "title": "Nebraska’s season long slide on offense",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(cfbfastR)\nlibrary(zoo)\n\nplays &lt;- load_cfb_pbp(2024)\n\nnebrollingepa &lt;- plays |&gt; \n  filter(pos_team == \"Nebraska\") |&gt; \n  mutate(sequential_play_number = row_number()) |&gt;\n  select(pos_team, def_pos_team, sequential_play_number, EPA) |&gt; \n  na.omit() |&gt; \n  mutate(cumulative_epa_mean = cummean(EPA))\n\nindrollingepa &lt;- plays |&gt; \n  filter(pos_team == \"Indiana\") |&gt; \n  mutate(sequential_play_number = row_number()) |&gt;\n  select(pos_team, def_pos_team, sequential_play_number, EPA) |&gt; \n  na.omit() |&gt; \n  mutate(cumulative_epa_mean = cummean(EPA)) \n\ncoloradogame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Colorado\")\nindianagame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Indiana\")\nohiostategame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"Ohio State\")\nuclagame &lt;- nebrollingepa |&gt; filter(def_pos_team == \"UCLA\")\n\n\nIf you’ve been following Nebraska’s football season, you’d be forgiven if you thought it started out great and has slowly driven into a ditch as time went on. Even after a loss against Illinois in overtime, hope remained high.\nThat is, until the Indiana game. A close loss at Ohio State revived some of that hope, and then a loss to UCLA crushed it.\nSo what’s happening?\nThe answer? The offense is slowly getting worse as the season goes on. That almost certainly has something to do with Nebraska hiring Dana Holgerson as an offensive consultant. How bad is it? The team is approaching the point that the average offensive play doesn’t add anything to the final score.\nWhat do I mean by that? There is a metric called Expected Points Added, and every play gets a score. Big plays have higher scores – 4, 5 even 6 points. Your two-yards-and-cloud-of-dust run from your own 15 will get you less than 1 point.\nAverage them together over the course of an entire season and you can get a sense of an offense. And, at this point of the season, Nebraska needs 10 plays to score a point. Indiana, the surprise team of the season, needs 5.\nHere’s a closer look.\n\n\n\n\nThis is what Nebraska’s offensive season looks like overall. If you’ve never seen a EPA average chart, the first thing you need to do is ignore the first game. There’s just not enough data and the average flies around. Where it starts to reveal who you are is later in the season, when more data comes in.\n\n\n\n\nWhere you want to start focusing is the Colorado game. You can see the glorious first half were the game got out of hand, and the dud of a second half where Nebraska’s offense went to sleep.\n\n\n\n\nNow the disaster at Indiana. Nebraska had 300 yards of offense that day … but that is buried by the 5 turnovers, not to mention going 0-4 on fourth down.\n\n\n\n\nNo one expected Nebraska to give Ohio State all it wanted. Vegas thought Nebraska would lose by at least three touchdowns. The offense played better than it had – you can see the line move up a bit – but alas, still a loss.\n\n\n\n\nUCLA was a tale of two halves. The first? Bad. The second? Better, and you can see the season average line curve up for that second half.\n\n\n\n\nNow. Want to be sad? Here’s what Indiana’s season looks like. The Hoosiers are remarkably consistent.\n\n\n\n\nWhen compared to Nebraska, Indiana started hot and has stayed hot. Nebraska started hot and … didn’t."
  },
  {
    "objectID": "posts/2014-11-20-a-classroom-experiment-in-twitter-bots-and-creativity/index.html",
    "href": "posts/2014-11-20-a-classroom-experiment-in-twitter-bots-and-creativity/index.html",
    "title": "A classroom experiment in Twitter Bots and creativity",
    "section": "",
    "text": "This semester, I’m teaching a class in Story Bots, which is really a programming class disguised as a journalism class. I’m teaching students enough programming that they can automate journalistically useful things, things like scrape a website and alert you to a potential story. Or take a dataset and turn each record into a journalistic narrative. \nOne of the assignments was to make a Twitter bot. They could do what they wanted with it, but it had to use Twitter and had to run on a simple cron job. It had to tweet, and they had to put their bot on Github when they were done (sans access keys). \nHere’s what they came up with.\nLady Plath by Sara Janak \nLady Plath takes a line from a Sylvia Plath poem and a line of Lady Gaga lyrics and randomly combines them into gloriously weird tweets. Example:\n\nHearted gambling-game you warily practice I’m in love with Judas\n\n— Lady Plath (@sylviagagaplath)\n\nNovember 18, 2014\n\n\nCode here.\nNebraska UFOs by Jeff Renken\nThis bot goes to the National UFO Reporting Center and checks for new reports in Nebraska. If it finds one, it tweets it out. Example:\n\n10/30/04 Lincoln, Two red lights. It was like they were on each end of a bar and just moving slowly from the east… http://t.co/Ud7LNhijhp\n\n— Nebraska UFOs (@NebraskaUFOs)\n\nOctober 30, 2014\n\n\nCode here.\nRandom Homer Simpson Quote bot by Jordan Tate\nA select few people who hashtag something #HomerSimpson will get a randomly chosen quote response from the Random Homer Simpson Quote bot. Does what it says on the tin.\n\n@lucioolima What do we need a psychiatrist for? We know our kid is nuts.\n\n— Random Homer QUOTES: (@RNDM_HomerSays)\n\nNovember 20, 2014\n\n\nCode here.\nTarantino ebooks by Tony Papousek\nThis bot does two things: uses Markov chains to write a new Tarantino script, 140 characters at a time, and responds to anyone who tweets “English motherf***er, do you speak it” with a simple “What?” (If you haven’t seen Pulp Fiction, shame on you). \n\nAs he tosses his other glove out the register,“ and they don’t lift a fuckin’ joke man! She’s supposed to be much of a bag of water\n\n— Tarantino Ebooks (@tarantinoebooks)\n\nNovember 20, 2014\n\n\nCode here.\nRebel_ebooks by Spencer Hansen\nExcited for the new Star Wars movie? Read lines of dialogue before they appear on screen though the miracles of Markov chains of the Empire Strikes Back script (the best movie, FYI). \n\nFeel the flow\n\n— rebel_ebooks (@rebel_ebooks)\n\nNovember 3, 2014\n\n\nCode here.\nMatt Waite ebooks by Daniel Wheaton\nI like to think I run my classes a little different from most other university courses and it’s conversations like this that give me hope it’s true:\nDaniel: “Can I have your complete Twitter archive?”\nMatt: “Uh, why?”\nDaniel: “I don’t want to say.”\nMatt: “Sure, why not.”\nAnd then this happens.\n\nCon: good luck getting those songs out of math\n\n— MattWaitEbooks (@MattWaitEbooks)\n\nNovember 20, 2014\n\n\nCode here.\nIn the interests of shared suffering, I made my own. It’s pretty tame by comparison. \nLNK Lost Dog Bot by Matt Waite\nThe City of Lincoln has a Lost Pets page, where they put reports of lost animals. I scrape the dogs and tweet out the new ones with a number to call if you’ve seen them. Simple.\n\nA chocolate Labrador Retriever went missing near 7400 Blk S 95 Ct on Nov. 19. If you see ’em, please call Animal Control at 402-441-7900.\n\n— LNK Missing Dog Bot (@LNKdogbot)\n\nNovember 20, 2014\n\n\nWith all this grading to do, I haven’t put my code up."
  },
  {
    "objectID": "posts/2011-10-17-help-me-refine-an-incomplete-idea/index.html",
    "href": "posts/2011-10-17-help-me-refine-an-incomplete-idea/index.html",
    "title": "Help me refine an incomplete idea",
    "section": "",
    "text": "The McCormick Foundation and the Poynter Institute for Media Studies are funding specialized reporting institutes in 2012 and are taking applications now. I saw this and had an idea for one, but I’m not convinced it’s fully baked. Help me out by adding your suggestions in the comments below.\nThe idea (the short version):\nReporting for News Apps: Getting, Cleaning, Vetting, Analyzing and Visualizing Data to Tell Stories on the Web.\nThe slightly longer version:\nDone right, news applications require a combination of skills, from investigative reporting to data literacy to information design principles to programming. A reporter working on a news app could face open records challenges, dirty data, questions about validity and accuracy, the formation of analytical approaches and the need to know when geographic data should be a map or not. And that’s before the first line of code hits the internet. There’s a lot to learn – and a lot to learn from. This specialized reporting institute would focus on the challenges specific to reporting for news apps, how the steps can be improved, how other fields within and outside of journalism have tackled these problems and how the results of the reporting-for-news-apps process can be extended to other parts of journalism. \nLimitations\n\nMcCormick/Poynter have specific goals they wish to fulfill with their conferences. Full list here.\nSpecifically, they want the institute to focus on a topic. The first topic that came to my mind was education data, specifically school test score data that every state has and many news organizations produce apps around. Thoughts?\nThey place an emphasis on the diversity of the conference, in type of journalist who attends (e.g. broadcast, print, online), type of outlet (newspaper, ethnic media, independent) and the speakers. Technology conferences have a well-earned “white dude” problem. Thoughts on how we reach beyond the core of nerdy dudes who do this now and beyond the proto-nerd who might be interested in attending?\n\nI think this is solid idea for a conference, but I’m not sure it’s fully baked. So, instead of me sitting here wondering what else I should put in the application I’m going to submit by the deadline of Nov. 15, I’m going to ask you. What else should I put in there?"
  },
  {
    "objectID": "posts/2011-11-15-thinking-out-loud-the-management-wisdom-of-battlestar-galactica/index.html",
    "href": "posts/2011-11-15-thinking-out-loud-the-management-wisdom-of-battlestar-galactica/index.html",
    "title": "Thinking out loud: The management wisdom of Battlestar Galactica",
    "section": "",
    "text": "I’m going to News Foo in Phoenix in a few weeks, and I’m thinking of proposing an Ignite talk there called the Management Wisdom of Battlestar Galactica. I’m a huge fan of the re-imagined Battlestar Galactica series that was on Syfy. Besides having lead a rag-tag fleet of the last humans left alive in a Cylon holocaust, I think Admiral William Adama would have been a pretty decent project manager. Here’s what I’ve come up with as project management wisdom from the Admiral and the show:\n\n\nSine Qua Non. Means “without which not” or the must-have thing that makes everything else possible. I’ve argued that a good news app focuses on one thing and does it really well and that if you can’t say what that one thing is clearly, you don’t have a project.\n“It’s not enough to survive … One has to be worthy of survival.” If you want to do something special, you have to commit to it. This is as much about project groups as it is projects. It can be summed up thusly: Clock punchers should GTFO.\n“Then grab your gun and bring in the cat.” If you believe in what you are doing, then come prepared to fight for it. Stand your ground. Make an argument. Fight to make your project better. \n“Sooner or later, the day comes when you can’t hide from the things that you’ve done anymore.” There’s getting the job done and getting the job done right. Any corners you cut now will come back to you. Get it done, get it online, get people using it, iterate often, but know your technical debt will come due one day. \n“Sometimes you have to roll a hard six.” A hard six is two threes on a pair of six-sided dice in the game of craps. It’s one of the highest odds rolls in the game, but with higher risk comes higher reward. If you aren’t gambling big, why are you playing?\n\n\nThoughts? I kinda dig the SciFi movie explains Thing Not Related To Science Fiction genre of lightning talk. I’ve given a talk called The Matrix Explains the Current News Business several times and it works pretty well. Not sure about this one. Let me know if you have a thought.\nMy other idea? What can the Zen Buddhist concept of the Beginner’s Mind tell us about the future of journalism."
  },
  {
    "objectID": "posts/2012-01-04-data-journalism-class-description-your-thoughts/index.html",
    "href": "posts/2012-01-04-data-journalism-class-description-your-thoughts/index.html",
    "title": "Data journalism class description: your thoughts?",
    "section": "",
    "text": "I’m teaching a data journalism/investigative reporting class for the first time this spring. I’ve got the class pretty well mapped out – I know what I’m going to teach – but I’m struggling with a course description. Here’s what I’ve got. Fellow data nerds, what say you?\n\nEvery day, more of our lives is becoming digital and more of that life is getting stored in a database somewhere. With a historic explosion of data about everything going on right now, reporters need the skills to analyze and understand data to then write the stories hidden in the information. Gone are the days when a reporter could grab a notebook and say “I suck at math.” If that’s what you aspire to, leave now. Don’t want to be that reporter? Welcome. Data journalism harnesses the tools of the data analyst and uses them to do investigative reporting. We’re going to get our hands dirty with spreadsheets, databases, maps, some basic stats and, time permitting, some stuff I’ll call “serious future s**t.” And in the end, we’ve got a project to produce. So buckle up and hold on."
  },
  {
    "objectID": "posts/2007-09-11-why-the-journalist-in-programmerjournalist-matters/index.html",
    "href": "posts/2007-09-11-why-the-journalist-in-programmerjournalist-matters/index.html",
    "title": "Why the journalist in programmer/journalist matters",
    "section": "",
    "text": "In a comment, Ben asked how PolitiFact went from idea to PolitiFact.\n\nHow did that get refined into the site we see today? Was the content and feature refinement mostly the work of web people or people from the print newsroom? Any lessons learned to help the rest of us help our editors and reportorial colleges see the new dimensions web apps can bring to conventional content?\n\nThe content and feature refinement, at least at first, was the work of Bill Adair and I almost exclusively. After we first talked over the idea, I sketched out how I thought the database should be laid out based on Bill’s vision in Django. Arguably the best thing about Django is that out of the box it has a fantastic administration tool. The admin tool let me show Bill what I had done and let him enter in a few records into the database. Doing that, he’d have more ideas, or I’d think of something I wanted to put in, and I’d add them to the model code. After we got pretty happy with what we had, we split up: Bill started reporting out and writing a few items and I started building the views — Django’s query layer that takes data from the database and sends it to the templates for display. From this, both of us came up with new things we wanted and old things that didn’t work right. Out of all this work came the demo, which brought the decision to go forward, which brought on board our IT staff and a web designer, plus more input from both web and newsroom colleagues.\nThere’s a couple of lessons I see in our experience that may or may not work for you:\n\nKeep the number of people initially involved as absolutely small as possible.\nIdeally, the people involved at first should be the one with the vision and someone who can translate that vision.\nRamp up staff slowly and minimally. More people means less decisiveness.\nRuthlessly compartmentalize. Let designers design, reporters report, etc.\nAccept that you’ll never create the perfect app, so decide quickly what’s worth fixing, what’s not and what belongs in version 2.0.\n\nAnd that brings me to the title of this post. One of the key parts of developing PolitiFact was the ability to translate a standard style of news story into something bigger, broader, relational. I’m not saying that a skilled programmer can’t do this, but in my experience, this is where the journalist part of the term “programmer/journalist” has the most value.\nIf you’ve been a reporter, hopefully you’ve been paying attention to how people react to your stories, what information readers remembered, how readers approached stories. Hopefully, you’ve paid attention to the weaknesses of how you’ve approached some stories in the past. Also, as a reporter, hopefully you’ve paid attention to the parts of your stories that repeat, over and over and over again. Names, places, even the structure of certain story types.\nIf you have paid attention, it becomes a matter of creativity to translate that reporting experience into a web application. You rely on your experience writing those stories to figure out what parts you need in the database — a byline field, a headline field, a blurb field, a body text field, etc. Having written these stories before, you know what parts repeat — candidate name, candidate party, candidate home state. Having talked to readers, you know some of them only want to know about the candidate they care about, so you set up one-stop-shopping pages for each candidate. You know some readers are single issue people, so you set up and issues database and pages for everything you have on a single issue. You know some readers are political party people, so you do the same things for party that you did for issues. Now, I’m using PolitiFact as the guide here, but you can apply this to lots of different stories, ideas, databases, applications.\nThe reason I think the journalist/programmer idea is so interesting is that it’s the journalist part who can put themselves into the position of reporter writing the content and reader consuming the content, and it’s the programmer part who can translate all that into a web application, taking content in one side and displaying it on the other side in as many ways as you think readers would want.\nA lot of journalists I’ve talked to lately are all dazzled by the programming part of this equation. Don’t dismiss so quickly the importance of your journalism experience. If you’ve been paying attention to what you’ve been doing, that experience has prepared you well to create web apps."
  },
  {
    "objectID": "posts/2012-03-14-toward-a-solution-to-the-more-tech-in-j-school-problem/index.html",
    "href": "posts/2012-03-14-toward-a-solution-to-the-more-tech-in-j-school-problem/index.html",
    "title": "Toward a solution to the more tech in J-school problem",
    "section": "",
    "text": "First, lets state some general conditions and agree to them:\n\n\nThere is a generally agreed upon need for tomorrow’s journalists – no matter what area of the craft they intend to go into – to have more technological skill and experience than their past counterparts.\nThere is not a generally agreed upon way to accomplish this increase in technological skill within a j-school curriculum.\nThere is not a generally agreed upon list of tech skills that journalism students should/must have before graduating to become a journalist today.\nThere is a … tension … in newsrooms and faculties over the balance between focusing on reporting/storytelling/fundamental skills and on new technologies for storytelling. Whether you believe this tension is well founded or a not-mutually-exclusive waste of time, it exists and must be acknowledged.\nThere are a finite number of classes a student can take in any major and the list of skills and tech that have come up at various discussions and conferences vastly outstrips this class hour limit.\n\n\nI’ve been thinking about this a lot lately. It started with me thinking about what is the technological/computer science answer to Doing More With Less. Ah yes, Doing More With Less, the trite bullshit line publishers trot out after axing half the staff. It is bullshit, but there’s also a legit question there. How could you do more with less, because guess what? There’s less. A lot less. Ignoring that is angrily yelling at clouds. Pointless.\nIn academia, the do more with less argument isn’t necessarily about less. It’s about an absolute limit. There are only so many credit hours a student can take in their major. In that limit you have to teach, you know, Journalism. Reporting, writing, editing, design, photography, videography, “the basics.” You don’t hear from people anymore arguing that these classes shouldn’t include more tech. For example, traditional print reporting classes are using digital recorders and cameras on smart phones and video cameras and live tweeting to cover stories. J-schools, by and large, have made that adjustment, from what I hear from my colleagues around the country. The classroom reporting classes are trying to mimic the professional experience, and that has meant more tech.\nThe question, then, is how do you get some of these other emerging ideas into the curriculum? Where does programming fit in? Or mobile design? What about completely out-there ideas that should be getting a test run in a university setting before going out into the broader industry? Like drones? Or sensor networks? Or machine learning? Or algorithm-written stories? How do you fit those into a curriculum many believe is busy enough teaching “the basics”? How do you deal with a not-insignificant number of people who don’t believe any of this should be in the curriculum at all?\nMy thinking lately?\nDon’t. \nJust don’t. Forget the curriculum. Forget classes. Do something outside of class. Create an opportunity that doesn’t have credit hours attached that has value and fire away.\nWhat do I mean by this?\nIf you’ve been paying attention around these parts lately, you’ll know that I’ve started a Drone Journalism Lab and have just entered the Knight News Challenge with an idea of building sensor networks for news. I get asked regularly, are you going to teach a class in drones or sensors?\nNope.\nCouple of reasons for that. First and foremost, I want to be clear that both are just tools. A smartphone is a really great tool for storytelling that opens all kinds of ethical questions about their use. Would you teach a class in smartphones? No, you wouldn’t. That’d be silly. So using a drone to report a story is … slightly more complicated … but similar to using a smartphone to report a story. It’s a tool to tell a story. It’s not a completely new form of story. Might drones end up in other classes? Yep. Count on it. Sensors? Yep, count on it. Are they their own classes? I don’t think so. I can make an argument for them being their own class, but for now I’m saying no.\nSo what are they? \nEvery campus has student media where students take what they learned in class and apply it to doing more journalism. They get more experience. In sports, they’re called repetitions or reps. The more reps you get, the better you get, the more you can do. Students need reps.  But J-school isn’t the only place for this. Other disciplines have students who work in labs as lab techs or research assistants. They get paid, they get experience, they get reps. \nSo what if we did this for journalism too?\nI called it the Drone Journalism Lab for a reason. It’s going to be a lab. Where students work. And through grants and undergraduate research programs, they’ll get paid to do so.  They’ll get hands on experience with a very interesting technology, get to take part in the research and they’ll get to put it on their resumes. And given the number of news organizations that have asked me about it, they’ll have no trouble finding a job if this all goes according to plan. Number of classes required? Zero.\nThe Knight News Challenge app? A good portion of the funds requested is to pay students to work on the project. Again, hands on, in the lab, taking part, without course credit on the line. No need for a curriculum committee meeting. No need to fret about what basic skill is going to get short shrift because we’re talking about sensors.\nI’ve talked about starting a hacker space here in the college where students could hang out and build stuff. A kind of proto-incubator stage space where students build things they can then take and build a business around. At the time I was thinking about web stuff or Kinect hacks or things like that. But now I’m thinking it could be a whole universe of things. And again, not things in the curriculum.\nI have no idea if this is a solution or not, but it seems to solve a lot of problems. It creates some problems – space, funding, resources to name a few, plus how do you measure this for faculty time commitments? I’m sure it causes more problems than that. I just haven’t thought of them yet.\nBut if you believe J-school needs more tech, and you accept the reality that universities aren’t going to allow us to double the number of credit hours required to get a degree, then these kinds of outside-the-curriculum ideas are part of the future."
  },
  {
    "objectID": "posts/2011-10-03-news-nerd-rage-the-trouble-with-conferences/index.html",
    "href": "posts/2011-10-03-news-nerd-rage-the-trouble-with-conferences/index.html",
    "title": "News nerd rage: the trouble with conferences",
    "section": "",
    "text": "I’ve been having somewhat of an existential crisis of late. I’ve been a speaker at multiple journalism conferences a year for more than a decade running now and I have started to wonder at the value of all that talking. Not that I feel all those sessions weren’t valuable – they were – but were they as valuable as they could have been? Could we have gotten a few more people over the wall? Could we have snagged a few more minds? \nMichelle Minkoff has been thinking about this too, and asked me for some advice. You should head over and read her post. It’s what got me off the carpet to write what’s rattling around in my head.\nEvery journalism conference I’ve been to are 90 percent panel discussions, maybe some hands-on classes and the remainder keynote speeches with a Big Name Speaker. How each conference selects speakers varies – some identify people who would be interesting on a given topic and invite them, others invite speakers to propose panels. \nWhat I’ve come to believe:\n\n\nThe best panels are about things anyone can do, not about some crazy thing you did that no one will ever be able to do or only works because of some cosmic combination of factors in your city or state.\nPanels are a really, really crappy forum for very specific technical information. \nThe best panels entertain and inspire as much as they inform. They tell a story, from speaker to speaker to speaker. There is laughter and people leave the room ready to run through a brick wall. More tent revival, less reading bullet points. \n\n\nThe problem with this line of thinking:\n\n\nNot everyone can inspire or entertain. There is a legit “diversity of voices” argument to be had about journalism conference panel speakers and it goes beyond the YAFWG (yet another fking white guy) argument. Being a good speaker is a really good skill to have** but it shouldn’t eliminate you from the speakers list. \nWe’ve shown people The Light! They’re ready to go out and do what you inspired them to do! Congrats. You’ve rubbed them on the carpet, sent them running out the door to … what? A newsroom that doesn’t care? A life of blindly Googling for help? There are people who take this challenge and thrive. There are far more people who are confronted with the vastness of the unknown and quit.\n\n\nSo, what can we do about this?\nI think we have to rethink the News Nerd/technical journalism panel. We have to quit fooling ourselves that an hour at a conference is sufficient to do anything more than hold someone’s attention for an hour. We have to stop inspiring people and then giving them little direction after they walk out.\nIn Michelle’s post, Jeremy Bowers at the Washington Post sums it up thusly:\n\n\n\n\n“There’s missing support for the middle-class of news developers. This is a particularly glaring gap, because it’s the most difficult part of the incubation of the adolescent coder.”\n\nI think I have an idea that might work.\nProposed: The Super Panel. It goes like this.\n\n\nWe start with a panel designed specifically around inspiring, entertaining and informing. We make no secret of this – we celebrate it, in fact. We choose speakers specifically because they inspire and entertain.\nAfter the panel, those so moved are invited to an unconference-style session where the people who want to go further are thrown together with the panel’s speakers and others recruited to help to map out the next moves. Install some software? Map out a group project? Start hacking away? Up to those who show up.\nAfter the conference, the super panel speakers plus those recruited to help agree to run a study group/mentoring program online. Maybe through Google Groups. Maybe something else. I don’t know. Haven’t thought this all the way out. The point being there’s a support group of people who were at this session who are working on a project together to learn after the conference is over. There’s infrastructure and support in place after you leave.\n\n\nI realize this completely changes the dynamic for panel speakers: Done right, there’s a lot of work that goes into preparing for a panel. This adds to that 100 times over. I also realize also that not every panel and not every panelist is cut out for this. But it seems like every conference could do a handful of these Super Panels in addition to the normal panels. Nobody has time to start some online study group based on every panel they went to at a three day conference. But one? Two? If they really want it they will make time. \nI think Super Panels would serve several purposes: First, it would take away the argument that conferences leave people high and dry and all alone after they’re over. Second, it would leave someone who claims that they want to learn this stuff with a lot fewer excuses. Third, it would much better serve this middle class of news nerd who is already in the choir, enjoys but doesn’t need the sermon and wants the technical stuff that just doesn’t come across well in a panel. \nThoughts? Improvements?\nEdit: See this if you think 1) I’m being specifically critical of any one organization and 2) you think I’m just writing this to write it instead of doing something about it."
  },
  {
    "objectID": "posts/2025-11-24-parsing-pdfs-with-antigravity/index.html",
    "href": "posts/2025-11-24-parsing-pdfs-with-antigravity/index.html",
    "title": "Parsing PDFs with Antigravity",
    "section": "",
    "text": "Among the wisest things I’ve ever seen written about AI is “I want AI to do my laundry and dishes so that I can do art and writing, not for AI to do my art and writing so that I can do my laundry and dishes.”\nThe same can be said for journalism. I want AI to do the chores so I can do the journalism. Time I’m not manually pulling apart PDFs is time I can spend talking to people.\nLast week, Google launched their much anticipated Gemini 3 model, and much is being said and written about it. One very interesting thing they did was launch a Visual Studio Code version of their own called Antigravity. It’s a development environment with an “agentic coding surface” added as one of the primary interfaces to it.\nI’ll be honest, when I first read about it, I was pretty meh. So it’s Claude Code (which you can plug into Visual Studio Code!) or Open AI’s Codex, but for Google this time. Okay. Fine. But I started seeing some overreaction to it online, and it made me curious.\nI’m ready to say we may be looking at a truly impactful data journalism tool here. I don’t want to fall into the same trap and overreact by saying it’s as big as the spreadsheet or the search engine, but I’m also not saying it isn’t.\nIn short, it has astonishing potential as a data journalism tool.\nWhat has me so excited? My employer’s salary “database” which is a 1,957 page PDF. A PDF formatted in such a way as to make parsing it a practical impossibility. And before you ask: state law says if they “publish” data in this form, then they don’t have to give it to you in a different form. Many of us have asked.\nWhat makes it so hard are people who get paid out of multiple budget pots. Take, for example, me. I have one job according to the university: professor of practice. I get paid out of one account. My entry in the pdf is one line. Easy. Colleagues of mine might have multiple jobs. Some administrators in my college are half administrators (pot one), half professors (pot two). But they also have endowed positions, so they get paid from a third pot. In the PDF, they’re on four lines. Pots one, two and three and a fourth that is the total. But only the first line gets all of the data. The rest? Blank.\n\n\n\nThree people. Three salaries. Eight lines of data.\n\n\nNotice also that names and where they get paid from blur together. Notice how the length of the alternate funds also overlap the name and position columns. There is no reason this data needs to look like this, but the university considers this as being responsive to a public records request. Want to analyze this data? Want to compare it across time? Compare administrators to faculty? Good luck.\nThat is, until Gemini 3 and Antigravity came along.\nAfter an afternoon of messing around with Antigravity to fiddle with the design of this website, I decided to just try something. I had been messing with DeepseekOCR, an open weights model that you can run on your own hardware that is very good at finding tables in PDFs and converting those to markdown tables. I was very impressed. But I wondered how well Antigravity/Gemini 3 would do with this pdf.\nAnswer: Gobsmacked. Gob. Smacked.\nI put the PDF and a screenshot of the first page in a folder, connected that folder to Antigravity, and wrote this half-assed prompt from the couch.\n\nI am attempting to extract structured data from a frustratingly formatted PDF. What I need at the end is a csv file that has the data contained in the screenshot. I can handle the intricacies of the data after I get the structured version. Can you take a look at the screenshot first to see if you can extract the tabular structure?\n\nI didn’t even ask it to do the whole PDF. I just wanted the screenshot of the first page. That’s it. What it did was devise an implementation plan, wrote a walkthrough of what it did as it was doing it, then wrote a Python script using pdfplumber that extracted the data out of all 1,957 pages, and then wrote a cleaner script to fix some formatting weirdness. It took my prompt, worked for about 10 minutes and spit out a csv file that was orders of magnitude better than anything I had managed myself in years of on-and-off messing with this file.\nAll I did was stare at it as it kept trying things and checking them, improving the code using random selections of data to check if it was all working. And then, it finished. I couldn’t believe it worked, so I opened it and was blown away with what it did.\nIt wasn’t perfect, however. In fact, it assumed that those extra rows where people got paid from other pots were a mistake and it filtered them out.\nSo I went back to the prompt:\n\nI’ve been doing some of my own spot checking and there is a basic assumption at the beginning that is not correct. That assumption is lines 34-38 of the clean_salary_data.py. Specifically:\n# Basic check: First column should be Cost Element (6 digits) # OR sometimes it’s empty if it’s a continuation? No, looking at the data, most data rows have it. # Let’s look at row 26: 512100,“Batman, Renee”,F,…\nfirst_col = row[0].strip()\nThere are entries where the next line, which does not contain a cost object *is* a continuation. This data is university salary data, and how they show professors with endowed chairs, for example, is to put their faculty job on one line with a cost object, then the next line without one is their endowed chair line and at the bottom is a total line for that person. Not capturing the next few lines is causing some issues with accuracy. Can we capture those? For the vast majority of data, your method works extraordinarily well. It’s just not working for the few who have multiple salary inputs.\n\nAbout 8 minutes later, I had an astonishingly good – much better though still not perfect - version of this data.\nWhat’s wrong with it?\nIt didn’t want to make assumptions about spacing, so it left odd spacing that is an artifact of the PDF. So some people are Ma tt Waite or Profe ssor. The overlapping columns are an issue I’m likely going to have to contend with manually. I’m going to have to fill in blank columns and total up people to get to one row one person.\n\n\n\nThe output from Antigravity.\n\n\nBut I can’t stress this enough: this is light years beyond any tool I’ve been able to throw at this in years of trying. Every NICAR, I throw some new tool at it and leave disappointed. This is the first time my gob has been smacked by something an AI is doing.\nCould I have written this code? Sure. I’ve even tried using pdfplumber to do it and didn’t have the same results. It would have taken me much longer, and frankly that’s probably enough to get me to go away. I’ve got papers to grade and students upset with me about how long it’s taking.\nCan’t say this enough: astonished at what this might mean for freeing journalists up to do journalism instead of un-screwing up government PDFs.\nWant to see all of the output? It’s on GitHub."
  },
  {
    "objectID": "posts/2008-01-04-data-ghettos/index.html",
    "href": "posts/2008-01-04-data-ghettos/index.html",
    "title": "Data ghettos",
    "section": "",
    "text": "Note: This post came from a version of this blog that got lost in a server failure. It’s been restored from old RSS feeds, Google caches and other sources. As such, the comments, links and associated media have been lost.\n\nOne resolution for this year: Post more often. Starting now.\nI’m not sold on the whole Data Desk/Data Center idea that a lot of newspaper websites are trying out. I hate to say all this because at a lot of places, the people responsible for them are my friends. But for all the love I have for putting data online, there’s something that has bothered me about the way they’re going about it. A friend summed it up for me recently: The Data Ghetto.\nThe Data Ghetto is that one mishmash page where all of that site’s databases are lumped together.\nI won’t take the time to criticize how these pages are constructed — the criticisms are obvious and even people who have made them don’t like them. But if you take a step into one of the databases and you get to my second problem with them: couple of search boxes and a button.\nIs that really it? Is that the big newspaper.com push into data? Sprawling, barely organized pages to get to a couple of search boxes and a button? This fails on a number of levels:\n\nCreativity: Can we offer no more creative way into the data than to make a user put stuff here and hit search? Search is fine in context, but it’s also limiting. What if someone doesn’t know how to spell something, or doesn’t know what they want, or all they want to do is explore the data their own way? You’ve cut that off. In my opinion, browsing is much better. If your data is normalized — i.e. all the cities are spelled the same way, etc. — then you can let people click on the things they’re interested in and get those things. And in the process, they may see other things they’re interested in. To see it in action, look at how PolitiFact is browsable (example here). Or better yet, if you don’t believe me, look at chicagocrime.org. There are 10 different ways to browse that data. There’s one search box. Search is a part of it — it has a value in context — but it shouldn’t be your whole app.\nRepeat customers: A lot has been written about the traffic these database sites get. But I want to see what the traffic is like months after it first goes up. What’s the traffic like after the third or fourth update. The reason I ask is because some of these search apps to me seem like a pure voyeur play. What I mean by that is the user sees a salary database, goes and looks up their neighbor and … what? They’re done. They’ve answered the one question they wanted to ask. How are you bringing people back to your data?\nShaky business model: Are we really building a business model, or even a component of a business model, around making public data searchable? Because guess what? Google is too. That’s right. The search giant is dealing directly with government agencies to help them make their own data searchable. Sound familiar? Think your data ghetto can compete with Google? Do you think people are going to remember your newspaper.com url over Google? Really?\n\nHere’s the to-be-fair portion of the post: I have exactly one data-driven app under my belt (PolitiFact). I have a half dozen more in the works, so I’m thinking about this stuff constantly. But for now, I can only talk. I can’t show, at least not yet.\nThat said, here’s how we can get out of the data ghetto: add some journalism to it.\nBack in November, Will Sullivan tried to coin a term where multimedia and data collided into something he, jokingly, called multimedata journalism. Of note was a New York Times effort where they did a story about people freed from prison by DNA evidence. They interviewed 137 of 200 people released. They then put an app online that allows you to click on each name and see details about each case (data) and hear their story (audio).\nI’d argue there doesn’t need to be a new term: This is what it’s supposed to be. Journalists are supposed to add context and value to information. Heaving databases online should be no different. Does each app require the type of effort that the NYT put in? No. But flatly serving up data with no context or analysis or value outside the record itself is hardly journalism. A public service maybe, but not journalism.\nNext post: Instead of bringing journalism to your app, why not bring your app to your journalism?"
  },
  {
    "objectID": "posts/2009-01-20-data-content-content-data/index.html",
    "href": "posts/2009-01-20-data-content-content-data/index.html",
    "title": "Data = Content: Content = Data",
    "section": "",
    "text": "Mark Potts had some nice things to say about the new version of PolitiFact that we recently launched. But one of the things he wrote I wanted to amplify:\n\nNone of this really looks like traditional journalism. The Obameter doesn’t follow conventional story formats in any way, and is really a hybrid between data, reporting, news and information presentation. We need to see a lot more of this. There are a many different ways to tell a story, especially online, and the more experimentation we see with journalism forms, the faster the state of the art will evolve and thrive.\n\nPolitiFact may not look like tradional journalism, but it very much is. It’s a story type that’s been around for decades, a type of accountability journalism that’s been around much longer than that. The difference is that we aren’t just creating a field for a headline and a field for a story and calling it quits. The difference is that we view content as data and the database as an act of journalism in itself. Each promise in the database is a piece of journalism and a piece of data. And all the acts of journalism that combine to make up the database form one meta act of journalism. But without using data as an organizing principle, most of what makes PolitiFact more than just a collection of stories would be impossible.\nI see too much “innovation” going on far away from the core product – Video! Twitter! Social Networks! – and not enough innovation happening at the atomic level of journalism – the story. Yeah, yeah, I know: your CMS sucks, it won’t do anything, blah blah blah. We’re going to ride that excuse right into the grave. Well, if newsroom content systems can’t do it, it’s time to start building new structures for stories and ripping that content right out of the hands of the CMS."
  },
  {
    "objectID": "posts/a-simple-example-ai-agents-doing-journalism/index.html",
    "href": "posts/a-simple-example-ai-agents-doing-journalism/index.html",
    "title": "A simple example of AI agents(?) doing journalism(?) work",
    "section": "",
    "text": "Let’s start this with some confessions:\n\nI’m at best an enthusiastic amateur with AI. I know more than most, and I know nothing in the grand scheme.\nExample: I’m not sure I have any idea of what an AI agent is. I think I do, but there’s so much marketing hype around them that I can’t know for sure. People much, much, much smarter than I am aren’t sure either.\nI teeter on the edge of two extremes. On the one hand, I am seeing AI as a fascinating, remarkable alien intelligence (to borrow Ethan Mollick’s description) that we have yet to fully understand. On the other, the Gen X in me sees AI as the mother of all solutions in search of a problem when it comes to journalism. All the so-called big problems AI “solves” in journalism – more content to sell ads against! – nobody wants.\n\nSomething I’ve been coming around to, though, is maybe there isn’t a Manhattan Project level world changing use case for AI in journalism. Maybe Chris Albon has the right of it, that the real value is AI saving a human an hour of work … millions of times a day.\nFor months I’ve been trying to think of some moon-shot idea to use AI to do … something, anything … big. And every thing I came up with would be terrible, destructive, or flat-out-insane to deploy without massive human investment, and then what would the point be?\nAnd then, randomly, one day a confluence of ideas popped into my head and what fell out is an example of AI agents(?) doing the work of journalism(?) that actually works.\nThe ideas that collided in my brain were:\n\nThis Francesco Marconi tweet from April that I think did the best job of laying out a vision for AI agents in journalism. At least it’s the one that made the most sense to me.\nThis R Package wrapping the Google Gemini API.\nGemini having a free tier to try some stuff out. You get 15 requests a minute – one every 4 seconds – and 1,500 a day.\nThe “aim small, miss small” mantra in teaching marksmanship.\n\n\nWhy R and not Python?\nThe honest truth is there is no good reason why I’m using R to do this vs Python, which most of the AI world is using. The reason is because I teach Data Journalism to undergrads using R at the Harvard of the Plains and believe strongly that I can take absolute beginners – people who can’t spell code – from zero to data analysis faster with the R and the Tidyverse than I can with Python and Pandas. So I have a hammer, this here looks like a nail, and so we’re doing this with R. But there’s absolutely nothing special about this code that you couldn’t match in Python.\nWhen I teach Data Journalism, I use population estimates from the US Census Bureau to teach students how to calculate percent change. That way, we can see which counties in the state grew the fastest and who shrank the fastest. It’s a story as old as time in the Great State of Nebraska. Rural areas are shrinking, urban areas are growing.\nAn extremely common thing to do with this data is to make a map. Here’s the 2022-2023 change map in Datawrapper for Nebraska.\n\n\nIf you click on a county, you’ll get a pop up box that gives you the county name and then the data. Population in 2023, population in 2022 and the percent change. It’s been done a million times before. It does the job.\nBut what if we could make it better? What if we had small narrative summaries for each county? An a headline for each one? Can I assign a human to do this? 100 percent I can. I have an army of undergrads and a gradebook to hold over them. I assure you, if I was cruel enough, we could do this for all 3,400 counties in the US.\nBut why do that when we can have AI do this in minutes instead of making humans miserable for hours?\n\n\nFeeding Gemini numbers, getting back narrative\nThe gemini.R package couldn’t make sending something to Gemini any more simple. The hardest part – which is not hard – is getting an API key from Google. How do you do that? Go here: https://makersuite.google.com/app/apikey. So I always have mine and don’t lose it, I set it as an environment variable. To do that, run usethis::edit_r_environ() and add GOOGLE_GEMINI_KEY=\"your key here\" to your environment, save that file and restart R Studio (or whatever IDE you use).\nYou can test it out with something like this:\n\nlibrary(tidyverse)\nlibrary(gemini.R)\nlibrary(glue)\n\nsetAPI(Sys.getenv(\"GOOGLE_GEMINI_KEY\"))\n\ngemini(\"Write me a haiku about the joy and sadness of being a Nebraska football fan\")\n\nWhat do you get back?\n\nRed and white, we cheer,  Hope springs eternal, then fades,  Another close loss.\n\nOuch.\nBut that’s really it. Just gemini(\"Words here\") and off it goes to an AI and back comes the results in plain text. So the first hard part is turning data into a text block we can send to Gemini. So I pull a dataset of Nebraska county population estimates, I’m going to thin the number of columns I’m working with first, then create the percent change column, create a GEOID column made up of the state and county FIPS number so I can join it to my map later, rank the counties by population change and then mash it all together into a single text blob called the base_narrative. It’s literally just Column Name: Number, Column Name: Number repeated over and over.\n\ncountypopchange &lt;- read_csv(\"https://the-art-of-data-journalism.github.io/tutorial-data/census-estimates/nebraska.csv\")\n\nstatenarrative &lt;- countypopchange |&gt; \n  select(COUNTY, STATE, CTYNAME, STNAME, POPESTIMATE2023, POPESTIMATE2022, NPOPCHG2023, NATURALCHG2023, NETMIG2023) |&gt;\n  mutate(POPPERCENTCHANGE = ((POPESTIMATE2023-POPESTIMATE2022)/POPESTIMATE2022)*100) |&gt; \n  mutate(GEOID = paste0(COUNTY, STATE)) |&gt; \n  arrange(desc(POPPERCENTCHANGE)) |&gt; \n  mutate(PCTCHANGERANK = row_number()) |&gt; \n  mutate(base_narrative = glue(\n  \"County: {CTYNAME}, Population in 2023: {POPESTIMATE2023}, Population in 2022: {POPESTIMATE2022}, Population change: {NPOPCHG2023}, Percent change: {POPPERCENTCHANGE}, Percent change rank in {STNAME}: {PCTCHANGERANK}, Natural change (births vs deaths): {NATURALCHG2023}, Net migration: {NETMIG2023}\")) \n\nWhat does a base_narrative look like?\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\nA real exciting read, no?\nNow we need to make an agent. It helped me to think of journalism as an assembly line. We have data as raw materials, and now we need to assign a worker to a process to convert raw material into something new. In any news process, the first worker is the journalist creating the thing. A reporter going to city hall. A photographer going to a breaking news event. So it makes sense that our first agent is the author of the narratives for each county.\nLike any good LLM prompt, we’re going to start by giving our author agent a role – a job to do. Our agent is a demographics journalist from Nebraska. We give that agent a task with some details – keep it short but approachable. And then we give it the data.\nBelow the role is a function that takes in a county name, finds the base_narrative for that county and then we merge together the author_role and the base_narrative when we send it to Gemini. We store the results in a variable called … results and do a little cleanup on it (Gemini likes to cram newline characters in the results). I’ve added a five second sleep between every county to keep from running afoul of Google’s free tier API limits. Theoretically I should be able to set it to four seconds, but I don’t need my account banned over a second. With those pauses, our author takes about eight minutes to write small narratives about each of the 93 counties.\n\nauthor_role &lt;- \"You are a demographics journalist from Nebraska. Your job today is to write short -- 2-3 sentence -- summaries of population estimates from the Census Bureau for each county in the state. I will provide you the name of the county and a series of population numbers for the county. Your job is to turn it into a concise but approachable summary of what happened in that county. Here is the data you have to work with: \"\n\nauthor_agent &lt;- function(county) {\n  county_narrative &lt;- statenarrative |&gt; filter(CTYNAME == county)\n  results &lt;- gemini(paste(author_role, county_narrative$base_narrative))\n  results &lt;- gsub(\"\\n\", \"\", results)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$CTYNAME))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$CTYNAME, base_narrative = county_narrative$base_narrative, capsule_narrative = results)\n}\n\n# Use map_df to directly create the final dataframe\nauthor_agent_results &lt;- purrr::map_df(statenarrative$CTYNAME, author_agent)\n\nWhen it’s done, we end up with a dataframe called author_agent_results which has the county name and the new narrative.\nWhat does it look like? Remember, we gave it this:\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\nAnd we got back:\n\nBanner County saw a significant population increase in 2023, growing by 17 people for a 2.6% jump, the highest rate of growth in the state. This growth was entirely due to an influx of new residents, as the county experienced no natural population change.\n\nWe could stop here, but why do that? Good journalism is often a layered process involving multiple sets of eyes reviewing the work along the way, and other skilled people adding to the product. So who would normally get this next? How about we fact check the work?\nExact same pattern, just a different role for the AI.\n\nfact_check_role &lt;- \"You are a fact-checking editor. Your job today is to compare the information in the base narrative to the capsule narrative in the data provided to you. You will compare each number in the base narrative to the capsule narrative to make sure they are the same, and then you will check the context of how each number was used in comparison to the original base narrative. To be clear: the base narrative is the correct information. When you are finished, return just a single word. If everything is correct, respond Correct. If any part of it is not correct, respond Incorrect. \"\n\nfact_check_agent &lt;- function(county_input) {\n  county_narrative &lt;- author_agent_results |&gt; filter(county == county_input)\n  input_string &lt;- paste(\n    fact_check_role,\n    \"Base narrative:\", county_narrative$base_narrative,\n    \"Capsule narrative:\", county_narrative$capsule_narrative)\n  results &lt;- gemini(input_string)\n  results &lt;- gsub(\"\\n\", \"\", results)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_input))\n\n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, fact_check_results = results)\n}\n\n# Use map_df to directly create the final dataframe\nfact_check_agent_results &lt;- purrr::map_df(author_agent_results$county, fact_check_agent)\n\nHonestly, this part needs work. It flags about 10 percent of the results as being incorrect, but they aren’t. The reason it flags them is the author agent was a little glib with the numbers – saying “a slight increase natural growth because of more births” without giving the numbers themselves. The fact checking editor bot here does not like that. So it might need a little tweaking to see if we can get the fact checker to lighten up a bit.\nQuit now? Nah. How about we add a little local flair to each capsule. Google knows a lot of stuff, so why not add some kind of geographic context to each county. To do that, I created a rewrite editor and commanded it to add details like the region of the state, the county seat, the largest city, or some other detail to a single clause in the original narrative.\n\nrewrite_role &lt;- \"You are a re-write editor. Your job is to add a little local geographic context to a demographic capsule about a county in Nebraska. You'll do this by adding a clause to the paragraph I'll provide you. That clause should tell you something about that county. Maybe the county seat, or the largest city, or the region of the state it is in. We just need a clause added to one of the sentences, and do not do anything to show where you added it. Here is the capsule: \"\n\nrewrite_agent &lt;- function(county_input) {\n  county_narrative &lt;- fact_check_agent_results |&gt; filter(county == county_input)\n  results &lt;- gemini(paste(rewrite_role, county_narrative$capsule_narrative))\n  results &lt;- gsub(\"**\", \"\", results, fixed = TRUE)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$county))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, rewrite_county_narrative = results)\n}\n\n# Use map_df to directly create the final dataframe\nrewrite_agent_results &lt;- purrr::map_df(fact_check_agent_results$county, rewrite_agent)\n\nWhat did it give us for Banner County?\n\nBanner County saw a significant population increase in 2023, growing by 17 people for a 2.6% jump, the highest rate of growth in the state. This growth was entirely due to an influx of new residents, as the county experienced no natural population change, likely due to its location in the sparsely populated northwestern corner of the state.\n\nHmmm. Is the fact that there were the same number of births and deaths caused by it’s location in the northwest corner of the state? Debatable. And, honestly, this re-write bot has been the source of the most questions I have about this whole enterprise. We’ll talk more about that below.\nOne last thing: Why not give every county a headline instead of just simply having the county name in the map? Done and done.\n\nheadline_writer_role &lt;- \"You are a headline writer. Your job is to write a short headline based on the summary given to you. This headline should be short -- it has to fit into a small space -- so bear that in mind. Here is the capsule: \"\n\nheadline_agent &lt;- function(county_input) {\n  county_narrative &lt;- rewrite_agent_results |&gt; filter(county == county_input)\n  results &lt;- gemini(paste(headline_writer_role, county_narrative$rewrite_county_narrative))\n  results &lt;- gsub(\"\\n\", \"\", results, fixed = TRUE)\n  Sys.sleep(5)\n  print(paste(\"Processed\", county_narrative$county))\n  \n  # Return a single-row tibble\n  tibble(county = county_narrative$county, base_narrative = county_narrative$base_narrative, capsule_narrative = county_narrative$capsule_narrative, rewrite_county_narrative = county_narrative$rewrite_county_narrative, headline = results)\n}\n\n# Use map_df to directly create the final dataframe\nheadline_agent_results &lt;- purrr::map_df(rewrite_agent_results$county, headline_agent)\n\nAnd what does that look like in Banner County?\n\nBanner County Booms: 2.6% Growth Fueled by New Residents\n\nHere is the exact same map, same data, but now with headlines and narratives written for each county. Click on one and you’ll get a human-friendly narrative about that county, and the numbers below that if you want them.\n\n\nIs Christopher Nolan going to make a movie about this moment? Not hardly. Have I “Saved Journalism”? Nope. Not even close. Did I save a human a few minutes of drudgery 93 + 93 + 93 + 93 times? Yes. Yes I did. Is this idea extensible and repeatable? It sure is. I think that’s good enough for now.\n\n\nWhere to go from here\nFrom here, what this needs is a block of code that pushes the results of the headline editor agent to Google Sheets, where a human can go through each one and make sure everything is fine. I’ve tried to do that in R Studio, which is not great, and limits the number of people who could do this. And to be sure, it needs to be done. Nebraska geography nerds – there are tens of us! – will able to find Arthur County, one of the smallest counties in the US by population. For those who can’t, the rewrite bot describes Arthur as “located in the northwestern corner of the state and home to the county seat of Arthur.” Half of that is objectively true. Arthur is much more arguably in the west-central part of the state, but it really depends on where you draw the lines. It’s arguable enough that a good local editor would drop that part and leave the county seat part, which is correct. Arthur County’s county seat is … Arthur.\nThen, as a last step, I would automate the process of creating the Datawrapper chart with the DatawRapper library, which accesses the service’s API. Imagine the Census Bureau publishing the data, and then in a matter of minutes you have that map done and online. To do that, you’d have to have a little budget for API calls so you aren’t waiting 32 minutes for the whole thing to run – 8 + 8 + 8 + 8 minutes for each step. But even then, is there really a competitive contest over who can publish Census maps in Nebraska fast enough? No.\nBut if you, like me, are thinking about how we’re going to incorporate AI into journalism in ways that make sense; doesn’t risk the reputation of the organization you work for; and likely doesn’t horrify your audience to a point that they turn away from you for feeding them AI slop, this is an example of just how to do that.\nAim small, miss small."
  },
  {
    "objectID": "posts/2014-02-11-a-small-step-toward-solving-the-our-curriculum-is-too-full-problem/index.html",
    "href": "posts/2014-02-11-a-small-step-toward-solving-the-our-curriculum-is-too-full-problem/index.html",
    "title": "A small step toward solving the our curriculum is too full problem",
    "section": "",
    "text": "One of the arguments used to push back against adding new things into journalism school curricula is “Our curriculum is too full! We can’t possibly add anything more! What are we going to do? Stop teaching writing? Or editing?”\nFirst, the argument is silly — no one said anyone was going to stop teaching writing or reporting or whatever “fundamental” skill is most beloved by the combatant. \nSecond, it’s not an either or problem.\nPut another way, you can view a crowded curriculum as a challenge or a lament. Too many view it as a lament and throw their hands up.\nI’ve started picking away at this problem — viewing it as the challenge that it is — by focusing on a specific class: beginning reporting.\nLet’s start with a set of statements:\n\n\nI believe that all reporting classes need to beef up the amount of data journalism contained in them. Data is everywhere, and thus it’s part of modern journalism, and thus it should be in every reporting class.\nI believe the level of mathematics education that most journalism schools require (if any at all) is far too low.\nI believe the inside-joke “journalists are bad at math ha ha” has to stop.\n\n\nGiven that, the place to start is beginning reporting. Why there? Because if you expose math and data to beginning reporters, they will not know any better than to think that math and data are part of the job. Because guess what? Math and data are part of the job.\nThe argument that I get against adding data to a beginning reporting class is that it can’t be done because we need to spend as much time as possible teaching them how to write and how to report. Learn how to interview people. Learn how to write an effective lead. Learn how to write using AP Style. Learn how to find the news. Learn how to come up with good story ideas. And many of them don’t get enough of all that in beginning reporting. \nAnd, to be fair, they have a point. Until you write a few thousand leads, they’re hard to do and even harder to do well. For students who don’t read news, finding the news in something is difficult. It takes repetition.\nBut what if we combined some skills training and showed how one leads to another which leads to another? \nWhat if we combined the basic math-for-reporters section we teach with how to do those exact same problems with a spreadsheet, and then used that spreadsheet with real data to find real stories? Math + Data = Story ideas. \nWell, I’ve started to do exactly that, and I’m hosting it on GitHub. It’s a work in progress, but the idea is to take basic math skills, show how you can do them on a spreadsheet, then take real data and do the same thing, but this time with an eye toward generating story ideas. I’d like this module to take a week of class time — with in-class and out-of-class learning. \nI have no idea when I’ll be done with it, but I intend to use it in my own beginning reporting class when it’s done. As always, pull requests, issues or criticisms are welcome. \nSome of the math is painfully basic, but I’ve found students who had no idea that mean and average were the same thing. They’d so completely fled from math that concepts like orders of operation were foreign to them. So laugh all you want at some of the topics covered, but I assure you, they’re rooted in experience. \nI have to believe that with a little creativity, the “our curriculum is too full” problem can be solved. I have to believe this because if it can’t, journalism education is screwed."
  },
  {
    "objectID": "posts/2008-01-12-molten-content-data-ghettos-and-why-your-cms-problems-are-an-excuse-not-a-reason/index.html",
    "href": "posts/2008-01-12-molten-content-data-ghettos-and-why-your-cms-problems-are-an-excuse-not-a-reason/index.html",
    "title": "Molten content, data ghettos and why your CMS problems are an excuse, not a reason",
    "section": "",
    "text": "Note: This post came from a version of this blog that got lost in a server failure. It’s been restored from old RSS feeds, Google caches and other sources. As such, the comments, links and associated media have been lost.\n\nThe other component of the data ghetto that bothers me is that you can’t find that data outside the ghetto. Please, someone point me to a place where there’s dynamic content being fed to the story level pages. I have yet to see where someone’s crime data is being fed into a story about a crime, i.e. a map of murders from the data ghetto’s crime application dynamically generated on a story page about a murder. Or a list of the largest donors to a politician from a campaign finance app on a story about a politician.\nAnd that seems to be a problem we’re creating for ourselves — we’re only thinking about getting the data online, not about what to do next. Or about what else could we do with our data. Or what could someone else do with it if we let them. We’re content with a couple of search boxes, a button and a results page. And we’re content to leave it right where we put it.\nHere’s why I’m thinking about this now: I spent all this time building PolitiFact to be a layered, data-driven approach to political and fact check journalism. What have I spent my time doing since? Trying to figure out how to get my content out of that site and into other forms. First was the automatically generated email newsletter (sign-up here). Then it was a widget, which required me turning the Truth-O-Meter into a JSON stream that I could parse with a little javascript. Now? We’re syndicating PolitiFact content to newspapers who sign up at a whole other site. Now, subscribers can go to PolitiFactMedia and get our content for their publications via a password protected site. That site includes a pure REST API for automated import into whatever CMS the customer is using.\nDoing all this got me thinking of another concept: molten content.\nI’ve always thought of the work I was doing as building something out of raw materials. As a reporter, I did interviews, read documents and analyzed data. All that raw material was worked into a story, a graphic, maybe some photos, and lately some online interactive content. Building news applications, I’m finding, is more like working with metal. The more malleable you make your content, the easier it is to mold your application into all the different places it may need to go.\nMost places, the data in the main newspaper.com CMS is cold iron — hard as hell to work with, if not impossible. So most of the time, we’re not going to be bringing our newspaper content to our applications. But what if we brought our application to our newspaper content?\nAsk yourself this: Why can’t you find data from a newspaper.com’s data ghetto outside of there? A large reason for a lot of data ghettos is that the CMS is on one set of servers based on one technology and the place for data is on a whole other server setup with another technology. That’s driven by the horror stories most newspaper.com webworkers have about how gawdawful their CMS is and how the CMS won’t serve up this or handle that (or hell, even shovel the previous day’s paper online cleanly).\nBut, if you designed your application right, your CMS problems are an excuse, not a reason.\nAs I go forward, I’m adding a hidden requirement to my applications: make the data molten — the stage where the metal is nearly liquid, easy to pour in whatever form I need it to go into.\nHere’s a simple example: PolitiFact’s widget. It was my first attempt at molten content. I’m going to breeze over the code for now — if anyone wants me to detail it, post a comment below and I’ll do it in another post.\nBasically, the widget is the sum of a few parts. First, a user embeds a little piece of script on their page (you can get it here). That calls a piece of Javascript that looks like this. That Javascript code then makes a couple of calls itself: to a CSS file and to a page that returns a query to the PolitiFact database in JSON format. That page is actually pretty hackish: It’s a pretty vanilla view in Django which then returns a template that fakes the JSON. If I were to do it over again, there’s better means to serialize data in Django. But my hack worked and I haven’t had time to make it work the more elegant way. Anyway, the script goes to that page, parses out that data and then writes it to your browser screen.\nHere’s what I mean by molten content. PolitiFact resides on a server at my employer’s server room. This blog is hosted somewhere else. That’s Django, this is Wordpress (which is PHP based)(* this is a post from my old blog. It’s all Django now). Different systems, different servers, different states, different everything. And, here’s that dynamic content:\nSo, if I can take PolitiFact and put it on my blog with these tools, why can’t we take it a step further and put any and all data from our news applications into our story pages?\nBecause, as I just showed you, we can. We can do it better than this. If you’re developing news apps and don’t know anything about web services, you should start learning.\nThe broader point here, divorced from technologies and implementations, is that we need to start thinking about where our data is going to go and what we’re going to do with it beyond search and results pages at our one URL. More on this in the coming months, when some other projects I’m working on go live."
  },
  {
    "objectID": "posts/2012-01-09-a-word-of-advice-for-code-year-participants/index.html",
    "href": "posts/2012-01-09-a-word-of-advice-for-code-year-participants/index.html",
    "title": "A word of advice for Code Year participants",
    "section": "",
    "text": "A whole herd of people are learning to program this year through free weekly lessons via email from Code Year. Some, like me, are interested in new lessons and approaches. Some people are starting from scratch. I have a word of advice to you, one I need to follow myself, as you get started on your Code Year.\nUnplug.\nTurn off Tweetdeck. Shut down IM. Turn off your email notifier. No one has liked your status in the last minute, so don’t check. Unplug. \nWhy? Because you’ll learn nothing in 20-30 second bursts between distractions. Learning to code, like many things in life, requires you to focus for extended periods of time. It requires to you really burrow in on details. This is probably a level of detail you may not be accustomed to. It’s probably concepts you aren’t familiar with. You need uninterrupted time to focus on the task at hand. \nIn short, you aren’t as good at multitasking as you think you are. So don’t do it. Unplug, focus, and really learn something.\nWant an experiment in how distracted you are? Get into the environment you intend to work on your Code Year exercises in. Turn on the stuff you normally have on. I’ve got Spotify, an IM client and Tweetdeck on myself right now, plus my Google notifier and my phone handy. Turn all the stuff on that you’d normally have running.\nNow read this. It’s a fantastic essay on how important solitude is to proper thinking. See how long you can go before you’re distracted.\nMe? Two paragraphs. It took me over an hour to read that one essay because oops, got an email and now someone @ replied me on Twitter and awwww, cute baby on Facebook and here’s a text about getting dinner tonight. And on and on and on and on.\nIt was shocking to me how my social/work habits had ruined my ability to concentrate on a thing and read it. Really read it. At times, the urge to check email was physical. I could feel myself getting uncomfortable because what if I did get an email? What if? Well, I better check. \nAnd so I’m making a change. At points in my day now, when I need to focus on something, I’m opting into a total digital blackout. Goodbye email notifier. So long Twitter. Shhhhh now Spotify. It’s thinking time. They’ll be there in an hour. And I’m not an on-call brain surgeon, so no one is going to die if I don’t get that text.\nIt’ll wait. It can all wait. It’s thinking time. Time to focus."
  },
  {
    "objectID": "posts/r-llm-starter-pack/index.html",
    "href": "posts/r-llm-starter-pack/index.html",
    "title": "An R + LLM starter kit",
    "section": "",
    "text": "I’ve written before, I am at best an enthusiastic amateur when it comes to AI, LLMs and R. But I’m braver/dumber than most, so for a talk I’m giving to NE-RUG – the Nebraska R Users Group – and to the NICAR data journalism conference, I’ve got some resources and some code to share.\n\nR libraries\nellmer: From the folks who brought you the tidyverse comes ellmer, a library that supports talking to a large number of LLMs. To talk to the big commercial LLMs, you’ll need API keys, and that usually means having an API budget. But what I like about ellmer is that it also talks to locally hosted models as well. More about that later.\nchores: Built on top of ellmer, chores is a neat way to make tools inside of R Studio that leverage LLMs to accomplish tasks. Examples are to help with certain kinds of code tasks, or help with explanations of what an error message means.\n\n\nExternal resources\nOllama: A cross-platform system of downloading, managing and running open-source LLMs on your local machine. With Ollama, you can run Meta’s Llama3 or DeepSeek’s R1 locally, using them to accomplish tasks without incurring costs. A rough rule of thumb is that you can run models slightly smaller than the amount of RAM you have. For example, my computer has 16GB of RAM, so I can run 14 billion parameter models (albeit somewhat slowly).\n\n\nBasics of ellmer\nellmer at first is very simple. You can just start a chat and … chat.\n\nlibrary(ellmer)\nlibrary(tidyverse)\nlibrary(glue)\n\nchat &lt;- chat_gemini()\n\nchat$chat(\"Tell me three jokes about journalists\")\n\nHere are three jokes about journalists:\n\n1.  An editor yells at a new cub reporter, \"Why isn't this story done yet?!\" \nThe cub reporter replies, \"I'm still waiting for all the facts to come in.\" The\neditor rolls his eyes and says, \"Facts? Son, never let the truth get in the way\nof a good headline!\"\n\n2.  How many journalists does it take to change a light bulb?\n    None. They just report that the room is dark.\n\n3.  What's the difference between a journalist and a pitbull?\n    A pitbull eventually lets go.\n\n\n\n\nStructured data output\nMost models can be used to extract structure from sentences.\n\nchat &lt;- chat_gemini()\n\ndata &lt;- chat$extract_data(\n  \"My name is Matt and I'm a 49 year old Journalism major\",\n  type = type_object(\n    age = type_number(),\n    name = type_string(),\n    major = type_string()\n  )\n)\n\nas.data.frame(data)\n\n  age name      major\n1  49 Matt Journalism\n\n\n\n\nSomething more like data journalism\nWhat if we wanted to normalize some messy, messy data. In Nebraska, the Department of Corrections publishes a live dataset of incarcerated people that you can download. You can ask and answer all kinds of questions from it – demographics, etc. But what you can’t do is figure out what is holding the most people, or how many people are there at least in part because of drug charges. Why? Because the charges they are serving time for are not normalized. Here’s an example of what they look like:\nPOS CNTRL SUB-METHAMPHETAMINE\nMURDER 1ST DEGREE\nPOSSESSION OF METHAMPHETAMINE\nPOS CNTRL SUB (METH)\nPOSSESS CONTR SUBSTANCE-METH\nDELIVERY OF METHAMPHETAMINE\nPOSS W/ INTENT DIST METH\nCan an LLM help us with this? Yes!*\n\nchat &lt;- chat_gemini()\n\nchat$extract_data(\n  \"POSS W/ INTENT DIST METH\",\n  type = type_object(\n    methamphetamine_related = type_boolean(),\n    drug_possession_related = type_boolean(),\n    drug_distribution_related = type_boolean(),\n    fully_spelled_out_no_abbreviations = type_string()\n  )\n)\n\n$methamphetamine_related\n[1] TRUE\n\n$drug_possession_related\n[1] TRUE\n\n$drug_distribution_related\n[1] TRUE\n\n$fully_spelled_out_no_abbreviations\n[1] \"Possession with intent to distribute methamphetamine\"\n\n\n* you have to ruthlessly check this. It’s good, but it is not perfect.\nWhy am I only doing one here? Because the free tier of Gemini limits you to about 5 queries a minute. But it wouldn’t be that hard to write a function that runs through your list of charges and inserts a pause after each one to ensure you stay in the free tier.\nWhat else can be done?\n\n\nNuclear powered population maps\nHere is a basic population map of Nebraska with one-year change values in it. We’ve all made this chart before. It’s simple, but it gets the job done.\n\n\nI’ve written a lot more about this process here but what if we could take the basic data from our population data and create narratives out of it. Instead of the boring tabular way of giving them the information, they get a human narrative of a few sentences.\nFirst, let’s create the base narrative. That is the words we will feed to Gemini.\n\ncountypopchange &lt;- read_csv(\"https://the-art-of-data-journalism.github.io/tutorial-data/census-estimates/nebraska.csv\")\n\nstatenarrative &lt;- countypopchange |&gt; \n  select(COUNTY, STATE, CTYNAME, STNAME, POPESTIMATE2023, POPESTIMATE2022, NPOPCHG2023, NATURALCHG2023, NETMIG2023) |&gt;\n  mutate(POPPERCENTCHANGE = ((POPESTIMATE2023-POPESTIMATE2022)/POPESTIMATE2022)*100) |&gt; \n  mutate(GEOID = paste0(COUNTY, STATE)) |&gt; \n  arrange(desc(POPPERCENTCHANGE)) |&gt; \n  mutate(PCTCHANGERANK = row_number()) |&gt; \n  mutate(base_narrative = glue(\n  \"County: {CTYNAME}, Population in 2023: {POPESTIMATE2023}, Population in 2022: {POPESTIMATE2022}, Population change: {NPOPCHG2023}, Percent change: {POPPERCENTCHANGE}, Percent change rank in {STNAME}: {PCTCHANGERANK}, Natural change (births vs deaths): {NATURALCHG2023}, Net migration: {NETMIG2023}\")) \n\nstatenarrative$base_narrative[[1]]\n\nCounty: Banner County, Population in 2023: 674, Population in 2022: 657, Population change: 17, Percent change: 2.58751902587519, Percent change rank in Nebraska: 1, Natural change (births vs deaths): 0, Net migration: 17\n\n\nNow, we’re going to add a system prompt to our chat to give Gemini some guidance. It’s not hard to do and you should do it.\n\nchat &lt;- chat_gemini(\n  system_prompt = \"You are a demographics journalist from Nebraska. Your job today is to write short -- 2-3 sentence -- summaries of population estimates from the Census Bureau for each county in the state. I will provide you the name of the county and a series of population numbers for the county. Your job is to turn it into a concise but approachable summary of what happened in that county. Here is the data you have to work with: \"\n)\n\nchat$chat(statenarrative$base_narrative[[1]])\n\nBanner County saw its population rise to an estimated 674 residents in 2023, \nadding 17 people from the previous year. This 2.6% increase makes it Nebraska's\nfastest-growing county, with all growth attributed to new residents moving into\nthe area.\n\n\nEarth-shattering? Hardly. But what if instead of world changing uses of technology, the right aim for AI is to offload tasks we would do because it would be better, but aren’t worth spending the time doing because we all have a limited time on this earth?\nIf you were to run this 93 times – or, you know, make a function to do that – and added a headline writing bot to this, here’s what your map now looks like."
  },
  {
    "objectID": "posts/2011-07-15-take-a-few-minutes-and-watch-this-and-1-tell-me/index.html",
    "href": "posts/2011-07-15-take-a-few-minutes-and-watch-this-and-1-tell-me/index.html",
    "title": "Take A Few Minutes And Watch This And 1 Tell Me",
    "section": "",
    "text": "Take a few minutes and watch this and 1) tell me we’re not living in the motherflippin’ future and 2) this bad boy couldn’t be used for some very, very cool journalism. Tornadoes, hurricanes, earthquakes, floods … you name it. Any kind of disaster where spatial extent is newsworthy."
  },
  {
    "objectID": "posts/nebraska-is-the-third-best-worst-basketball-team/index.html",
    "href": "posts/nebraska-is-the-third-best-worst-basketball-team/index.html",
    "title": "Nebraska is not the best worst team in basketball again. They’re third best worst.",
    "section": "",
    "text": "Last year, this post may have suggested that Nebraska would be better this year than last year. That Nebraska was the best worst team in college basketball, and with major recruits coming in and the Big Ten expected to take a step back, all looked up.\nOops.\nIt didn’t work out as expected – in spite of a late season surge. But let’s return to the question: Is Nebraska the best worst team in college basketball?\nSpoiler alert: Not this season.\nReturning to Sports Reference’s college basketball site, we find our friends the Simple Rating System and Strength of Schedule. The SRS is a mix of average point differential and strength of schedule. Given that, a team with a losing record could have a positive rating if they lose games close but play good teams.\nLike, say, Nebraska.\nTo find the worst teams, we’ll use the last place team in each conference by conference wins again.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(gt)\nlibrary(ggbeeswarm)\nlibrary(ggrepel)\n\nstats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/stats22.csv\")\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/logs22.csv\")\n\nstats &lt;- games %&gt;% \n  select(Team, Conference) %&gt;% \n  distinct() %&gt;% \n  right_join(stats, by=c(\"Team\"=\"School\")) %&gt;% \n  filter(Games &gt; 0)\n\nlastplace &lt;- stats %&gt;%\n  group_by(Conference) %&gt;% \n  arrange(desc(ConferenceWins)) %&gt;% \n  slice(n()) %&gt;% \n  filter(Games &gt; 10) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(OverallSRS))\n\nnu &lt;- lastplace %&gt;% filter(Team == \"Nebraska\")\n\nlastplace %&gt;% \n  select(Team, Conference, OverallWins, OverallLosses, OverallSRS, OverallSOS) %&gt;% \n  rename(W = OverallWins, L=OverallLosses, `Simple Rating` = OverallSRS, `Sched. Strength`= OverallSOS) %&gt;% \n  top_n(10, wt=`Simple Rating`) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"The Huskers didn't repeat as best of the worst\",\n    subtitle = \"Notable: NC State - 3OT winner against Nebraska - finished just a hair ahead.\"\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"By Matt Waite\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Source: [Sports Reference](https://www.sports-reference.com/cbb/seasons/2022-school-stats.html)\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(Team)\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &gt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &gt; 0\n    )\n  ) %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n\n\n\nThe Huskers didn't repeat as best of the worst\n\n\nNotable: NC State - 3OT winner against Nebraska - finished just a hair ahead.\n\n\nTeam\nConference\nW\nL\nSimple Rating\nSched. Strength\n\n\n\n\nWest Virginia\nBig 12\n15\n16\n10.65\n10.61\n\n\nNC State\nACC\n11\n20\n3.59\n6.08\n\n\nNebraska\nBig Ten\n10\n21\n3.54\n8.89\n\n\nGeorgetown\nBig East\n6\n24\n1.91\n8.31\n\n\nGeorgia\nSEC\n6\n25\n0.64\n8.48\n\n\nOregon State\nPac-12\n3\n27\n-0.76\n8.95\n\n\nSouth Florida\nAAC\n8\n22\n-2.86\n4.77\n\n\nNortheastern\nCAA\n9\n22\n-6.01\n-1.20\n\n\nDuquesne\nA-10\n6\n23\n-6.39\n2.02\n\n\nPepperdine\nWCC\n7\n25\n-6.48\n3.97\n\n\n\nBy Matt Waite\n\n\nSource: Sports Reference\n\n\n\n\n\n\n\n\nThis season, West Virginia takes the best worst crown. They played the toughest schedule and won 15 games, five more than Nebraska, and ended with a far higher simple rating. A familiar name on the list will be NC State, which won an three overtime thriller against Nebraska. They had one more win than the Huskers, and finished just a tiny bit ahead.\nBut what about all teams in college basketball? Where does Nebraska rank out against other teams and other conferences?\n\n\nCode\nggplot() +\n  geom_vline(xintercept = 3.54) + \n  geom_beeswarm(\n    data=stats, \n    groupOnX=FALSE, \n    aes(x=OverallSRS, y=Conference), color=\"grey\") + \n  geom_beeswarm(\n    data=lastplace, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"blue\") + \n  geom_beeswarm(\n    data=nu, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"red\") +\n  geom_text(\n    aes(x=0, y=\"Big Ten\", label=\"Nebraska\")\n  ) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska's simple rating is better than 5 conferences\", subtitle=\"The Husker's finished dead last in the Big Ten, but wouldn't have nearly everywhere else.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\nWarning: The `groupOnX` argument of `geom_beeswarm()` is deprecated as of ggbeeswarm\n0.7.1.\nℹ ggplot2 now handles this case automatically.\n\n\n\n\n\n\n\n\n\nLast year, Nebraska wins nine conferences. Now it’s five. None of them are powerhouse conferences.\nWhat about the Power Five?\nTo put West Virginia in perspective, there’s 28 Power Five teams with a simple rating worse than they had. Nebraska finishes 22nd among that group. Among that group: Miami, which beat second-seed Auburn in the NCAA Tournament and, as of this writing, is still playing basketball.\n\n\nCode\npowerfive &lt;- c(\"Big Ten\", \"Big 12\", \"Pac-12\", \"SEC\", \"ACC\")\n\nbetterpowerfive &lt;- stats %&gt;% filter(Conference %in% powerfive) %&gt;% filter(OverallSRS &lt;= 10.65) %&gt;% arrange(desc(OverallSRS))\n\nggplot() + \n  geom_point(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, size=OverallWins)) + \n  geom_point(data=nu, aes(x=OverallSRS, y=OverallSOS, size=OverallWins), color=\"red\") + \n  geom_text_repel(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, label=Team), point.padding = 4) +\nlabs(x=\"Simple Rating\", y=\"Schedule strength\", title=\"Nebraska had a harder schedule than multiple tournament teams\", subtitle=\"Nebraska isn't the best of the worst again, but teams in their ranking neighborhood had better fortunes.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nThe road forward for Nebraska is even murkier than last year. There’s coaching vacancies and roster turnover ahead, and after last year, no prediction here."
  },
  {
    "objectID": "posts/2013-06-10-here-is-john-keefe-and-i-talking-about-the-near/index.html",
    "href": "posts/2013-06-10-here-is-john-keefe-and-i-talking-about-the-near/index.html",
    "title": "Here Is John Keefe And I Talking About The Near",
    "section": "",
    "text": "Here is John Keefe and I talking about the near field future of sensors and journalism at the Tow Center’s Sensor Weekend. We gave the second keynote of the day."
  },
  {
    "objectID": "posts/2016-03-24-i-wrote-a-data-journalism-manual-for-my-college-in-1997-they-never-used-it-but-they-kept-it/index.html",
    "href": "posts/2016-03-24-i-wrote-a-data-journalism-manual-for-my-college-in-1997-they-never-used-it-but-they-kept-it/index.html",
    "title": "I wrote a data journalism manual for my college in 1997. They never used it, but they kept it.",
    "section": "",
    "text": "In October 1997, I was trying to graduate from college. The University of Nebraska-Lincoln’s College of Journalism and Mass Communications told me that I needed 19 credit hours to graduate mid year, and that’s how many I was taking.\nThat is, until they told me six weeks before graduation that I needed one more.\nTo get that credit hour, the Journalism Department chair agreed to let me complete an independent study. In that independent study, for one credit hour, I was to create a Computer-Assisted Reporting class. The proposal I had to write said I would create a syllabus, recommend texts and create class materials.\nIn the proposal, I wrote that “every reporter should know how to feed a city budget into a spreadsheet and figure out the percent changes in the budget.” I also said they should be able to query a database, but not every reporter would have to know stats like linear regression. “An introduction to statistics is essential, but any depth is a waste of time.”\nI was 22. It was 1997. We all make bad calls sometime.\nBut how do I know about this? Because the very same college that I wrote this proposal and subsequent manual for is now my employer. The same employer where I now teach data journalism. And that employer is cleaning out the archives, and today they found a file with my name on it.\nIn the file were notes from my senior exit interview. In that interview, I said that I was “not totally satisfied with the core courses” and believed the college should hire a data journalism/reporting professor like Steve Doig. Absolutely nothing has changed since that day. I’m still not totally satisfied with the core courses, and I would cut off a toe it it meant we got to hire Steve.\nBut also in that file was the proposal for the data journalism course and the course materials I wrote for the credit.\nIt’s equal parts cringe worthy and things I teach now, 19 years later.\nIn it, I introduce strange lingo like ASCII text, delimiters and zip files. I show how to use Excel to measure change in populations. I show how to group and count some crime data to show what day the Lincoln Police Department got the most larceny reports (in 1995) using SQL. I talked about how to negotiate for data, and avoid being overcharged for data by bureaucrats who didn’t want to give up digital records. I showed people how to sign up for NICAR-L (which I joined in 1995 and have been on ever since). I even talked about how to use the web to find data. How? By using Dogpile, of course, to search for the Census Bureau’s population projections data set. Google didn’t exist when I wrote this.\nThere’s even a joke where the punchline is Quattro Pro.\nI also made a bad bet on the geek/nerd label. The manual is called “Geek Like Me” and I talked about how everyone wanted to be a geek now. Alas, nerds ended up being the preferred term.\nFrom what I can tell, the manual was read by the department chair – he noted a dozen or so typos in highlighter – and it was put in a file. To my knowledge, the document was never used. We can surmise that file somehow survived the retirement of that department chair, survived the move from one building to another one across campus, and has sat in a filing cabinet for, let’s be honest, no good reason at all until this week.\nMy copy of it was put on a Zip Disk where I thought it would be safe. I long ago lost the ability to read that disk, if I could even find it. I had just assumed that my efforts – strung out over a series of all-nighters – had just faded into nothing. To be honest, I had forgotten all about it until today.\nAnd here it is.\nIn the 14 years between my graduation and me being hired as a professor of practice, a course appeared on the books: JOUR407 Investigative and Computer Assisted Reporting. No one here now knows how it got there, who proposed it, or if it was ever taught before I got here. But I teach that course now. And all of my materials are online.\nHere’s hoping GitHub doesn’t go the way of the Zip Disk."
  },
  {
    "objectID": "posts/2008-07-18-how-not-to-be-a-wordpress-hero/index.html",
    "href": "posts/2008-07-18-how-not-to-be-a-wordpress-hero/index.html",
    "title": "How not to be a Wordpress hero",
    "section": "",
    "text": "Note: This post came from a version of this blog that got lost in a server failure. It’s been restored from old RSS feeds, Google caches and other sources. As such, the comments, links and associated media have been lost.\n\nThere’s a group of fellow journalists who are getting really sick of what we’re calling Wordpress Heroes. What is a Wordpress Hero? Back home, we’d call them All Hat and No Cattle. A Wordpress Hero is someone smart enough to setup a blog and fire away with grand ideas, but too dumb to use things like, oh, evidence, reporting, examples and other such annoyances. All talk, no do.\nI’m all for conversation, sharing ideas, throwing things out there. What I’m tired of are people who make grand sweeping statements and do nothing to back them up.\nSo, in the spirit of being constructive, here’s three simple ways to not be a Wordpress Hero when writing about “the future of journalism.”\nUse personal experience. The thing that drives me most crazy about Wordpress Heroes is that they’re all full of ideas … for someone else. Do it themselves? That’s crazy talk. The only person who can make innovation happen is the person who is willing to suffer the consequences. I didn’t write that but it’s true. So either be ready to do what you write, or be ready to admit that you won’t or can’t.\nUse research. Research is not linking to another blogger’s opinion. Research is numbers, facts, examples of things you are talking about. Best kind of research? Your own, from your own project. See step 1. There’s a lot of people writing about “the future of journalism” and a precious lack of actual facts being talked about. Don’t add to it.\nBe humble. Got an idea? Great. We need all we can get. Have no idea if it will work? No problem. That’s what makes new ideas fun. Don’t have the first clue how or the power to make it happen? That’s fine. Still want to write about it? Go ahead. But do yourself and everyone else a favor: admit, up high and in clear language, that you don’t know how to make this idea work or don’t have the power to put it into play. A particularly annoying trait of the Wordpress Hero is writing with absolute conviction about things they know nothing about. The fix? Admit you know nothing. It’s okay. No one knows exactly what they’re doing with journalism on the web. If such a person existed, we’d be doing what they said and all would be well. That person doesn’t exist, and you aren’t that person. Neither am I. Admitting it is okay. It makes you more credible.\nPeople have asked me why I haven’t updated this blog in so long. Part of it has been that I’m too busy actually building something. Another part is that I’m sick of all the noise. The worse things get in the industry, the less we need Wordpress Heroes."
  },
  {
    "objectID": "posts/2014-03-02-a-5-step-nicar-recovery-plan/index.html",
    "href": "posts/2014-03-02-a-5-step-nicar-recovery-plan/index.html",
    "title": "A 5-step NICAR recovery plan",
    "section": "",
    "text": "First time at NICAR this year? Awesome. Welcome. This was my 15th conference. I started in 1997 in Nashville. I was a senior in college, desperate to find a job, and NICAR was an amazing and formative experience for me. I made friends, learned a lot and found a kind of nerdy spiritual home. NICAR has become like a weird nomadic family for me. It’s my tribe. A really nerdy, wonderful tribe.\nBut I can remember being in your shoes after my first NICAR. You’re bone tired, but JACKED UP. Excited, but uneasy. There’s so much to learn. So many people doing so many amazing things with tools you’ve never heard of before, or using tools you have heard of in ways you never dreamed possible. It’s really easy to be intimidated by it all.\nDon’t be.\nThe truth is, everyone at NICAR has either been where you are or are exactly where you are right now. I’m even there with you. So much to do. So much to learn.\nDon’t let this time go to waste. Don’t let this energy fade away. Don’t be intimidated by the enormity of it all. Here’s 5 steps to avoid the post NICAR crash and burn.\n1. Find a specific story or project you want to work on that will require a technology you want to learn to get it done.\nThe absolute worst thing you can do is go to work on Monday without an idea or plan to use what you saw, learned or got interested in. It is shockingly easy to go back to work, fall right back into your routine, go right back to the stuff you were working on before the conference and before you know it, it’s a month later and you’ve forgotten half of what got you interested in the first place.\nThe solution is to walk into the office with an idea and a plan to get it done. You’ve probably got a list of ideas right now. That’s great. Write them down, create a spreadsheet or a to-do list on your phone or whatever. Pick one idea on your list and post the rest of it somewhere you’ll see it regularly. Focus on one, get it done, then move on.\nAnd start working on it right away. If you can walk in and start working hour one minute one on Monday, great. If you’ve got to find time in the day, brown bag your lunch and start then. Can’t get the time at the office? Work on it at home. But don’t wait. You’ll be shocked how fast this time will go and it’s vitally important. Waiting is the worst thing you can do.\n2. Pick one thing. One. You can’t learn all the NICAR in one project.\nMy annual problem with NICAR is I leave with a stack of ideas and technologies I want to try RIGHT NOW. And that’s on top of the three or four side projects I want to work on all the time.\nIt’s tempting to want to try and inhale new things and just jam them into your skull. But it’s impossible. In fact it’s harmful to your progress. Pick one thing. One. One technology, one technique, one task, and do it. Just that one.\nIf you want to use Excel to look at your city’s budget, do that. Just that. Those amazing pivot tables and NodeXL social network graphs you saw will be there when you’re ready. If you want to write a scraper in Python to get data you can use for a story, do that. Just that. The news app that puts it on the web will come later. The point here is to focus.\nPick one problem, solve it.\n3. Your first project is going to suck. Do it anyway.\nThis is just generally true, but one of the real intimidation factors for me at NICAR was that the class I took was taught by a Pulitzer Prize winner who did this otherworldly story and how the hell am I supposed to do that?\nYou’re not. Not yet anyway. The Taj Mahal was not built by someone who had never built an ugly shack before.\nMy first projects were for my campus newspaper and probably bored student readers right into a nap (which in college is known as reader service). But I still did them. Each one taught me something. How to do this with Excel, how to incorporate numbers into a story without boring people, etc. The awards came later, but never would have happened without those first stories.\nSame goes for news applications. There’s a whole website dedicated to showcasing everyone’s first news app. Mine is especially atrocious. Hey, it was 2001, what can I say? We could barely spell internet back then.\nBut if you’re worried about living up to the standards of the people who taught your class, stop. Your first project will suck. You’ll look back on it later and cringe. And at a NICAR down the road, I’ll buy you a beer and we can argue about who has the more awful first project. And 50 people around us will join in. Everyone’s first project sucks. Do it anyway.\n4. Don’t quit until it’s done.\nI get this question all the time: “I’m a journalist. I want to learn how to program. Tell me how.” I have suggestions and websites and learning materials I can tell them about, but the most important thing I tell them is this: Don’t quit until it’s done.\nWhen your brain hurts because you’re really stretching to learn something new, don’t quit. When you’re straight up frustrated because it’s not working and you don’t know why, don’t quit. When you’re tired of it, or you’re thinking “maybe this wasn’t for me”, don’t quit. Keep going. Push through that. You don’t get to say “this wasn’t for me” or “I just didn’t like it” until it’s done. If you finish whatever you set out to accomplish, and you see it in the world, and at that moment you decide to never do this again, fine. That’s your call. But I’ll bet you cash money seeing what you made in the world will hit you like a drug, and you’ll want to do it again. And next time, it’ll go faster, and it’ll be better, and you’ll want to do it again.\nLearning something new is about not quitting when your mind and body and soul tell you to quit. Don’t quit. Be stubborn. The idea of quitting, of letting it beat you, should be offensive to you. Get mad. Better yet, get even. Finish it. Don’t quit.\n5. Remember: You are not alone.\nI said at the beginning two things I want you to remember: NICAR is a weird, nomadic family. And welcome to it.\nPeople have been saying for decades that NICAR feels different – and it is different – because of the culture of the conference. I can talk for hours about what that culture is and why it’s the way it is, but it boils down to this: It’s a giving culture.\nEvery one of the speakers? Volunteer. All of the hands-on teachers? Same. Did you talk to someone in the hall? Grab a speaker after to ask them a question? They almost certainly stayed right there and answered your question, right? I can remember time after time of Big Name Journalists From Big Name News Organizations dropping everything and showing me how to do something. I can count scores of times where they gave me a business card and said call me if you run into trouble. That generosity amazed me. Inspired me. Made me want to do the same.\nThat culture extends outside of the conference. The NICAR-L listserv has been helping people every day for 20 years now. Get on it. Not comfortable with that? Email the teacher of your hands-on class. You’ll be amazed at how generous they are. Meet someone at the conference? Email them.\nAsk for help, and help will be there. I promise. It’s part of being in the tribe. You do not have to do this yourself.\nBut here’s the deal: Remember all the help you got. Because someday someone is going to come to you in that same moment. Help them. That too is being part of the tribe."
  },
  {
    "objectID": "posts/2011-07-15-some-very-smart-posts-about-killing-your-cms/index.html",
    "href": "posts/2011-07-15-some-very-smart-posts-about-killing-your-cms/index.html",
    "title": "Some very smart posts about killing your CMS",
    "section": "",
    "text": "Since I wrote my opus hating on legacy CMSes, I have been kicking the idea around in my head here and there, pondering just what this thing would look like, from backend systems to code to the presentation layer. Never anything fully baked or worth writing down. \nIn the past week, I’ve come across two posts that are just brilliant. And they dovetail nicely together. \nFirst, Stijn Debrouwere asks a hell of a good question:\n\nAnd that question is: what would the ideal web delivery platform look like if our priority was to help us piece together different components, not build everything into a single app like, say, your average Drupal install. Existing CMSes aren’t built for that kind of environment, so we don’t just need to complement the CMS by leveraging different best-of-breed tools each with their specific focus, we actually need to replace the CMS with some kind of presentation layer software that’s better suited to the new distributed reality.\n\nThen, Eric Hinton at Talking Points Memo takes a crack at answering it:\n\nOn Monday we launched the second component of this incipient media-death-star, our frontpage publisher - nee Baroque. To get an idea of how it works watch a quick demo we made. It’s fast, formless and - best of all - built completely outside of our CMS. Currently, MT’s publishing engine notifies Baroque to cache new entries but it would be trivial to plug any other data source in. Eventually, when the core API and datastore is complete, MT won’t talk to Baroque directly at all. On this day, we will finally loose the shackles of the monolithic CMS model."
  },
  {
    "objectID": "posts/2009-11-23-what-would-you-want-out-of-a-class-taught-by-a-journalist-programmer/index.html",
    "href": "posts/2009-11-23-what-would-you-want-out-of-a-class-taught-by-a-journalist-programmer/index.html",
    "title": "What would you want out of a class taught by a journalist-programmer?",
    "section": "",
    "text": "You know journalism is in trouble when you know this: I’m being invited to teach a class at a respected journalism school. The fun part, and not a very surprising part given the state of the industry right now, is that neither they nor I have a really solid idea what the class is going to be.\nThe class will start in the fall of 2010, so we have time to figure it out. Obviously, given what I do now, they’re not asking me to give a seminar on modern American narrative. I’m a journalist who builds web sites. I got into it when I built PolitiFact, the first website to win a Pulitzer Prize. Since then, I’ve built a few more sites and gone into consulting for media companies on the side. So, I’m just guessing that’s what I’m going to be talking about. But that’s an awful broad area. Want to help?\nIf you were going to take a journalism class from a journalist/programmer, what would you want the class to be?\nMy pitch thus far has been this: The business of the press has been mostly unchanged for hundreds of years, so journalism schools have evolved during that time to teach craft. That’s still important. But now that the business is being turned inside out, it’s time for entrepreneurship in the journalism school. Not my line, but a good one: The future of journalism belongs to the entrepreneurs. So I’m thinking a class that’s part business, part product development, part programming, part journalism. Ideally, we’d build and launch a product, but that may be hoping for a lot in a single semester, non-required, non-core class. I want to pull people from other colleges in there – Nebraska has a fine entrepreneurship program at the business college, for example. I’ve got a million ideas, but I’m more curious what you think. What would you want out of a class like this?\nIf my comment system breaks for you or you don’t want to publicly share your thoughts, I’m at matt (dot) waite (at) gmail (dot) com."
  },
  {
    "objectID": "posts/is-nebraska-the-best-worst-basketball-team/index.html",
    "href": "posts/is-nebraska-the-best-worst-basketball-team/index.html",
    "title": "Is Nebraska the best worst team in college basketball?",
    "section": "",
    "text": "Nebraska fans haven’t had the best time watching basketball lately. The last two seasons have featured only seven wins in each season. This season they only won three games in the Big Ten, but that was an improvement over last season when they only won two.\nBut anyone watching Nebraska basketball this season could see there was a difference between last season’s squad and this one. And given that the Big Ten was rated as the best conference in college basketball during the season, it begs the question: Is Nebraska that bad?\nIn fact, are they the best worst team in college basketball?\nLet’s take a look at some numbers.\nSports Reference’s college basketball site produces a Simple Rating System and Strength of Schedule number for each team. The SRS is a mix of average point differential and strength of schedule. Given that, a team with a losing record could have a positive rating if they lose games close but play good teams.\nLike, say, Nebraska.\nTo find the worst teams, we’ll find the last place team in each conference by conference wins (minus the Ivy League, which didn’t play due to Covid).\nThe first question we’ll ask and answer is by rating, who is the best of the last place teams? Here’s the top 10 of the bottom.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(gt)\nlibrary(ggbeeswarm)\nlibrary(ggrepel)\n\nstats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/stats21.csv\")\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/logs21.csv\")\n\nstats &lt;- games %&gt;% \n  select(Team, Conference) %&gt;% \n  distinct() %&gt;% \n  right_join(stats, by=c(\"Team\"=\"School\")) %&gt;% \n  filter(Games &gt; 0)\n\nlastplace &lt;- stats %&gt;% \n  group_by(Conference) %&gt;% \n  arrange(desc(ConferenceWins)) %&gt;% \n  slice(n()) %&gt;% \n  filter(Games &gt; 10) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(OverallSRS))\n\nnu &lt;- lastplace %&gt;% filter(Team == \"Nebraska\")\n\nlastplace %&gt;% \n  select(Team, Conference, OverallWins, OverallLosses, OverallSRS, OverallSOS) %&gt;% \n  rename(W = OverallWins, L=OverallLosses, `Simple Rating` = OverallSRS, `Sched. Strength`= OverallSOS) %&gt;% \n  top_n(10, wt=`Simple Rating`) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"The Huskers are the top of the bottom\",\n    subtitle = \"They only won 7 games but have the best simple rating and toughest schedule among the last place teams.\"\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"By Matt Waite\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Source: [Sports Reference](https://www.sports-reference.com/cbb/seasons/2021-school-stats.html)\")\n  ) %&gt;% tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(Team)\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Sched. Strength`),\n      rows = `Sched. Strength` &gt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\", weight = \"bold\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &lt; 0\n    )\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"green\", weight = \"normal\"),\n    locations = cells_body(\n      columns = c(`Simple Rating`),\n      rows = `Simple Rating` &gt; 0\n    )\n  ) %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n\n\n\nThe Huskers are the top of the bottom\n\n\nThey only won 7 games but have the best simple rating and toughest schedule among the last place teams.\n\n\nTeam\nConference\nW\nL\nSimple Rating\nSched. Strength\n\n\n\n\nNebraska\nBig Ten\n7\n20\n6.08\n12.28\n\n\nUniversity of California\nPac-12\n9\n20\n4.89\n9.75\n\n\nDePaul\nBig East\n5\n14\n2.67\n8.83\n\n\nTexas A&M\nSEC\n8\n10\n2.20\n4.86\n\n\nEast Carolina\nAAC\n8\n11\n1.00\n3.83\n\n\nBoston College\nACC\n4\n16\n0.82\n8.97\n\n\nIowa State\nBig 12\n2\n22\n-0.72\n10.40\n\n\nLoyola (MD)\nPatriot\n6\n11\n-2.29\n-3.47\n\n\nIllinois State\nMVC\n7\n18\n-5.45\n0.38\n\n\nNorth Carolina-Wilmington\nCAA\n7\n10\n-6.84\n-6.47\n\n\n\nBy Matt Waite\n\n\nSource: Sports Reference\n\n\n\n\n\n\n\n\nNebraska has the best simple rating against the toughest competition of the top 10 of the bottom. Meaning they played better against tougher teams than any of these schools on here, including the Big East, the SEC and the Pac-12. The closest in competition to the Big Ten is the Big 12, and Nebraska is far better than Iowa State (which, this season, is a low bar).\nBut what about all teams in college basketball? Where does Nebraska rank out against other teams and other conferences?\n\n\nCode\nggplot() +\n  geom_vline(xintercept = 6.08) + \n  geom_beeswarm(\n    data=stats, \n    groupOnX=FALSE, \n    aes(x=OverallSRS, y=Conference), color=\"grey\") + \n  geom_beeswarm(\n    data=lastplace, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"blue\") + \n  geom_beeswarm(\n    data=nu, \n    groupOnX=TRUE, \n    aes(x=OverallSRS, y=Conference), color=\"red\") +\n  geom_text(\n    aes(x=2, y=\"Big Ten\", label=\"Nebraska\")\n  ) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska's simple rating is better than 9 conferences\", subtitle=\"The Husker's finished dead last in the Big Ten, but wouldn't anywhere else given their rating.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\nWarning: The `groupOnX` argument of `geom_beeswarm()` is deprecated as of ggbeeswarm\n0.7.1.\nℹ ggplot2 now handles this case automatically.\n\n\n\n\n\n\n\n\n\nAdmittedly, the nine conferences Nebraska wins aren’t exactly powerhouses. What about the Power Five?\nBy simple rating, Nebraska is better than 14 teams in the Power Five, including four SEC teams and three Big 12 teams. How much better? This bubble chart with the size of the dot scaled by number of wins shows that Nebraska is better than a lot of teams who won more games.\n\n\nCode\npowerfive &lt;- c(\"Big Ten\", \"Big 12\", \"Pac-12\", \"SEC\", \"ACC\")\n\nbetterpowerfive &lt;- stats %&gt;% filter(Conference %in% powerfive) %&gt;% filter(OverallSRS &lt;= 7.10) %&gt;% arrange(desc(OverallSRS))\n\nggplot() + \n  geom_point(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, size=OverallWins)) + \n  geom_point(data=nu, aes(x=OverallSRS, y=OverallSOS, size=OverallWins), color=\"red\") + \n  geom_text_repel(data=betterpowerfive, aes(x=OverallSRS, y=OverallSOS, label=Team), point.padding = 4) +\nlabs(x=\"Simple Rating\", y=\"\", title=\"Nebraska doesn't finish last anywhere else in the Power Five\", subtitle=\"In the Big 12 or the SEC, Nebraska is closer to the middle of the pack than the bottom.\", caption=\"Source: Sports Reference | By Matt Waite\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\nNebraska has the core of its team coming back and the Big Ten is not going to be as good as it was this year going forward. There’s been turmoil in some programs, turnover in others, and the NBA draft is going to take some talented players out of the conference.\nWhere does Nebraska finish next year? Signs point to better than last."
  },
  {
    "objectID": "posts/2009-03-03-build-something-or-stfu/index.html",
    "href": "posts/2009-03-03-build-something-or-stfu/index.html",
    "title": "Build something or STFU",
    "section": "",
    "text": "This blog has been quiet of late because I’ve been working in every spare moment I have on a couple of projects that are going to launch soon, good lord willing and the creek don’t rise. Given that I’m sleep deprived, stressed and generally ground down to a nub, it’s a bad, bad time for me to read my media-heavy RSS feeds.\nBefore I get myself into trouble, I just want to say this: If all these people who know so much about journalism on the web spent less time on waving their arms in hysterics and actually built something – created value, or tried a new model instead of opined on one – the world would be a very different place.\nSo, memo to journo-bloggers: Less talk, more walk. Build something or STFU. When that urge to run to Wordpress so you can tell us your thoughts on paid content or aggregation or community strikes you, stop yourself and spend that time actually creating something that does what you say. I’ll care about your thoughts when you can include a URL to your thoughts actually working.\nThose who are making a go at it, who are actually walking the walk, will understand this:\n\n“It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, and comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows the great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who know neither victory nor defeat. Shame on the man of cultivated taste who permits refinement to develop into a fastidiousness that unfits him for doing the rough work of a workaday world. Among the free peoples who govern themselves there is but a small field of usefulness open for the men of cloistered life who shrink from contact with their fellows. Still less room is there for those who deride or slight what is done by those who actually bear the brunt of the day; nor yet for those others who always profess that they would like to take action, if only the conditions of life were not what they actually are.” – Teddy Roosevelt, 1910"
  },
  {
    "objectID": "posts/2015-01-23-using-lego-to-teach-data-visualization/index.html",
    "href": "posts/2015-01-23-using-lego-to-teach-data-visualization/index.html",
    "title": "Using Lego to teach data visualization",
    "section": "",
    "text": "Today in my data visualization class, I made students visualize meaningful differences between this year’s Super Bowl teams, the Seattle Seahawks and the New England Patriots. Except I made them do it with Lego. \nA little silly, yes, but I wasn’t just gilding my Professor of the Year application, I swear. There was a purpose. \nThe first time I taught this class, I steered too hard into tools and code. We spent a little time on the history, theory and thinking behind data visualization and lot of time on teaching tools and later trying to cram enough d3 at them to make something. The problem was, in the race to get to the software tools, a lot of mental tools that would have helped them make better graphics didn’t get talked about. \nSo this time, I pledged to do it differently.\nIn the class – syllabus and other materials are online – we’re using Alberto Cairo’s The Functional Art and Edward Tufte’s Visual Display of Quantitative Information as texts. Today, we were talking about the form and function of graphics, about Cairo’s argument that the function limits the form. Cairo argues that the data, the questions the designer means the reader to ponder and the reader herself dictate the form, or at the very least constrain it to a limited set of choices. \nThat was the point I was trying to drive home: constraints. All journalism is constraint. Time, information, data, money, sources, access. So given a task and a constraint, what could you do?\nBring in the Lego.\nUsing squares and rectangles, I wanted them to make something. Visualize some data. We haven’t talked tools yet, so this would get them in the game without having to touch their laptops. \n\nIf you want to steal this idea, you need to do the following: \n\n\nMug an 8-year-old. I conveniently found one in my house and shook him down for a few hundred bricks and two large flat square pieces to act as a base. You may not have one so close. The number of bricks needed ended up being less than 100, so it doesn’t take much. I also swiped a couple of packs of sticky notes from the college office to act as data labels. \nChoose a topic or dataset that allows for some exploration and decision making. I chose the Super Bowl because it was fun and there is no shortage of stats in easily obtainable form online. I also thought – mistakenly – that football stats would be easily understood. I was surprised by the number of students who didn’t follow football that closely. \nGet out of the way. I split the class of eight in half and told them I wanted them to visualize meaningful differences between the teams. That’s it. They immediately set about discussing what they would visualize and how they were going to make the bricks mean something. The discussions about scale and how to make a brick represent X touchdowns or Y yards were fantastic. \n\n\n\nTo be clear, I wasn’t the first to do this, and one news organization did it as an election results tracker. But I’ll do it again. Class was energized, ideas were flowing and it was fun. Not a bad way to learn."
  },
  {
    "objectID": "posts/2008-01-28-thoughts-on-everyblock-and-context/index.html",
    "href": "posts/2008-01-28-thoughts-on-everyblock-and-context/index.html",
    "title": "Thoughts on Everyblock and context",
    "section": "",
    "text": "Note: This post came from a version of this blog that got lost in a server failure. It’s been restored from old RSS feeds, Google caches and other sources. As such, the comments, links and associated media have been lost.\n\nFirst, the grains of salt:\n\nI don’t live in an EveryBlock city. I think a key part of EveryBlock is the visceral connection you have with your neighborhood. I don’t have that, so view my comments with that in mind.\nHuge Adrian Holovaty fan. Huge Django fan. Big believer in breaking out of the story centric worldview.\n\nAnd now let me get this out of the way: EveryBlock is not something newspapers should fear. Here it is. I was wrong. I wrote that last year. Now, I don’t think newspapers need to fear this. Study it? Emulate parts of it? Learn from it? Absolutely. Fear it? No. Even if EveryBlock becomes crazy popular — and I think it will — it points to your content. If anything, EveryBlock will help people get to your content that interests them.\nOkay. I feel better now. On to the meat of the post.\nIs EveryBlock a data ghetto? This is tough. I can see why some would read my data ghetto post and think yep, EveryBlock is a data ghetto. I’ve honestly held off writing this post until I could answer the question for myself. It’s not completely clear cut, but I don’t think EveryBlock, as it stands now, is a data ghetto.\nWhy?\nI’ve seen several people say that EveryBlock is data without context, but that’s not entirely true. The context comes from the user through geography. The block is the context. The value you put into that context is based entirely on the fact that you live there. And, from that powerful context, the user provides further context by choosing what’s next, what is interesting to them.\nWhat also gets EveryBlock out of the data ghetto, outside of the whole geographic construct that almost no newspaper.com’s use, is that in some places EveryBlock provides it’s own context via graphing and counting instances of a thing. Compare that to data ghettos as I classified them: couple of search boxes and a results page. I’ll add another data ghetto indicator to the pile: single subject searches. On most newspaper.com data ghettos, you can search real estate transactions or crime - you can’t do both at the same time. Few have taken the next step of putting data together into one place.\nBut, but, not that context, this context: I think there are some legit complaints about context in EveryBlock. But I also think calling it data without context, perspective or meaning is wrong. The question is what context? And whose? That is a deeply complicated issue.\nMark Schaver points to the pothole paradox (short version: your pothole is interesting to you, the one not on your commute could not be any less interesting). So distance is important — the closer, the more you’re interested. But even that’s not that simple. If my neighbor has his lawn mower stolen, I care a lot. Two blocks away? Meh. But if someone in my neighborhood is murdered 5 or even 10 blocks away, I care a lot. It doesn’t have to be all that close. Same with building permits. I kinda care if my neighbors are pulling permits to build a garage or a bathroom. Much beyond that, I don’t. But if Wal-Mart is building a Super Center one or two neighborhoods over, I really care.\nSo I think the journalist’s complaint about context in EveryBlock — and it’s valid if you’re thinking like a journalist — is that there’s no mechanism to make one data point stand out from another. The example Schaver pointed to was the death of Heath Ledger. In New York’s EveryBlock? Yep. But on the map view, deaths are just another dot. Hollywood stars, homeless people, crack dealers — just another dot.\nNot all dots are created equal. And there are legions of factors as to what makes one more interesting than another, and another legion of factors as to why some things are more interesting to different people. Interestingness — or News Value — is an exponentially complicated equation.\nTo my mind, that EveryBlock doesn’t try to set some subjective one-size-fits-all standard of importance for data is proof that Adrian isn’t lying when he says EveryBlock is a supplement, not a competitor, to traditional news sources. Pointing out Important Stuff and making a Big Deal Out Of It is what newspapers do. EveryBlock doesn’t even try.\nThat said, I think the next step — for EveryBlock, for newspaper data ghettos, whoever — is personalization. Imagine if EveryBlock took your home address and a list of the things you cared about and displayed data with some kind of distance/importance weighting algorithm. Not everything, like now. Just a good guess at what’s important to you based on the distance from your home and how important you said certain things were to you. And done in such a way that you can tweak your own settings. Just have a kid? Might be time to move the schools slider up. And while you’re there, move the restaurant review sliders down. Trust me. With a new baby, you won’t be going out for a while.\nEven switching to some kind of straight distance algorithm solves one problem with EveryBlock: Just because it didn’t happen in your neighborhood/block/zip code doesn’t mean it wasn’t near you. What if you live on the edge of a neighborhood — one block over is Whispering Pines but you’re in Whispering Oaks? If someone is murdered one block away, but they’re in Whispering Pines, do you care? Of course you do. It’s one block away. Using a distance algorithm, you aren’t hemmed in by boundaries or forced to check more than one neighborhood — no matter how easy it is — to be sure you’ve seen all you want to see.\nDon’t take anything I’ve written to mean I don’t think EveryBlock is amazing. It is. I’m blown away by it. I’m humbled by it — first thing I thought when I was going through it was “crap, I need to work harder/sleep less/take brain steroids if this is the bar being set.” I just want to be clear that for EveryBlock and for anyone doing news data apps, context is an extremely important factor … and extremely complicated."
  },
  {
    "objectID": "posts/2014-12-09-everything-i-know-about-data-i-learned-from-70s-album-rock/index.html",
    "href": "posts/2014-12-09-everything-i-know-about-data-i-learned-from-70s-album-rock/index.html",
    "title": "Everything I Know About Data I Learned From 70s Album Rock…",
    "section": "",
    "text": "Everything I Know About Data I Learned From 70s Album Rock Radio, my Newsgeist 2014 Ignite talk."
  },
  {
    "objectID": "posts/2011-10-14-journalism-students-vs-tech-focused-students/index.html",
    "href": "posts/2011-10-14-journalism-students-vs-tech-focused-students/index.html",
    "title": "Journalism students vs. tech-focused students",
    "section": "",
    "text": "Here in my first semester on the faculty at the Harvard of the Plains, I get to work with students in both the College of Journalism and Mass Communications and the Raikes School for Computer Science and Management. With both groups of students, I’m working through problems that can be described as technology+journalism = ??\nWhen confronted with a code challenge – HTML, JavaScript, Python/Django whatever – I have noticed student reactions can be classified thusly:\nJournalism student: I can’t do this. This is impossible. I’ll never get this. \nTech oriented student: I can’t do this now, but learning how is just a matter of time and effort, so no worries.\nAt first, I just blew off the journalism student’s reluctance as a different strain of the “journalists can’t do math” disease that has never been cured. But the more I think about it, the more I worry.\nI strongly believe that the future of journalism requires journalists who can program – who can work in, construct, manipulate and advance digital distribution of content and information. The future of the industry needs technologists as well, probably more than technologists need journalism, but my focus is getting journalists elbow deep into the tools that will shape their future.\nI’ve always hated the newsroom culture of almost celebrating math ignorance. For an industry that prides itself on being smart, to turn around and glorify ignorance is dumb. And to continue to push that self-selection just calcifies ignorance and weakens journalism.\nBut if that same math-is-hard self-selection extends to code, then we’ve got a serious problem.\nThe question on my mind is what to do about it? How can we bring a little more “just a matter of time” swagger to journalism students?"
  },
  {
    "objectID": "posts/2013-01-26-writing-stories-with-code/index.html",
    "href": "posts/2013-01-26-writing-stories-with-code/index.html",
    "title": "Writing stories with code",
    "section": "",
    "text": "There’s a lof of interest and attention right now around the idea of computers writing stories from data. As newsrooms shrink and business models implode, managers are casting about for anything that can keep the pages filled/updates flowing. Stories about automated news all seem to ask the straw-man question: “Can software replace humans?”\nI have two thoughts about this:\n\nSoftware bots will never be able to write the most compelling stories, because telling a story is an inherently human act that requires real humanity to do well. There is no algorithm for humanity.\nIt is trivially simple for a software bot to write mundane, data-based (as opposed to databased) stories that fill a lot of news sections these days. For boring, grind it out, have-to-do-it-but-no-one-wants-to stories, a bored developer can bang out a bot that’ll write a decently nuanced story on that topic using ultra-basic programming logic in a day.\n\nHow trivially simple? How ultra basic?\nLet’s write a software bot that can write the annual “Crime is up/crime is down” story from the FBI’s Uniform Crime Reports release. It’s a simple story to write and very little changes from year to year, other than a few numbers and a couple of quotes. \nTo do this, I’m going to use Python and a simple list of data I got from the UCR’s data tool. \nFirst, I need a list of data to iterate over. For this example, I’m actually using a Python list. Most of the time, you’d have an array of objects from a database or a row of data from a csv. It really doesn’t matter, but here’s what I’m starting with.\ncities = [[“Beatrice Police Dept”,“NE”,433.7,281.4,280.9], [“Bellevue City Police Dept”,“NE”,159.5,125.2,139.6], [“Columbus Police Dept”,“NE”,107.3,69.3,122.1], [“Fremont Police Dept”,“NE”,209.0,130.8,189.4], [“Grand Island Police Dept”,“NE”,417.6,486.1,346.2], [“Hastings Police Dept”,“NE”,188.8,204.1,132.5], [“Kearney Police Dept”,“NE”,219.9,204.8,201.4], [“Lavista Police Dept”,“NE”,58.5,52.0,101.5], [“Lexington Police Dept”,“NE”,255.6,207.1,303.0], [“Lincoln Police Dept”,“NE”,509.6,457.9,486.9], [“Norfolk City Police Dept”,“NE”,147.2,148.5,181.7], [“Omaha Police Dept”,“NE”,605.6,533.4,556.0], [“Papillion Police Dept”,“NE”,61.3,77.9,142.9], [“Scotts Bluff Police Dept”,“NE”,375.0,358.8,232.7], [“South Sioux Police Dept”,“NE”,124.9,134.1,142.3]]\nAs you can see, it’s a list of cities in Nebraska with the violent crime rates for the three most recent years (in this case, 2008, 2009 and 2010). \nThe most common form of story a journalist can write is called the inverted pyramid – most important thing first, second most second, third most third, and so on. So, lets use that common structure to write our story. So what is the most important thing? I’d say it’s the trend. Is crime up or down? That would make our lead something in the form of “(city) police reported (more/less/same) violent crime in 2010 than 2009, according to federal statistics.”\nSo, how do we write that in code? Well, like anything, there’s a lot of ways to do it. I’m sure companies that do this for a living are using much, much more sophisticated methods, but it doesn’t require it. Take a look:\n#first, import the string library that we’ll need later and loop through our list of cities\nimport string\nfor city in cities: \n    #clean up the city name \n    clean_city = city[0].replace(” Police Dept”, ““) \n    #Lets get rid of that City business in a couple of names\n    clean_city = clean_city.replace(” City”, ““) \n    #determine the year over year trend \n    if city[4] &gt; city[3]: \n        direction = “more” \n    elif city[4] &lt; city[3]: \n        direction = “less” \n    else: \n        direction = “the same” \n    #write the lead\n    lead = “%s police reported %s violent crime in 2010 compared to 2009, according to federal statistics.” % (clean_city, direction) \n    print lead\nWhat do you get? Something like this:\n\nBeatrice police reported less violent crime in 2010 compared to 2009, according to federal statistics.\n\nBellevue police reported more violent crime in 2010 compared to 2009, according to federal statistics.\nColumbus police reported more violent crime in 2010 compared to 2009, according to federal statistics.\nAward winning? Hardly. Gripping narrative? No way. But, with a national dataset, I just wrote a lead for every city in America. And it would take less than a second to do so. \nSimple, right?\nWell, you might be asking, how hard is it to keep going? Well, let’s do that in the next post. Let’s change the lead based on a longer term trend. If you’re interested in the code, it’s here."
  },
  {
    "objectID": "posts/2011-09-27-me-vs-130-journalism-101-students-the-epic-qa/index.html",
    "href": "posts/2011-09-27-me-vs-130-journalism-101-students-the-epic-qa/index.html",
    "title": "Me vs. 130 Journalism 101 students: The epic Q&A",
    "section": "",
    "text": "A few weeks ago, I was invited to speak at the University of Nebraska-Lincoln College of Journalism and Mass Communication’s Journalism 101 class taught by Professor Scott Winter and the dean of the college, Gary Kebbel. Thinking it was just a simple come, talk, answer questions and leave, I was surprised to find that they had all come with questions and, while I was talking, were texting them to a single Google Voice account for me to answer. Holy Inbox Disaster, Batman. So it’s taken me a bit to dig out, but here’s my answers, for anyone who cares. I included their questions as written (texted) and lumped them together when they were all asking about the same thing.\n\nQ: I was creepin on ur blog and i was wondering, what’s up with ur blog?? / Why is your resume not posted on your site? / Your bio on TampaBay.com says you’re a News Technologist\n\n\nA: My personal website is a disaster of epic proportions. When you spend every day building websites, the last thing you want to do on your own time is build more websites. Maybe now that I’m doing something else I’ll clean the mess up. As for Tampabay.com, that’s an interesting problem on the web. As you build an online reputation and get involved in things, you leave a trail of biographies around that you lose control of when you leave a company or organization. I’m not the only one who no longer works at the St. Pete Times who has a bio that says otherwise, and I can do little about it (and News Technologist was my title when I was there).\n\n\nQ: What’s a bogus piece of advice aspiring journalists think they should follow? / What is your best advice for students like us going into journalism, advertising, PR? / Is it at all possible to be a journalist apart from the digital world? (not making websites, constantly using twitter)\n\n\nA: Bogus advice? Believe you can be a  journalist apart from the digital world. Look at yourself. How often do you check email/Facebook/text messages/Twitter/Friendster/Orkut/MySpace/Pownce every hour, including when you’re in class. Think of how much of your life is spent on a computer or on a mobile device. And then realize you are the news consumer of the future. Is it even possible to be a journalist apart from the digital world? I suppose it arguably is – even the smallest town newspaper uses email, but may not have a website. But you would have to work – actively put out effort – into removing yourself from the digital world and be a journalist. That’s not a world you want to be a part of, and it’s willfully ignorant of every signpost we have of where life is going.\n\n\nBest advice? Don’t. Waste. Your. Undergraduate. Education. Recognize this: Journalism – all forms – is going to be in turmoil for years. Traditional business models will be under assault if not outright destroyed. So, the future belongs to those who do, who think differently, who have new and unique skills. Take entrepreneurship classes. Better? Start a business. Take more statistics than are required. Take computer science classes. Do not major in journalism and minor in journalism and journalism, er, English and Sociology (like, ahem, someone). Take classes that will prepare you for an industry that is aching for people with an entrepreneurial spirit who can work with the web on a fundamental level and visualize how to make ideas into a part of a business.\n\n\nQ: What would you say gave you your start?  Did social networking play an important role?\n\n\nA: The Daily Nebraskan gave me my start. And social networking at the time was called “talking to people.” I started college when dialing into AOL was voodoo devilry no one understood (1993). I would tell you that social networking plays an important role in my career now, and I’ve seen people who developed a name and a reputation on social networking sites get jobs because of it. So social networking is a great way to make a name for yourself, but it’s not always a path.\n\n\nQ: Now that you have a Pulitzer, what motivates you? Was that motivation before?\n\n\nA: I always dreamed about winning a Pulitzer, but I always thought it was just that – daydreaming. I never thought I’d be a part of something that would win one. Even after I learned PolitiFact had won, it still didn’t feel real. Still doesn’t in a lot of ways. My motivations have always been odd – I get bored easily, I read constantly, I can’t stand being mediocre at something I care about and I distrust well worn paths. So I find something new that few are doing and I figure out a way to do something interesting with it. I believe in journalism and it has given me license to tinker around with some really cool stuff in the name of moving the ball forward, so to speak. Being at a university is going to let me experiment with applying wildly different ideas to journalism to see what it reveals about our future.\n\n\nQ: What made you want to come back to lincoln and teach? / Why would you come back to UNL as a professor? / What made you decide to start teaching after all you have done? / How do you like being a professor rather than a journalist? / Why did you decide UNL? / Why did you decide to come back to UNL when the other things you’ve done are so exciting compared to Nebraska? / What made you want to be a professor when you already had this great website that won the Pulitzer prize? / Why choose to teaching students to overcome your success than to keep building onto your success?\n\n\nA: First, this is home for me and my wife (who is also an alumna of the college). We wanted our children to grow up near family in a place with great schools and in a state that values education. Florida has none of those things. So moving back to Lincoln was for family and personal reasons. Why come to UNL? It’s been obvious to me, traveling the country, working with all kinds of media organizations, that there is a giant chasm between what the industry needs to get to a digital future and the workforce that’s out there. To be blunt: There’s not enough people like me, who are journalists or passionate about journalism who can code, analyze data, build websites, think beyond a story and think product, think business model. So, I could sit back and complain about it, or I could do something about it. Around the time I was thinking of this, I met Dean Gary Kebbel for the first time. We had beers a few times and got to talking. He’s thinking along the same lines I am – or a least he said he was after I got a few beers in him – and so it seemed like a good fit. \n\n\nQ: Is the issue with fact and truth due to people’s poor media literacy or just apathy? / Would you agree that todays society is not as concerned about hearing true facts - especially the political viewpoint - as they should be? / With the evolving media convergence - and the new voices appearing every day - do you fear that truth will get lost in the shuffle? / Do you find fact checking to be a weapon against cynicism about the media? / Do you feel that your fact checking has changed the political scene? / In your opinion, how do you think we can increase the awareness of citizens under 30 in current events and strengthen their participation?\n\n\nA: Yes.\n\n\nOh, you want more? I think you’re all right. Poor media literacy, a political climate created by leaders who want people to act on emotion instead of fact, an explosion of voices and streams of information – this is the world we live in. Is fact checking a weapon against cynicism in the media? Yes, but it feels like shooting a shotgun at a tsunami. Did you hit it? Yes. Did it do any good? Not hardly. Has PolitiFact/Factcheck etc. changed politics? In some ways yes, in the most visible ways no. And how do we increase awareness of those under 30? I think you have to do things differently. The traditional news broadcast and newspaper story aren’t working. Jon Stewart is one way, but I really want to believe there’s more ways to do this out there. And this is what I mean by the future belongs to those who do. Who says you can’t be the one to build something that reaches people and makes them better informed?\n\n\nQ: Do you enjoy covering politics? / Were you always a political reporter or did this just kind of happen, and why? / How did you come up with Politifact.com and how do you regulate it?\n\n\nIn the newsroom of the St. Pete Times, there is a quote board. On that quote board is one from me, telling the world that “I would rather serve fries” than cover politics. I have not ever been nor ever will be a political reporter. I hate politics. Can’t stand politics. But that’s exactly why I was interested in building PolitiFact. If I could build a political website that would interest someone like me, who gets angry whenever a political talk show comes on, then I thought you’d have something.\n\n\nQ: How do you decide what statements get attention on Politifact and which ones get left behind? / How big is your staff to keep politifact running and reputable? / Who does the research for politifact? / Where do you get the “truth” from? The source itself? / How did you create such an extensive network of data that you could insure that the facts you presented were facts? / Where do you check the facts that you post on the website?\n\n\nA: Editors listen for things people are talking about, facts and talking points that rise into the national conversation, and have a verifiable fact in them. So long as there’s a verifiable fact in there – “Candidate X is ruining American values!” isn’t a verifiable fact – then they go after that. We’ve said from the beginning that it’s a subjective process, which is why you don’t see us producing “Who lies more” stories. We don’t know. You can’t know based on our numbers, because we don’t fact check everything someone says, or even a representative sample. It would be impossible to do that. So we have to pick and choose. There’s five people on the national staff, plus researchers and other people who help out from time to time. There’s a pattern to how they research an item, and it starts by asking the person who said it for their source. A reporter then checks out that source – often calling the source and asking if they were quoted accurately. Often, they check with official sources – government documents, records or reports. Where things lead from there varies widely. But we have to be careful to go to original sources instead of relying on think tanks and organizations that have an agenda. \n\n\nQ: If you could alter one specific part of politifact right now what would it be?\n\n\nA: I’ve been advocating for a redesign for some time. It’s time. I want it to get cleaner and leaner, with some parts better emphasized. No major surgery on the functionality, just look and feel stuff.\n\n\nQ: Do people who you rate as not telling the truth ever contact the site trying to defend their statements? / Do you get criticized or questioned often about the accuracy of the information on politifact? / Why did you choose to focus so closely on Obama and judge him on his promises, and not follow those promises of others so closely? / Where do you get your research from to decide if the politics on your website are true or not? How do we know its reliable? / I’m fearful that politifact may not be a balanced website.  There’s no shortage of corruption on both sides.  Does the website reflect this? / Being the finder of falsehoods in the political world, did u find yourself leaning to one side after your research…?\n\n\nA: All of these questions are hinting at or coming out and asking about bias. And whenever anyone says bias, I want you to think very hard about why someone would declare something biased. Did it confront their carefully cultivated narrative? Did it challenge a belief? Is it actually biased or is this person just unwilling to consider other viewpoints or do they have a vested interest in you believing a specific thing? That said, yes, people defend themselves all the time and we’ve indeed changed rulings because of new information. Do we get criticized? Daily. It’s part of the game now in modern politics. Why did we choose to focus on Obama? We didn’t. There’s a GOP promise tracker too, right next to Obama’s. That said, he is the president, so a brighter focus on him is warranted. “Balance” is too often interpreted as “we quoted the left and the right equally.” That is a false concept, and one PolitiFact has said from the beginning they were not going to fall prey too. If the national conversation is dominated by a person, idea or talking point, we’ll fact check that regardless of what the other side is saying because it’s not dominating the national conversation. \n\n\nQ: What did you initially expect the outcome of politifact to be?\n\n\nA: That at best we’d make it to the end of the 2008 presidential election and have to find something new to do. I honestly thought we’d turn it off after the Florida primaries. But once it took off the first week it was online, I knew we were going to make it past the primaries. But I did not expect to celebrate the site’s 4th birthday.\n\n\nQ: What would you say to people who argue that journalism is a dying field?\n\n\nA: I would tell them that they’re looking at it all wrong. Traditional journalism – newspapers, cable news, etc. – are in serious trouble because of the internet, how people are using it, changes that are coming about because of it, etc. But at the same time, journalism as a practice is flourishing. There’s more choice, more voices, more journalism now than there has been at any time in history. It’s easy to just blow off journalism as dying because the metro daily is in deep trouble, but journalism has been around for a very long time. We’ve been telling stories to each other since we were living in caves and painting on walls. So journalism isn’t going anywhere, but the business models that fund it as a business are going to see tremendous change, destruction, upheaval and rebirth for the near future.\n\n\nQ: What was your favorite story to report on? Least? / What is the most interesting story you have ever covered? / What’s the most interesting story you’ve ever covered? / As an investigative reporter, what was your favorite story to write? / What is the most exciting story you have ever covered?\n\n\nA: I’ve covered thousands of stories. Picking a few is tough. The guy thrown out of an all you can eat buffet for eating too much was fun. Covering a shuttle launch was amazing. Tornadoes are great fun and a horrible grind all at once. Covering corruption at a small city hall was great fun too. Looking at the moon through the eye of a hurricane is something I’ll never forget. Getting to do cutting edge investigative reporting with tools no one had ever used before was awesome. The great part about being a journalist is that there’s always another story. \n\n\nQ: What is your theory on “quote” leads?\n\n\nA: Theory? Fact, my friend. Fact. Either you are in my 202 reporting class or word is starting to get out. Quote leads, I told my students, are stupid, lazy and should never appear in a newspaper except for two circumstances: The quotes are “We’ve cured cancer” or “We’ve made contact with an alien life form.” Short of that, your quote lead sucks.\n\n\nQ: What inspired you to create the website Tampa Bay Mug Shots? Can you make one for Nebraska? / How did you come up with the Tampa Bay,mug shots website? / Why did you create the Tampa Bay mug shots website and how is it ethical while keeping the families of the person in mind and their right to privacy? / What are the reactions that you’ve gotten from people about the mugshot website? Is the morality behind the website a consideration or a constraint?\n\n\nA: Frankly, I wasn’t inspired to build it – I was asked to do it by my boss. So we set out to build it the best way possible, both from the standpoint of the user there to look at mugshots and from the point of the accused. We started with the premise of we’re going to do this, that these are public records that the public has the right to see, so how do we do it in such a way that’s fair as we can be. So we took great care to make sure names did not get indexed in Google. It wasn’t fair for someone’s first result in Google to be our site because we’re good at Search Engine Optimization and they aren’t. So we blocked Google, and because we couldn’t know the outcome of each case, we deleted everyone after 60 days, which is about the average for a misdemeanor case to make it’s way through the courts. So guilty or innocent, we delete the record. That leaves the argument on if we should have done this at all, and I’ve just agreed to disagree with people on that. \n\n\nAnd no, I won’t build one for Nebraska. I’ve got too much other interesting work to go through that again.\n\n\nQ: Why did you decide to cover the topic and write essays dedicated to Florida’s Wetlands? / What made you decide to write about Florida’s wetlands? / In ur paving paradise u showd the illusion of conservation tht didn’t actually heed destruction of wetlands. How prevalent do u think this illusion module is? / What do you propose we do about the disappearing wetlands?\n\n\nA: How wetlands came about is kind of a theme of my career. I really had no interest in wetlands, and only a tangential interest in environmental journalism at all. But I walked into the Times office one day and was assaulted by a very excited Craig Pittman, the Times’ hugely talented environmental writer.  He had this great story about wetlands that he needed some data help with that could take it from a good story to a really great one. So we sat down and started looking at what we could do, and the challenges just kept getting more and more interesting. We went from your basic “get records and analyze” story to me taking graduate courses in satellite imagery analysis and doing things journalists had never done in a newsroom. That’s what interested me more than anything – the challenge. I was going to get that story if it killed me. \n\n\nHow prevalent do I think the “illusion of protection” idea is? I think it’s far more common than we know. I think there is a ton of good investigative journalism to be done looking at the gap between the rhetoric and the actual execution of laws and regulations. I think you’ll find that there’s a significant gap between what the public hears about how government regulation works and how it actually works on the ground.\n\n\nWhat do we do? Read Chapter 12 of the book Craig and I wrote: Paving Paradise: Florida’s Vanishing Wetlands and the Failure of No Net Loss. Available now on Amazon.com.\n\n\nQ: What inspired you to be a founder of a company? / As an aspiring business owner, what advise would you give to get a company started? / Can you tell us more about hot type consulting llc? What responsibilities do u have as a co-founder?\n\n\nA: A friend, Chase Davis, and I got to talking at a hotel bar during a conference about how we were picking up little freelance gigs here and there, so what if we put it together into a company. So, that’s what we did. We pretty much had work before we even got the paperwork together. Our first job was building the content management system for the Texas Tribune. We got so busy doing that I didn’t have time to get our paperwork done to incorporate us, so they couldn’t pay us until we did that. And that’s Hot Type in a nutshell. It’s Chase and I and all our friends in the business who want to make some extra money building custom web applications for media companies. We’ve worked with everyone from huge corporations to small startups. My responsibilities, apart from programming and getting work done are as the back office for the company. So I handle the accounting and legal work that comes up (contracts, etc.) as well as client development and anything else really. When you’re a company of two, every responsibility is your responsibility.\n\n\nAdvice for starting a business? Start. Get $1,000 together and hire an attorney and an accountant to handle the legal and financial startup paperwork and once that’s done, get to work. Get a website up, get some business cards and get out there and compete. Sitting around talking about doing it does no good. \n\n\nQ: Do you think a Gatewing X100 will actually be used for journalism in the future? Do you think it can withstand flying through natural disasters?\n\n\nA: I do think UAVs will be a part of journalism in the future, but only when they get cheap, plentiful and ultra brain-dead simple to use. Is the Gatewing that bird? No. It’s $65,000, as big as a large suitcase and more than your average reporter can handle. And, I think if you’re going to use it in disaster reporting, it has to be ultra portable (think backpack). So the Gatewing fails there too. But it’s so freaking cool I can’t help but be fascinated with it. And I think the applications for UAVs are all after the natural disaster is over, not during it. So after the hurricane has destroyed everything, use a UAV to assess the damage. \n\n\nQ: Are we doomed?\n\n\nA: Courage, my good man. Courage. Fortune favors the bold. Traditional models are in deep trouble, but innovative thinkers and hard workers will have a place in the future."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html",
    "href": "posts/an-academic-integrity-friendly-pal/index.html",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "",
    "text": "One of the struggles on campus these days is all about where to draw the lines when it comes to AI in the classroom. There’s no end of discussion about students using ChatGPT to cheat, particularly on writing assignments. How do you stop it? How do you adapt to it? How do you convince students to do the work?\nTeaching students to write code is no different. I add a layer of difficulty in that I teach journalism and sports media students how to code. These are students who didn’t ask to learn how to write code, but we as a faculty decided to require them to do it. Thus, they have incentives to cheat. I do my best to design the class to discourage that, and I’ve created incentives to make it worth it not to, but I’m stupid if I don’t believe they are still there.\nBut I’m also a bit dim if I don’t acknowledge that Large Language Models can help with learning how to code. The trick is, once again, where to draw the line.\nThe classes I teach are all data analysis in R using the tidyverse and R Studio as the IDE. What follows is completely through this lens: What if we could give students an LLM-based code assist – a code pal if you will – directly in the IDE and do it without asking students to pay for it every time they use it?"
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#getting-started",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#getting-started",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Getting started",
    "text": "Getting started\nThere’s a bunch of steps to get this set up and it’s going to take a decent chunk of your hard drive when all is said and done. Doing this also requires a decent amount of power. How much? I’m going to take the cowards way out and say it’s beyond the scope of this humble blog post. Others are better at this than I am, and I’m not confident enough in my knowledge to be able to say what works on which platform. I’m doing this on an M1 MacBook Pro with 16 GB of RAM. Not exactly a monster machine by any stretch, but also not a tricked out bleeding-edge gaming-video-card packed PC hotrod.\nStep 1: The first thing you need is Ollama. We’ll use that to download, manage and serve up our local LLM. Install it per your operating system. The LLM we’ll be using today is qwen2.5-coder. Once you have Ollama up and running, you can get qwen2.5-coder installed and running with ollama run qwen2.5-coder\nThat will install the 7B version – the 7-billion parameter model. That should run and give you decent performance on just about anything. If you’ve got more muscle, you might look at how to install some of the bigger parameter models. Generally, the more parameters, the better the results.\nStep 2: The next thing you need is pal, an R library that adds a way to consult an LLM inside R Studio. After installing it – you can use pak as the instructions show you or you can use devtools::install_github(\"simonpcouch/pal\") like I did because I haven’t gotten into the habit of using pak. Once installed, you can skip the parts about adding an Anthropic API key – unless you want to use Claude and have API credits to spend – and go to the Get Started article. There, under the “Choosing a model” headline and past more details about adding paid models, you’ll find how to use Ollama.\nThe least confusing way to do this, in my opinion, is to add this to your .Rprofile. In the R console, run usethis::edit_r_profile() and add this:\noptions(\n  .pal_fn = \"chat_ollama\",\n  .pal_args = list(model = \"qwen2.5-coder\")\n)\nNOTE: If you installed a bigger model than I did, you should specify which model you used in the the .pal_args. Note mine does not say what parameters I have. If you installed the 14b model, for example, your .pal_args should say “qwen2.5-coder:14b” instead of just “qwen2.5-coder”. Save that file and restart R Studio so they take effect.\nYou’re almost ready to get started. Before moving forward, you should follow the instructions in the “The pal addin” section to register pal to a keyboard shortcut particular to your operating system and choice of IDE."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#writing-your-own-pal",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#writing-your-own-pal",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Writing your own pal",
    "text": "Writing your own pal\nThe mechanics of writing your own pal could not be easier, thanks to the library. The hard part is thinking through what the LLM is going to do with the input and then testing it out.\nLet’s make an Academic Integrity Friendly pal that tries to create friendlier and more helpful error messages.\nIn the R console, run library(pal) and then prompt_new(\"whatiswrong\", \"suffix\")\nThe first part of that is the name you’re giving your pal, the second is where it’s going to put the results. You can use “replace” to … well … replace what you highlight. You can use “prefix” to put it above your code. And “suffix” puts it after your code. We want ours to act like an error message, so suffix makes sense.\nDo that and a markdown file will pop up. It’s templated, so it could not be easier to fill out. Here’s what I’m using to make my pal:\nYou are a terse assistant designed to help R users debug code. Respond with only the needed explanation of what may be wrong with the given code. Do not write code for the user, just explain in plain language.\nAs example, given:\ndf |&gt; filter(column_name = “word”)\nReturn:\nWhen using a filter, you must use == for equal to instead of =.\nSave it and then run directory_load() to get your pal in the shortcut menu."
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#using-your-pal",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#using-your-pal",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "Using your pal",
    "text": "Using your pal\nUsing your pal is now just a matter of messing up some code. Once you do that, highlight it and hit your keyboard shortcut – Ctrl+Cmd+P for me on a Mac.\nHere’s an example of what it looks like:\n\n\nI need to use it more to know if it’s going to be any good. I need to try it with crappier code and more complex errors. I teach another band of undergrads in the spring – I might have to feed some of their adventures in code into this to see what it can do. Also worth trying? Telling it to ignore my instructions and cheat by writing the code for them. Will it listen to the student or me? If it doesn’t listen to me, then what’s the point?"
  },
  {
    "objectID": "posts/an-academic-integrity-friendly-pal/index.html#a-note-on-equity-in-the-classroom",
    "href": "posts/an-academic-integrity-friendly-pal/index.html#a-note-on-equity-in-the-classroom",
    "title": "An academic integrity-friendly code pal for R Studio",
    "section": "A note on equity in the classroom",
    "text": "A note on equity in the classroom\nI’m lucky in that I have a relatively recent laptop with a decent amount of power provided by my employer. Do I want a newer faster one? Sure I do. Every nerd does the second a new one is announced. But I have a good enough machine to do this.\nNot everyone does.\nEvery semester, the first day of class, I assign all the installations they’ll need for the semester. Step 1 is update your operating system. Every semester, this assignment is an exercise in perspective for me. My college has a laptop requirement. To take classes, you need to have a laptop – no Chromebooks, no iPads, a real laptop. What it is, we don’t care, so long as it can run the Adobe Creative Suite. Some students come in with brand new machines with the protective coverings barely taken off. And then I get some that are held together with duct tape and prayer. Machines with keys missing. Machines that are 6 years old and never once updated. Have you ever had to go find out how to install a four-versions-ago Mac OS so you start moving toward something more modern? I have.\nAll this to say I’m talking about academic integrity here but I am not talking about academic equity. I can’t assign this. I can’t make this part of a class. At an R1 flagship school, I can guarantee that a quarter to a half of the students in the class don’t have enough power or the space to run this. It’s going to be far worse elsewhere.\nSomeday, maybe, but not today."
  },
  {
    "objectID": "posts/2013-09-03-the-minimum-viable-participant/index.html",
    "href": "posts/2013-09-03-the-minimum-viable-participant/index.html",
    "title": "The Minimum Viable Participant",
    "section": "",
    "text": "In any software development project, you have a line in the sand called the Minimum Viable Product. It’s the point where you’ve got it working well enough and with enough features that the thing has a chance. Barely. It’s not a goal or a standard, it’s a marker of progress.\nThere’s been a bunch of debate about what journalism schools should be doing now to change curriculum in the face of disruption in the industry. It’s a good debate. I agree with some, have criticisms of others and view this whole “cram more into a degree” issue as a challenge not a lament. \nBut I was thinking about this like a software product manager the other day. Really I was stealing a line from Brian Boyer. Who is our audience? What are their needs? And that got me thinking of what are we trying to produce. Is there a conceptual framework we can work toward? Build a curriculum around? \nIntroducing the Minimum Viable Participant. \nThe Minimum Viable Participant is the baseline for a student journalist to take part in the digital future. It’s the bare minimum of skills they will need to be a part of modern journalism, and what it will become in the future.\nSo what does the Minimum Viable Participant need to know? Well, this quickly turns into a Rorschach test for whatever particular flavor of digital media is your bag, but let’s take a stab at this. In no particular order, the Minimum Viable Participant in the digital future of media should…\n\nBe able to tell stories.\nKnow how the internet works at a fundamental level.\nKnow how to learn new things without formal instruction.\nBe able to capitalize on ideas and execute on them.\n\nIs that list perfect? No. I’m already debating it in my head. But it is utterly agnostic on the how, which I think is important. Saying the Minimum Viable Participant must know JavaScript means your list is worthless when something comes along and replaces JavaScript. How we tell stories is changing all the time, so specifying how to tell them is giving directions to Obsoleteville. \nThe Minimum Viable Participant needs to be curious, industrious and knowledgeable about how the online world works. They need to be these things so they can take advantage of it, or hack it, to get stuff done. They need to learn how to learn, especially in our modern times with MOOCs and free lessons and tutorial videos absolutely flooding the internet. I can’t tell you how many professionals have said we need to teach a class in how to search for the information you need. So, fine, let’s teach a class in how to learn. \nHaving an eye on the product – Who are our users? What are their needs? – should be the main basis of all curricular developments (though it often isn’t, which is a subject for another time). So before we decide what classes to teach, what are the answers to those questions? What does the Minimum Viable Participant in the digital future look like?"
  },
  {
    "objectID": "posts/2009-04-27-the-key-lesson-i-learned-building-politifact-demos-not-memos/index.html",
    "href": "posts/2009-04-27-the-key-lesson-i-learned-building-politifact-demos-not-memos/index.html",
    "title": "The key lesson I learned building PolitiFact: Demos, not memos",
    "section": "",
    "text": "So there was a little news around here lately. PolitiFact won a Pulitzer Prize. To say I’m still in shock is an understatement. A week later, it doesn’t seem real.\nAll week long, we’ve been talking about how PolitiFact started, how it all came together. It’s been fun remembering how it started out with Bill Adair having an idea and me having an idea on how we could pull it off. The crude mock-ups, the development environment on a box that was headed for the trash. I still can’t believe we did it. But out of the many lessons I learned on PoltiFact, one stands out. It’s become mantra for me.\nDemos, not memos.\nTo be clear, my bosses thought PolitiFact was a good idea from the start. But there was a material difference between how they reacted to memos and how they reacted to seeing it working.\nThe sales job got easier. The abstract became concrete. The conversations changed from “what do you mean by” to “what if we did this.”\nHere’s why I think demos, not memos works:\n\nIdeas are cheap and plentiful. Execution is hard. One of the best books I’ve ever read is The Myths of Innovation by Scott Berkun. One of the myths he explores is that ideas are everywhere. The trick is picking which ones to execute on because you have limited resources and limited time to do that. If you’re really passionate about your idea, building a demo makes yours stand out from the blizzard of ideas that are spawned every day.\nMeetings suck. My favorite game right now? I sit in meetings, guessing what everyone around the table makes, and then I figure out how much this meeting is costing. Then I figure out how many page views at so much CPM its going to take to break even on just that meeting. I’m pretty sure I’m descending into madness, but the point is this: a demo cuts down on meetings. You don’t have to spend time explaining what you want to do. You just show it. A demo is not a finished product – there’s still plenty of work to be done – but the meetings stop being about the idea and start being about execution.\nRequirements documents suck. If you’ve never participated in a software project, a requirements document is a list of everything your app has to do. They are usually written by people who aren’t developers and usually come out of meetings that have very little to do with what users want or is technologically feasible. What bothers me about them is that requirements documents remove large swaths, if not all, developer creativity from the process. PolitiFact succeeded technologically because the guy with the vision and the guy who could build it worked together. We went back and forth, both with ideas, iterating through the development. Form and function were blurred together. Requirements documents say here’s what we want, go build it and nothing more. Thinking that way means never being any better than your document.\n\nNow, does building a demo mean guaranteed success? No. Not even close. I can still count the number of not-this-blog sites I’ve launched on one hand. I have 14 projects that I’ve started building out as demos that have gone nowhere. Sometimes, an idea you think is great just isn’t. Sometimes seeing it up on the screen reveals that. Sometimes just trying to build it reveals what a giant hairy ball of pain your idea is going to be. But you won’t learn that from a memo. In a memo, everything is easy. Which is why demos matter."
  },
  {
    "objectID": "posts/2012-01-17-a-completely-arbitrary-list-of-takeaways-from-two-unconferences/index.html",
    "href": "posts/2012-01-17-a-completely-arbitrary-list-of-takeaways-from-two-unconferences/index.html",
    "title": "A completely arbitrary list of takeaways from two unconferences",
    "section": "",
    "text": "This past weekend, I attended Spark Camp: Data, an unconference in Austin focused on using data to tell stories, whatever they may be. A month before, I was at News Foo, an unconference at Arizona State University that brings technologists and journalists together to talk about … whatever they want to talk about regarding the future of news. Both conferences included a lot of chatter about journalism schools and what they should be doing. People I talked to were all fascinated to hear I teach programming and data at a journalism school.\n\n\nAt both, listening to this discussion going on, I came away with some random thoughts about journalism school curricula, programmer-journalists and the future. Here they are in a completely unnecessary and arbitrary list.\n\n\nThe number of things Journalism is asking its journalism schools to teach could fill three degrees plus a couple of minors. Business, law, economics, entrepreneurship, computer science, data science, and also all the journalism fundamentals. We have no idea what The Future is, other than that it’s wildly different from the past, so we’re tossing everything into What Journalism Schools Should Be Teaching and the list is starting to look a little silly. Especially when you consider we have 40 credit hours to work with.\n\n\nI view this as a challenge, not a lament.\n\n\nYou are number 114 on the list of people who have asked me if I have any students who are budding journalist-developers ready to start busting out apps in your shop. You are also deep on the list of people telling me you’re looking for people and having a hard time finding them. There’s 10 fish in this pond right now and everyone has a line in the water.\n\n\nThe number of students I have seen who are budding journalist-developers ready to start busting out apps in your shop: 0. Why? Comes down to passion. I haven’t seen that student take what we talked about in class and run off on their own. It seems they’re still waiting for something. I don’t know what that is.\n\n\nWhere are these future journalist-developers who will Save Us All? What can we do to find them? I don’t know. I think more about it every day.\n\n\nI think the problem with finding these students starts with reward structures. Students are told from even before they walk on campus that being a journalist means Being a Good Writer, Being a Good Editor, Being a Good Photographer. No one is telling them they could be an application developer, or a data journalist, or a media entrepreneur. Or if they have heard it, that voice is getting drowned out by traditionalists. A disturbing amount of time, the traditionalists drowning those students out are other students. Until we can attach a reward to this – until it cracks the consciousness of students that there are jobs in this path – I think we’ll continue to struggle. \n\n\nI still believe you can teach journalists to be programmers and people like that will be vital to the Future of Journalism, but I’m also starting to think more and more about what a journalism minor for a computer-science major would look like. \n\n\nA potential archetype of a journalist-developer student? That kid who messed around with programming in high school and loved it while having a blast on the student paper. But they came to college and thought they had to get a CS or similar degree because that’s what They told them. You know They. They tell people a lot of crap. I’m looking for that techie carrying regret in their heart for not pursuing journalism. I can unburden that regret. And get you a job.\n\n\nThe pipeline of techie/journo students to internships to jobs is a problem that is going to be with us for a while. There’s not enough students right now. Plain and simple. And I worry that because of this supply problem, the internships and jobs will go away. And just when that happens, we’ll solve the supply problem and have the reverse.\n\n\nProblems I would love to have to talk about: What is the career path for a developer in a newsroom? There isn’t one right now. Who will be the first to hire a developer as an assistant managing editor or above? I ache for the day we have to discuss this instead of the scarcity of talent. I long for the day when we have to debate turning over editorial strategy to someone who came into the newsroom to build apps. That will be a great day."
  },
  {
    "objectID": "posts/2007-08-22-announcing-politifact/index.html",
    "href": "posts/2007-08-22-announcing-politifact/index.html",
    "title": "Announcing PolitiFact",
    "section": "",
    "text": "Note: This post came from a version of this blog that got lost in a server failure. It’s been restored from old RSS feeds, Google caches and other sources. As such, the comments, links and associated media have been lost.\n\nIt’s been quiet around here for a while. There’s a good reason. It’s called PolitiFact and it marks a major shift in my career.\nWhat is PolitiFact?\nThe site is a simple, old newspaper concept that’s been fundamentally redesigned for the web. We’ve taken the political “truth squad” story, where a reporter takes a campaign commercial or a stump speech, fact checks it and writes a story. We’ve taken that concept, blown it apart into it’s fundamental pieces, and reassembled it into a data-driven website covering the 2008 presidential election. The whole site is inspired by Adrian Holovaty’s manifesto on the fundamental way newspaper websites need to change. Adrian’s main theme was that certain kinds of newspaper content have consistent pieces that could be better served to the reader from a database instead of a newspaper story. I built PolitiFact with that in mind.\nIf you think about it, a statement from a politician has consistent pieces. It has a speaker, that speaker has a political party, the statement has a subject and a forum in which they said it. After we fact-check that statement, we assign a ruling to it, summing up the veracity of what they say on a scale from true to false to pants on fire (a personal favorite). All of those things become fields in a database. Statements are all in their own database, and we also are writing more traditional stories on the statements. And those articles have some common pieces, like a byline.\nWe’re experimenting with greater transparency, by listing our sources for each statement and story. We’re taking into account that YouTube is going to be a significant factor in this election by embedding YouTube videos of statements when we can find them.\nPolitiFact was born when St. Petersburg Times Washington Bureau Chief Bill Adair called me in very late May with an idea he had. He wanted to take the “truth squad” idea and expand it. And he wondered if we could somehow use databases with this idea. He didn’t know how we could do that, just that we should, and that was why he was calling me. I was knee deep in learning Django, the rapid development web framework, and immediately knew we could use Django to make this happen. Based on our conversation, I quick sketched out a series of related tables — models in Django parlance — and PolitiFact was born.\nLearning Django has been a transformative experience for me. PolitiFact is the first Django app I’ve completed, and it won’t be the last. Not even close. Before this, I’d never developed a website before — I don’t count installing WordPress on a hosting account as developing a website — or done anything in Python. Learning Django was a challenge for someone like me with no programming experience, but the framework puts incredible abilities into your hands once you learn what you are doing. The documentation is a truly remarkable resource: It is a monument to it’s quality that 98 percent of PolitiFact comes from the documentation.\nBeyond being an experiment in journalism or web development, PolitiFact is an experiment in entrepreneurship. We’ve developed a product that uses reporting labor from the St. Petersburg Times and our sister company Congressional Quarterly to create something that doesn’t originate in print. All the talk and all the focus lately in web journalism circles is on local, local, local and to some degree they’re right. But there’s also something to be said for just putting a good idea on the web that people might find useful. We think we’ve done that. Now the important part: how are people going to respond? We have no idea. We’re anxious to find out.\nCredit where credit is due: I took Bill’s idea and translated it into the Django framework, but there’s a lot more that went into PolitiFact than that. I stopped learning HTML in 1998, so all the credit for how PolitiFact looks belongs to Martin Frobisher. From the Times IT department, Charles Goddard has shifted from being a java nerd to a python whiz with alarming speed; Dave Brown has built an infrastructure that boggles my mind (18 million page views an hour!); Ed Nicholson’s energy has been invaluable; and we’d still be dead in the water if Tim Aston hadn’t taken a leap of faith to let me run with Django on little more than a pitch and an ugly demo model. From the news side, Bill Adair has been equal parts salesman, cheerleader and evangelist, and PolitiFact would have withered and died long ago without his commitment. And I would never have been given the time to make this work if it weren’t for ME/Web Christine Montgomery and Executive Editor Neil Brown not listening to my technobabble and seeing the end product.\nI’ll be writing a lot more about PolitiFact, Django, and how this has all forever and fundamentally changed my thinking on journalism. Stay tuned."
  },
  {
    "objectID": "posts/2013-05-17-a-new-way-for-data-journalists-to-thwart-newsroom-it-the-raspberry-pi/index.html",
    "href": "posts/2013-05-17-a-new-way-for-data-journalists-to-thwart-newsroom-it-the-raspberry-pi/index.html",
    "title": "A new way for data journalists to thwart newsroom IT: the Raspberry Pi",
    "section": "",
    "text": "One of my old jokes is that newsroom IT puts the No in Innovation, so I’m always on the lookout for ways to get around them. And I’ve been playing around with a good one: The Raspberry Pi.\nUnfamiliar with the Pi? The Model B Pi is a $35 computer that’s about the size of a deck of cards. It’s got an ethernet port, and you supply the hard drive in the form of an SD card, the keyboard, mouse and monitor. Now, for $35, you’re not getting a ton of horsepower, but for simple repetitive tasks it works great.\nWhat kind of simple, repetitive tasks? Let’s pretend for a second that you wanted to set up a scraper that dumped data into a database every hour. Ideally, you’d have a server somewhere and you’d set up a task on it – I like using ’nix’s cron for things like this – and off it would go, mindlessly gathering data for you and putting it into a database. You could then go about your life, stopping by from time to time to get that data and do whatever you’re going to do with it. So you ask newsroom IT for this and, of course, the answer is no. And no we won’t give you the money to run this in the cloud for a few bucks a month either. \nEnter the Pi.\nFor $35, you can write your scripts, put them in a cron job and off it’ll go, gathering your data for you. No need for a server, no need for a server administrator, no need to make sure your work computer stays on and running the whole time, just some elbow grease to get the script running and an ethernet connection to the internet. \nI’ve had my Pi running a repetitive task for two weeks now and it’s plugging along without issue, having gathered 50,000 records without me having to do anything. In a month, I’ll have a dataset worth analyzing, and it will only ever cost me $35. And I can use it for other things as well. \nA cheap scraper bot. Useful!"
  },
  {
    "objectID": "posts/2014-03-31-unl-cojmc-students-two-dataprogrammingfuture-courses-for-the-fall/index.html",
    "href": "posts/2014-03-31-unl-cojmc-students-two-dataprogrammingfuture-courses-for-the-fall/index.html",
    "title": "UNL CoJMC students: Two data/programming/future courses for the fall",
    "section": "",
    "text": "If you’re a UNL CoJMC student looking for courses in the fall, here’s two I’m teaching you should take a look at:\nJOUR407: Investigative and Computer-Assisted Reporting: This class is all about data journalism – emphasis on both parts of that phrase. I’ll show you how to incorporate data and data analysis into your stories, regardless of format. You’ll learn techniques that will help you break a dataset down and interview it like a source. We’ll get our hands dirty with data and we’ll turn those into stories. We’ll add some data visualization to the toolbox as well. The not-close-to-complete syllabus is here. \nJOUR491: Intro to Storybots: More and more, publishers are using software bots to write stories. They cover baseball games and earthquakes. The dirty secret is that basic bots are not hard to write. It takes basic programming knowledge and basic journalism knowledge. Combine the two and you can write thousands of stories in a matter of seconds. In this class, we’ll learn how to make our own storybots, we’ll ponder the nature what it means to be human and where this is all going in journalism. Want a taste? Read this. I’ve put the very first sketches of the class online here."
  },
  {
    "objectID": "posts/2013-05-01-adventures-in-prototyping/index.html",
    "href": "posts/2013-05-01-adventures-in-prototyping/index.html",
    "title": "Adventures in prototyping",
    "section": "",
    "text": "Things we have done today in my office:\n\nStolen two cups of dirt from a construction site.\nMade a pot of mud with one cup of dirt.\nMicrowaved the second cup of dirt in the faculty lounge.\nMeasured the the sensor output of totally dry dirt versus a soaking wet pot of mud.\nUsed the point-slope form of linear algebra to determine the formula for the line between dry and wet. In a journalism school. And it worked.\n\nMore about this later, including what it’s all about and code."
  },
  {
    "objectID": "posts/2013-07-15-replacing-reporters-with-robots-i-might-be/index.html",
    "href": "posts/2013-07-15-replacing-reporters-with-robots-i-might-be/index.html",
    "title": "Replacing Reporters With Robots I Might Be",
    "section": "",
    "text": "Replacing reporters with robots? I might be trolling a little, but robotics and programmatic bots are going to play a greater role in reporter’s futures than you might think."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "",
    "text": "I do not know a lot about college basketball. I follow the travails of my employer and a little about the Big Ten Conference as a whole, but at best it’s surface knowledge. I kinda know who is good that we’re going to play and who isn’t. Beyond that, nada.\nWhich is bad when it comes to tournament time.\nMy typical pattern of filling out bracket is Have I Heard Of This Team, Do They Have a Legendary Coach or Do I Hate Them For Some Reason. Depending on the answers, I make my pick. It’s not rocket science, and it rarely works.\nThis season, along with my SPMC460 Advanced Sports Data Analysis class, I decided to try use machine learning to get me a better bracket. The class is about the use of machine learning in sports, and so we’re going to use classification algorithms to decide a simple W or L designation.\nWhat follows is the logic and the code I used to fill out my bracket.\nHow did it go?\nNot great. Could I have done better guessing? Doubtful."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#toolkit",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#toolkit",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Toolkit",
    "text": "Toolkit\nWhat I’m using to make and feed the model is the Tidyverse and Tidymodels set of libraries, the gt library for presenting some tables later, and doParallel for parallel processing, because the xgboost model training takes a while.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\n\nset.seed(1234)\n\nrequire(doParallel)\ncores &lt;- parallel::detectCores(logical = FALSE)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#feature-engineering",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#feature-engineering",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Feature engineering",
    "text": "Feature engineering\nThe data I’m using is scraped from Sports Reference and it’s the box scores of every Division I college basketball game since the start of the 2014-2015 season. The data is a little funny in that each game is in there twice – I’m scraping school pages, so the Team is always that school, and the Opponent is someone else. So a game between two Division I schools will appear twice – one for each Team.\nMy logic in picking predictors was that how efficient teams are with the ball is important, so I estimated the number of possessions and then calculated offensive and defensive ratings (which is points per 100 possessions).\nI then wanted some kind of a metric of how good of a win or how bad of a loss a particular game was. So I calculated the score margin and added it to the opponent’s simple rating from Sports Reference. So a team losing close to a good team isn’t a bad loss. A bad team beating a good team is a great win. And so on. So if you’re a team beating up on bad teams, you don’t get a lot of credit for that.\nThen, I used the teams cumulative mean over the course of the season to estimate what they would have going into the game. Obviously, you can’t know how a team will play going into a game, but I figure that they’ll be somewhere around their cumulative mean, which should pick up if the team is playing better or worse over a few games.\nThen, for tournament purposes, I cut that to the last 10 games of the season. You are who you are in your last 10 games before the end of the season.\nAt least, that was my thinking.\n\n\nCode\ngames &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/cbblogs1521.csv\") %&gt;% mutate(\n  Possessions = .5*(TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA)) + .5*(OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA)),\n  OffensiveRating = (TeamScore/Possessions)*100, \n  DefensiveRating = (OpponentScore/Possessions)*100,\n  ScoreDifference = TeamScore - OpponentScore,\n  WinQuality = case_when(is.na(OpponentSRS) == TRUE ~ ScoreDifference, TRUE ~ ScoreDifference + OpponentSRS)\n  ) %&gt;%\n  group_by(Team, Season) %&gt;%\n  mutate(\n  Cumulative_Mean_Offensive = cummean(OffensiveRating),\n  Cumulative_Mean_Defensive = cummean(DefensiveRating),\n  Cumulative_Mean_WinQuality = cummean(WinQuality)\n  ) %&gt;% \n  filter(between(Game, max(Game)-10, max(Game))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n Outcome = case_when(\n  grepl(\"W\", W_L) ~ \"W\", \n  grepl(\"L\", W_L) ~ \"L\"\n )\n) %&gt;%\n  mutate(Outcome = as.factor(Outcome)) \n\n\nThen, to get both sides of a match-up to be the correct stats, I used some joining to combine them into a single dataset with the cumulative stats for each side that will then use to train a model.\n\n\nCode\nselectedgames &lt;- games %&gt;% \n  select(\n    Season, Team, Date, Opponent, Outcome, Cumulative_Mean_Offensive, Cumulative_Mean_Defensive, Cumulative_Mean_WinQuality, TeamSRS, TeamSOS)\n\nopponentgames &lt;- selectedgames %&gt;% \n  select(-Opponent) %&gt;% \n  rename(\n    Opponent = Team, \n    Opponent_Cumulative_Offensive = Cumulative_Mean_Offensive, \n    Opponent_Cumulative_Mean_Defensive = Cumulative_Mean_Defensive, \n    Opponent_Cumulative_Mean_WinQuality = Cumulative_Mean_WinQuality, \n    OpponentSRS = TeamSRS, \n    OpponentSOS = TeamSOS\n    )\n\nbothsides &lt;- selectedgames %&gt;% \n  left_join(opponentgames, by=c(\"Opponent\", \"Date\", \"Season\")) %&gt;% \n  na.omit() %&gt;% \n  select(-Outcome.y) %&gt;% \n  rename(Outcome = Outcome.x)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#modeling-with-tidymodels",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#modeling-with-tidymodels",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Modeling with Tidymodels",
    "text": "Modeling with Tidymodels\nThere’s a growing supply of tutorials on how to use tidymodels to do machine learning, and one of the authors of the library, Julia Silge, has a long list of posts that greatly helped me figure this all out.\nTo start the modeling processing, I’m going to split my data into training and testing sets.\n\n\nCode\nbracket_split &lt;- initial_split(bothsides, prop = .8)\nbracket_train &lt;- training(bracket_split)\nbracket_test &lt;- testing(bracket_split)\n\n\nI then created a simple recipe, which doesn’t do much except set aside some fields as ID fields instead of making them predictors.\n\n\nCode\nxg_rec &lt;- \n  recipe(Outcome ~ ., data = bracket_train) %&gt;%\n  update_role(Team, Opponent, Date, Season, new_role = \"ID\")\n\nsummary(xg_rec)\n\n\nThen I define my model, which I am going to tune all of the hyperparameters in an xgboost model later.\n\n\nCode\nxg_mod &lt;-   boost_tree(\n  trees = tune(), \n  learn_rate = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  loss_reduction = tune(), \n  sample_size = tune(), \n  mtry = tune(), \n  ) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"xgboost\", nthread = cores)\n\n\nWith a recipe and a model definition, I can create a workflow, which will now handle a whole lot of chores for me.\n\n\nCode\nbracket_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xg_mod) %&gt;% \n  add_recipe(xg_rec)\n\n\nTo tune my hyperparameters, I am going to use a Latin Hypercube, which is a method for generating near-random samples of paremeters to try.\n\n\nCode\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), bracket_train),\n  learn_rate(),\n  size = 30\n)\n\n\nTo test these hyperparemeters, I am going to make some cross-fold valiations samples that we can use.\n\n\nCode\nbracket_folds &lt;- vfold_cv(bracket_train)\n\n\nAnd now comes the part that makes my laptop fan turn into a jet engine. The next block uses parallel processing to try the 30 samples from the Latin Hypercube and tests it against my cross fold validation samples. It … takes a while.\n\n\nCode\ndoParallel::registerDoParallel(cores = cores)\n\nxgb_res &lt;- tune_grid(\n  bracket_wflow,\n  resamples = bracket_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\ndoParallel::stopImplicitCluster()\n\n\nBut out of it, we get the best combination of hyperparameters to use as inputs into our model. I’m going to use area under the curve as my evaluation metric to determine what is best.\n\n\nCode\nbest_roc &lt;- select_best(xgb_res, \"roc_auc\")\n\n\nAnd I can now feed that into my final workflow.\n\n\nCode\nfinal_xgb &lt;- finalize_workflow(\n  bracket_wflow,\n  best_roc\n)\n\n\nAnd I can now train a model to use on bracket games.\n\n\nCode\nxg_fit &lt;- \n  final_xgb %&gt;% \n  fit(data = bracket_train)"
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#evaluating-the-model",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#evaluating-the-model",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Evaluating the model",
    "text": "Evaluating the model\nSo how does this model do?\n\n\nCode\ntrainresults &lt;- bracket_train %&gt;%\n  bind_cols(predict(xg_fit, bracket_train))\n\nmetrics(trainresults, truth = Outcome, estimate = .pred_class)\n\n\nAgainst my training set, not bad. I can predict the correct outcome of a basketball game better than 70 percent of the time.\nHow about against data the model hasn’t seen yet?\n\n\nCode\ntestresults &lt;- bracket_test %&gt;%\n  bind_cols(predict(xg_fit, bracket_test))\n\nmetrics(testresults, truth = Outcome, estimate = .pred_class)\n\n\nJust about the same, which means my model is robust to new data.\nI have made a machine learning model that is better at this than I could be.\nMission accomplished.\nI think."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#play-in-games",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#play-in-games",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "Play In Games",
    "text": "Play In Games\nI’m not going to bore you with the tedium of applying this to every game in each round. My notebook that does it all is almost 900 lines of code long, and this post is already getting long. But here’s an example of what it looks like using the play-in games.\nTo do this, I needed to make a tibble of the games, with the team and opponent. The date doesn’t matter, but it’s needed because my model is expecting it.\nThen, I need to get the right data for each team and join it to them so each game has the predictors the model is expecting. Then, using the model, we can predict the outcome.\n\n\nCode\nplayin &lt;- tibble(\n  Team=\"Norfolk State\",\n  Opponent=\"Appalachian State\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Wichita State\",\n  Opponent=\"Drake\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Mount St. Mary's\",\n  Opponent=\"Texas Southern\",\n  Date = as.Date(\"2021-03-19\")\n) %&gt;% add_row(\n  Team=\"Michigan State\",\n  Opponent=\"UCLA\",\n  Date = as.Date(\"2021-03-19\")\n)\n\nplayingames &lt;- selectedgames %&gt;% \n  group_by(Team) %&gt;% \n  filter(Date == max(Date), Season == \"2020-2021\") %&gt;% \n  select(-Date, -Opponent, -Outcome) %&gt;% \n  right_join(playin)\n\nplayingames &lt;- opponentgames %&gt;% \n  group_by(Opponent) %&gt;% \n  filter(Date == max(Date)) %&gt;% \n  ungroup()  %&gt;% \n  select(-Season, -Date, -Outcome) %&gt;% \n  right_join(playingames, by=c(\"Opponent\")) %&gt;% \n  select(Team, everything())\n\nplayinround &lt;- xg_fit %&gt;% \n  predict(new_data = playingames) %&gt;%\n  bind_cols(playingames) \n\nplayinround &lt;- xg_fit %&gt;% \n  predict(new_data = playinround, type=\"prob\") %&gt;%\n  bind_cols(playinround)\n\nplayinround %&gt;% select(Team, .pred_class, Opponent, .pred_L) %&gt;% \n  gt() %&gt;% \n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;% \n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), \n                         color = \"grey\", weight = px(1)),\n    locations = cells_column_labels(everything())\n  )\n\n\nSince these games have already happened, we know the outcome, and my model got 3 of 4 correct. The only miss was predicting Norfolk State would win, but it also happens to be the game the model has the least amount of confidence in.\nThis might actually work."
  },
  {
    "objectID": "posts/ncaa-bracket-with-machine-learning-2021/index.html#hows-it-going",
    "href": "posts/ncaa-bracket-with-machine-learning-2021/index.html#hows-it-going",
    "title": "How I (poorly) filled out my NCAA bracket with machine learning",
    "section": "How’s it going?",
    "text": "How’s it going?\nIn a word: horrible.\nAfter two rounds, my bracket is better than 38 percent of brackets on ESPN, which puts me in 9.1 millionth place, give or take. I’ve been as low as 10.8 millionth place, so I’ve come up a bit. I still have three of my four Final Four teams and four of eight Elite Eight teams.\nWhen the dust has settled, I’m going to come back and evaluate. Here’s screenshots of my bracket."
  },
  {
    "objectID": "posts/2009-01-25-twitter-marketing-and-the-devil/index.html",
    "href": "posts/2009-01-25-twitter-marketing-and-the-devil/index.html",
    "title": "Twitter, marketing and the devil",
    "section": "",
    "text": "Everything you need to know about using Twitter for marketing and PR is at this very descriptive url:\nhttp://www.howtousetwitterformarketingandpr.com/\nThat’s right: Don’t. Don’t use Twitter for marketing.\nWhy?\nLet me draw a comparison for you: What would you call someone who uses a program to automatically send you an annoying volume of email? A spammer. So what makes you think that because it’s 140 characters and on some trendy web service that blasting away with soulless and automatic links isn’t spam?\nSo that’s why I say automated means to add content to Twitter, like Twitterfeed, are the devil: It tempts you into believing that you can automate marketing your content to this massive group of people with zero effort. Who could possibly think that’s bad? Here kid, here’s some free candy. Just get in my car. Like all get rich quick schemes — make thousands of dollars working from home and only hours a week! — the reality never lives up to the hype.\nStill not convinced? Let me give you some publicly available numbers.\nI started the PolitiFact Twitter account in September 2007, months before the Iowa Caucuses, months before most people cared about the election. We didn’t use it for quite a while — I wasn’t sure what I wanted to do with it. It started as an experiment with programming using the Twitter API (a machine interface to Twitter that allows me to pass content directly from the PolitiFact database to Twitter without needing someone to do it). Then I discovered Twitterfeed. Wow. Feeding my RSS feed onto Twitter for nothing! And no effort from me! Cool.\nSo I set up Twitterfeed, set up some links on PolitiFact and sent the link out to some friends of mine hoping they’d circulate it into the wider Twitter world. We did that from around the Iowa Caucuses until after the conventions in late August. Total followers in 8 months: about 200.\nThen, after the first debate, I got the idea to send out links and information during the debates. Live. Using me, a human, to do it. So, like a comedian tapping the mic and asking if this thing is on, I got on our Twitter account and said “Hey, I’m going to live tweet the next debate.”\nBy the start of the debate, we had doubled our previous follower total. By the end of it, we’d tripled it. We added better than a follower a minute as the debate went on. We added several hundred more followers during the final debate (in spite of our servers crashing as the debate when on).\nBefore our Twitter account became human, we had about 200 followers. Now? We have over 1,000.\nSo what did I do different? I added a human voice — my own. I answered people’s questions, about why we ruled a certain way or how we come to a ruling. People asked us to check something that caught their ear. They made suggestions about site design that we put into place on the relaunch. On one ruling, several people called information to our attention that ended up changing the ruling by one notch on the meter.\nFull disclosure: I didn’t eliminate Twitterfeed entirely. I throttled it back to only adding one link tweet an hour. I’m not saying Twitterfeed is valueless. I’m saying it’s not a solution. Tweeting a new story or a new item has value. The balance is in volume.\nBy being human, four times more people now come in contact with the PolitiFact brand and PolitiFact content. Twitter now routinely shows up on the top 10 referring domains — where people are coming to our content from — for PolitiFact.\nYou know what that’s called? Marketing.\nSo I can here you complaining already: But I’m so busy. How can I possibly be expected to be on Twitter on top of all these other things I’m doing? Truth is, most days, I’d spend no more than a half an hour on our Twitter account. Mid to late afternoon, I’d get on there, answer any questions, maybe throw out a link to our own stuff or something else interesting that I’d read that day. And then I’d log off until the next day. It doesn’t have to be a time suck — you can control how much time you spend Twittering your content. And be upfront about it — if people ask why you didn’t respond faster, tell them the truth: you were busy writing more stories or adding more features.\nSo how to do you market your content on Twitter?\nYou don’t.\nYou be yourself. You share your content with your followers and talk about it with them like you would with your friends at the coffee shop (just in 140 character chunks). And by not marketing, you reach way more people than you’ll ever be able to with Twitterfeed and other so-called marketing tools."
  },
  {
    "objectID": "posts/2018-06-12-choosing-a-world-cup-team-to-root-for-in-each-match-an-algorithmic-approach/index.html",
    "href": "posts/2018-06-12-choosing-a-world-cup-team-to-root-for-in-each-match-an-algorithmic-approach/index.html",
    "title": "Choosing a World Cup Team to root for in each match: An algorithmic approach.",
    "section": "",
    "text": "The World Cup is here. For a large swath of the planet, we do not have a team in the tournament, which raises a question: Who do you root for game by game?\nI’m teaching a sports data visualization and analysis course this fall, so I started thinking if there was an assignment in here somewhere. Could students develop some kind of algorithm? And the more I thought about it, the more I realized: Nope. \nHere’s why, in pseudocode, that an algorithmic approach to choosing a side in each match is not something you can do in code: \nfor each of two teams in a match:    if you’ve been to that country:        take that team    else if you’ve been to both:        take the one where you have better friends    else if you have good friends or no friends in both:        take the one where you had a better meal    else if the food is pretty good in both:        take the one with the less horrible national drink    else if you’ve been to neither:        take the one that didn’t bribe their way into the tournament    else if both arrived seemingly fairly/both bribed everyone to get there:        take the one who used to be a beloved ally until recently    else if both were beloved allies until recently:        take the one who you haven’t fought a war against at some point in history    else if you’ve never fought a war against either:        take the one that has a player you’ve heard of before    else if neither has a player you know:        take the one with the cooler nickname (example: the Indomitable Lions of           Cameroon, sadly not in the tournament)    else if neither country are all that friendly:        take the one not run by a murderous dictator    else if both are run by a murderous dictator:        take the one with a better uniform    else if both murderous dictatorships have decent kits:        take the one that has nuclear weapons    and if all else fails:         root for Iceland. Their whole country has a population equal to Honolulu, Hawaii."
  },
  {
    "objectID": "posts/2009-04-12-telling-the-google-bot-no/index.html",
    "href": "posts/2009-04-12-telling-the-google-bot-no/index.html",
    "title": "Telling the Google Bot no",
    "section": "",
    "text": "On every web project I’ve worked, one of the key/top/vital priorities was to make sure that Google could index every single last word of the site so that if someone was searching for what we had, they’d find it. My most recent project turned that on its head.\nWhat if you don’t want Google to index everything? What if you only want Google to index this, but not that?\nThe project where this came up was Tampa Bay Mug Shots, a site that displays the mug shots of people booked into county jails in three Florida counties as they come in. To make the site, we wrote bots that go and scrape the county jail websites every hour. The information we gather is already in the public domain and has been for years. Many counties have several years – 14 years in Hillsborough County – of mugs available.\nFrom before we even had written a line of code, we were concerned about the Google issue. We did not want the first result in Google for someone’s name to be our site. The goal of our site was to show you who was passing through local jails at any given moment, not to become a permanent repository of criminal records. For many, many reasons, we can’t do that. It’s difficult to track an offender through the system with reporters who know what to look for. To do it in an automated fashion with bots is impossible. And not only that, it’s not the job of a newspaper company to become a repository of criminal histories. Nor should it be.\nSo from the start, the site was constructed with the First Result in Google notion tops on our mind. Here’s what we did to stop the Google Bot:\n1. Mugs expire after 60 days: The queries that bring up a mug are date-based. The query checks if the booking date on the record is older than 60 days. If it is, it serves up a template that says it’s been removed. If it’s newer than 60 days, it serves up the mug. This serves two purposes: 1. If someone links to a mug on their blog, and 100 days later someone finds it and follows it, they’ll find nothing. 2. Since we can’t know the resolution of any case, we just drop the mug, convicted or acquitted. One of the most frequent questions I get is about that issue: What if the charges are dropped? The answer is the same as if the person is convicted and punished to the fullest extent of the law: On our site, the mug is gone in 60 days. It’s a matter of public record that someone was arrested and booked into jail, and transparency here is an important part of the justice system. But since we can’t know the outcome of a case, we elected to just drop the mug after a set period.\n2. The mug shot galleries are made with Javascript, which the Google Bot ignores. If you ever want Google to just never find something, wrap it in Javascript. This is a mistake a lot of sites make, building rich interfaces – or whole applications – out of Javascript which are almost totally ignored by Google. In this case, we used it to our advantage. To Google, there is no link to an individual’s page on those galleries for the bot to follow. No link, no indexing.\n3. The sitemap we submitted to Google doesn’t include individual mugs. The best way to get Google to index your site is to give the bot a road map in the form of a Google sitemap. Ours only includes our index pages – the browse by height, weight, eye color, etc. pages. We simply didn’t include a map to every individual booking photo.\n4. The individual pages have a meta tag that says “don’t index me.” Even with our other efforts, Google could still find individual pages. Anytime anyone linked to a person in their blog, or Twitter stream or other places where the Google Bot goes, the bot could follow that link and index that individual’s page. So, our last step was to attach a tag to those individual pages that says to Google “nothing to see here, move along.”\nFor the tech heads, we used Django, and we can target the Google Bot at individual pages through a block tag in our templates. We put a\n{% block google %}\non our base template in the head tag and on the mug shot detail template, we put\n\ninto that block. We can do it anywhere now, should we find a need.\nI can’t say I think it will be a common case where you don’t want Google to index something, but sometimes privacy trumps SEO. Hope this helps anyone else facing this trade-off."
  },
  {
    "objectID": "posts/2012-02-02-using-python-to-access-tweets-from-the-command-line/index.html",
    "href": "posts/2012-02-02-using-python-to-access-tweets-from-the-command-line/index.html",
    "title": "Using Python to access tweets from the command line",
    "section": "",
    "text": "Here at the Harvard of the Plains, I teach a class in digital product development I like to call Programming as an Act of Journalism. Lots of people ask me about it and I’m always a bit cagey about it because, to be frank, I’m still kind of making it up as I go. My course goals could actually fill an entire degree, so I spend a lot of time pushing and pulling against my wants and needs for the class. But the basic outline is I take College of Journalism students who know nothing about code and product development and we build a prototype of a product they invent in 16 weeks. And the first eight weeks of hands-on classes are spent doing intro to programming type work. \nI’ve started developing lessons I call Small Wins for the class so they can see stuff working on the screen. Here’s the first: a lesson in using Python to access the Twitter API. It’s the follow up class to basic Python (variables, strings, integers, functions and lists). The goal is to talk about using libraries, introduce programmatic thinking and data structure and, well, see some stuff happen in front of their eyes.\nIn the class, I have students install a virtual machine and put Ubuntu Linux on that. Why? Lots of reasons, but the main ones are that I want to expose students to a new environment while at the same time keeping it compartmentalized so they don’t fear “messing up their computer” by working in the terminal. So this walkthrough assumes you’re using Ubuntu.\n\nInstall pip, a package management library for Python.\n\n$ sudo apt-get install python-pip python-dev build-essential \n\nInstall python-twitter, a Python wrapper around the Twitter API that makes accessing the API breathtakingly simple (really!).\n\n\n$ sudo pip install python-twitter\n\nStart Python\n\n$ python\n\nCreate an instance of the Twitter API class, which creates an object with several methods that we’re going to be using. Think of it like buying a Swiss Army knife. You get the knife and a whole bunch of tools that go with it. You just have to unfold them to use them.\n\n&gt;&gt;&gt; import twitter\n\n\n&gt;&gt;&gt; api = twitter.Api()\n\n\nPick a Twitter user to gather their tweets. For this example, we’ll use Stephen Colbert (@StephenAtHome). Then we’ll use one of our tools in the Twitter API class – GetUserTimeline, which does what you think it does.\n\n\n&gt;&gt;&gt; statuses = api.GetUserTimeline(‘StephenAtHome’)\n\n\n\nCongrats. You now have Stephen Colbert’s latest tweets. Stand in appreciation of what all the python-twitter library has done for you. It created an http request, sent that request to the properly formatted API URL, ingested the response, converted the structured JSON file it returned into a Python dictionary and returned it to you as an object for you to use. What’s that? You’re not excited? You want to see them? Okay, fine.\n\n\n&gt;&gt;&gt; for tweet in statuses:\n\n\n…     print tweet\n\nYikes. What is that? It’s all the data that comes with each tweet. Take a look at it all. Background colors, times, dates, all kinds of stuff. Well, that won’t do. Maybe we just want to see the text of the tweet. If you look at the output, you’ll see a pattern in the output. You’ll see things like “protected”: false and “screen_name”: “StephenAtHome”. Those are called key/value pairs. The key is the name of the attribute – like an ID or the text of the tweet – and the value is what it sounds like it is.\nIn our case, we want to see the text of the tweet. To do that, you have to address the key. In Python, we do that with dot notation. What does that mean? It means you can pick the key in an object and display it by adding a dot after your object and then the key name. So like this: nameofobject DOT name of key\n\n\n&gt;&gt;&gt; for tweet in statuses:\n\n\n…     print tweet.text\n\n\nExercise:\n\n\n– Looking at the keys, what would you have to do to see if each tweet was favorited?\n\n\n\nOkay, one users tweets are nice, but what if I want to see tweets about a subject, not a person? For that, we’ll use a different function in the python-twitter library, GetSearch:\n\n\n&gt;&gt;&gt; tweets = api.GetSearch(“politifact”, per_page=100, page=1)\n\n\nExercise:\n\n\n– Using what you’ve learned, how would you print out the text of the tweets gathered in step 8?\n\n\nThoughts, suggestions, criticisms? The comment box awaits you. Want to go further with this? Here’s the python-twitter code and documentation."
  },
  {
    "objectID": "posts/2013-09-12-unconference-panel-pitch-i-will-someday-make/index.html",
    "href": "posts/2013-09-12-unconference-panel-pitch-i-will-someday-make/index.html",
    "title": "Unconference panel pitch I will someday make",
    "section": "",
    "text": "I want to solve a journalism problem with a MakerBot. What is it?\nI hate having a fantastic solution and no problem to solve. Being a solution in search of a problem sucks. I’m hoping to get students thinking about this in an open lab time I’m running. But seriously. I want a MakerBot. My gut says there’s a journalism problem to solve here. But I don’t know what it is yet. \nYet."
  },
  {
    "objectID": "posts/2012-06-09-numeracy/index.html",
    "href": "posts/2012-06-09-numeracy/index.html",
    "title": "Numeracy",
    "section": "",
    "text": "Reading the paper this morning, I was interested to read two stories. The first was about the University of Nebraska Board of Regents approving at 3.75 percent tuition increase. The second was about those same regents moving forward with cutting the number of credit hours to graduate. The two stories were separate, and in print were in different sections.\nWhy am I interested in this? Because of the logic bomb contained within them that you’ll find if you just do the math.\nIn the tuition increase story, you learn that the increase will add $94 to $116 to a students bill every semester.\nIn the degree requirements story, you learn that cutting degree requirements is “aimed at saving students and their families money.” In fact, cutting the required number of credit hours from 125 to 120 will save students $1,000 to $1,500 in tuition and fees in total.\nWait a second, those are two different time periods. One looks at per semester, the other looks at total cost. What happens when we put them on the same scale?\nIf a student takes the traditional 8 semesters to graduate, that means the tuition increase will add $752 to $928 to the student’s total bill.\nIn other words, because of these two things combined, the real savings to students is $248 to $572.\nWhich raises a question not being asked: Why raise tuition if you’re trying to cut costs? Or, on the flip side, why cut credit hours to save money if you aren’t going to save any?"
  },
  {
    "objectID": "posts/another-year-another-attempt-another-bracket-disaster/index.html",
    "href": "posts/another-year-another-attempt-another-bracket-disaster/index.html",
    "title": "Another year, another attempt, another bracket disaster",
    "section": "",
    "text": "Once again, I attempted to predict the outcome of the NCAA tournament using machine learning, and I had a class-load of students try the same.\nIf you like the madness part of March Madness, this year is for you.\nIt is not for machine learning algorithms based on regular season performance. At least not mine.\nOf the 14 brackets I and my students produced, using 14 different methods, we came up with 7 unique national title winners.\nZero of them are right.\nThe best brackets had two of the four Final Four teams, but none picked a team still playing now to win it all with the Final Four set.\nI had hope for my bracket picking algorithm this year. It had what felt like a good mix of upsets and favorites, some madness and some method.\nWhat follows is a post mortem and an attempt to figure out what went wrong. If anything went wrong. How do you predict St. Peter’s? You don’t. It’s what makes this fun.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(zoo)\nlibrary(hoopR)\nlibrary(gt)\n\nset.seed(1234)\n\nkenpom &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/ratings.csv\")\nnamekey &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/nametable.csv\")\nsimplestats &lt;- read_csv(\"http://mattwaite.github.io/sportsdatafiles/simplestats.csv\")\n\nteamgames &lt;- load_mbb_team_box(seasons = 2015:2022) %&gt;%\n  filter(game_date &lt; as.Date(\"2022-03-17\")) %&gt;%\n  separate(field_goals_made_field_goals_attempted, into = c(\"field_goals_made\",\"field_goals_attempted\")) %&gt;%\n  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c(\"three_point_field_goals_made\",\"three_point_field_goals_attempted\")) %&gt;%\n  separate(free_throws_made_free_throws_attempted, into = c(\"free_throws_made\",\"free_throws_attempted\")) %&gt;%\n  mutate_at(12:34, as.numeric) %&gt;% \n  mutate(team_id = as.numeric(team_id))\n\nteamgames &lt;- teamgames %&gt;% left_join(namekey) %&gt;% left_join(kenpom, by=c(\"team\" = \"team\", \"season\"=\"year\")) %&gt;% left_join(simplestats, by=c(\"School\" = \"School\", \"season\" = \"Season\"))\n\nteamstats &lt;- teamgames %&gt;% \n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    team_offensive_efficiency = (team_score/possessions)*100,\n    true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100,\n    turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    free_throw_factor = free_throws_made/field_goals_attempted,\n    team_rolling_true_shooting_percentage = rollmean(lag(true_shooting_percentage, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_turnover_percentage = rollmean(lag(turnover_pct, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_free_throw_factor = rollmean(lag(free_throw_factor, n=1), k=10, align=\"right\", fill=NA), \n    team_cumulative_mean_true_shooting = lag(cummean(true_shooting_percentage), n=1, default=0),\n    team_cumulative_mean_turnover_percentage = lag(cummean(turnover_pct), n=1, default=0),\n    team_cumulative_mean_free_throw_factor = lag(cummean(free_throw_factor), n=1, default=0),\n    team_cumulative_o_eff = lag(cummean(team_offensive_efficiency), n=1, default=0),\n    team_rolling_o_eff = rollmean(lag(team_offensive_efficiency, n=1), k=10, align=\"right\", fill=NA)\n  ) %&gt;% ungroup() %&gt;%\n  rename(\n    team_sos = OverallSOS,\n    team_srs = OverallSRS,\n    team_luck = luck\n  )\n\nteamstats &lt;- teamstats %&gt;% \n  select(game_id, team_id, team_offensive_efficiency) %&gt;%\n  mutate(team_id = as.numeric(team_id)) %&gt;% \n  rename(opponent_id = team_id, opponent_offensive_efficiency=team_offensive_efficiency) %&gt;% \n  left_join(teamstats) %&gt;%\n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_cumulative_d_eff = lag(cummean(opponent_offensive_efficiency), n=1, default=0),\n    team_rolling_d_eff = rollmean(lag(opponent_offensive_efficiency, n=1), k=10, align=\"right\", fill=NA)\n    ) %&gt;% ungroup()\n\nopponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id))\n\nnewteamstats &lt;- teamstats %&gt;% \n  inner_join(opponent) %&gt;% \n  mutate(\n    orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds),\n    drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds),\n    team_rolling_orb = rollmean(lag(orb, n=1), k=10, align=\"right\", fill=NA),\n    team_rolling_drb = rollmean(lag(drb, n=1), k=10, align=\"right\", fill=NA),\n    team_cumulative_mean_orb = lag(cummean(orb), n=1, default=0),\n    team_cumulative_mean_drb = lag(cummean(drb), n=1, default=0),\n    team_efficiency_margin = team_cumulative_o_eff - team_cumulative_d_eff,\n    team_recent_efficiency_margin = team_rolling_o_eff - team_rolling_d_eff,\n    team_recency = team_recent_efficiency_margin - team_efficiency_margin\n    ) \n\nteam_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit()\n\nopponent_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit() %&gt;%\n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage,\n    opponent_rolling_free_throw_factor = team_rolling_free_throw_factor,\n    opponent_rolling_turnover_percentage = team_rolling_turnover_percentage,\n    opponent_rolling_orb = team_rolling_orb,\n    opponent_rolling_drb = team_rolling_drb,\n    opponent_cumulative_mean_true_shooting = team_cumulative_mean_true_shooting,\n    opponent_cumulative_mean_turnover_percentage = team_cumulative_mean_turnover_percentage,\n    opponent_cumulative_mean_free_throw_factor = team_cumulative_mean_free_throw_factor,\n    opponent_cumulative_mean_orb = team_cumulative_mean_orb,\n    opponent_cumulative_mean_drb = team_cumulative_mean_drb,\n    opponent_cumulative_o_eff = team_cumulative_o_eff,\n    opponent_cumulative_d_eff = team_cumulative_d_eff,\n    opponent_efficiency_margin = team_efficiency_margin,\n    opponent_srs = team_srs,\n    opponent_sos = team_sos,\n    opponent_luck = team_luck,\n    opponent_recency = team_recency\n  ) %&gt;%\n  mutate(\n    opponent_id = as.numeric(opponent_id)\n    )\n\ngames &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate(\n  TeamResult = as.factor(case_when(\n    team_score &gt; opponent_score ~ \"W\",\n    opponent_score &gt; team_score ~ \"L\"\n))) %&gt;% na.omit()\n\ngames$TeamResult &lt;- relevel(games$TeamResult, ref=\"W\")\n\ncumulativesimplemodelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, opponent_efficiency_margin, team_efficiency_margin, team_sos, team_srs, opponent_sos, opponent_srs, opponent_luck, team_luck, opponent_recency, team_recency, TeamResult) %&gt;% na.omit()\n\ncumulative_split &lt;- initial_split(cumulativesimplemodelgames, prop = .8)\ncumulative_train &lt;- training(cumulative_split)\ncumulative_test &lt;- testing(cumulative_split)\n\ncumulative_recipe &lt;- \n  recipe(TeamResult ~ ., data = cumulative_train) %&gt;% \n  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = \"ID\") %&gt;%\n  step_normalize(all_predictors())\n\nlog_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nlog_workflow &lt;- \n  workflow() %&gt;% \n  add_model(log_mod) %&gt;% \n  add_recipe(cumulative_recipe)\n\nlog_fit &lt;- \n  log_workflow %&gt;% \n  fit(data = cumulative_train)\n\nteamstats &lt;- teamgames %&gt;% \n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,\n    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),\n    team_offensive_efficiency = (team_score/possessions)*100,\n    true_shooting_percentage = (team_score / (2*(field_goals_attempted + (.44 * free_throws_attempted)))) * 100,\n    turnover_pct = turnovers/(field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    free_throw_factor = free_throws_made/field_goals_attempted,\n    team_rolling_true_shooting_percentage = rollmean(true_shooting_percentage, k=5, align=\"right\", fill=NA),\n    team_rolling_turnover_percentage = rollmean(turnover_pct, k=5, align=\"right\", fill=NA),\n    team_rolling_free_throw_factor = rollmean(free_throw_factor, k=4, align=\"right\", fill=NA), \n    team_cumulative_mean_true_shooting = cummean(true_shooting_percentage),\n    team_cumulative_mean_turnover_percentage = cummean(turnover_pct),\n    team_cumulative_mean_free_throw_factor = cummean(free_throw_factor),\n    team_cumulative_o_eff = cummean(team_offensive_efficiency),\n    team_rolling_o_eff = rollmean(team_offensive_efficiency, k=10, align=\"right\", fill=NA)\n  ) %&gt;% ungroup() \n\nteamstats &lt;- teamstats %&gt;% \n  select(game_id, team_id, team_offensive_efficiency) %&gt;%\n  mutate(team_id = as.numeric(team_id)) %&gt;% \n  rename(opponent_id = team_id, opponent_offensive_efficiency=team_offensive_efficiency) %&gt;% \n  left_join(teamstats) %&gt;%\n  group_by(team_short_display_name, season) %&gt;%\n  arrange(game_date) %&gt;%\n  mutate(\n    team_cumulative_d_eff = cummean(opponent_offensive_efficiency),\n    team_rolling_d_eff = rollmean(opponent_offensive_efficiency, k=10, align=\"right\", fill=NA)\n    ) %&gt;% ungroup() %&gt;%\n  rename(\n    team_sos = OverallSOS,\n    team_srs = OverallSRS, \n    team_luck = luck\n  )\n\nopponent &lt;- teamstats %&gt;% select(game_id, team_id, offensive_rebounds, defensive_rebounds) %&gt;% rename(opponent_id=team_id, opponent_offensive_rebounds = offensive_rebounds, opponent_defensive_rebounds=defensive_rebounds) %&gt;% mutate(opponent_id = as.numeric(opponent_id))\n\nnewteamstats &lt;- teamstats %&gt;% \n  inner_join(opponent) %&gt;% \n  mutate(\n    orb = offensive_rebounds / (offensive_rebounds + opponent_defensive_rebounds),\n    drb = defensive_rebounds / (opponent_offensive_rebounds + defensive_rebounds),\n    team_rolling_orb = rollmean(orb, k=5, align=\"right\", fill=NA),\n    team_rolling_drb = rollmean(drb, k=5, align=\"right\", fill=NA),\n    team_cumulative_mean_orb = cummean(orb),\n    team_cumulative_mean_drb = cummean(drb),\n    team_efficiency_margin = team_cumulative_o_eff - team_cumulative_d_eff,\n    team_recent_efficiency_margin = team_rolling_o_eff - team_rolling_d_eff,\n    team_recency = team_recent_efficiency_margin - team_efficiency_margin\n    )\n\nteam_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, opponent_id, game_date, season, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_rolling_o_eff, team_rolling_d_eff, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit()\n\nopponent_side &lt;- newteamstats %&gt;%\n  select(game_id, team_id, team_short_display_name, team_score, team_rolling_true_shooting_percentage, team_rolling_free_throw_factor, team_rolling_turnover_percentage, team_rolling_orb, team_rolling_drb, team_cumulative_mean_true_shooting, team_cumulative_mean_turnover_percentage, team_cumulative_mean_free_throw_factor, team_cumulative_mean_orb, team_cumulative_mean_drb, team_cumulative_o_eff, team_cumulative_d_eff, team_efficiency_margin, team_rolling_o_eff, team_rolling_d_eff, team_sos, team_srs, team_luck, team_recency) %&gt;% na.omit() %&gt;%\n  rename(\n    opponent_id = team_id,\n    opponent_short_display_name = team_short_display_name,\n    opponent_score = team_score,\n    opponent_rolling_true_shooting_percentage = team_rolling_true_shooting_percentage,\n    opponent_rolling_free_throw_factor = team_rolling_free_throw_factor,\n    opponent_rolling_turnover_percentage = team_rolling_turnover_percentage,\n    opponent_rolling_orb = team_rolling_orb,\n    opponent_rolling_drb = team_rolling_drb,\n    opponent_cumulative_mean_true_shooting = team_cumulative_mean_true_shooting,\n    opponent_cumulative_mean_turnover_percentage = team_cumulative_mean_turnover_percentage,\n    opponent_cumulative_mean_free_throw_factor = team_cumulative_mean_free_throw_factor,\n    opponent_cumulative_mean_orb = team_cumulative_mean_orb,\n    opponent_cumulative_mean_drb = team_cumulative_mean_drb,\n    opponent_cumulative_o_eff = team_cumulative_o_eff,\n    opponent_cumulative_d_eff = team_cumulative_d_eff,\n    opponent_efficiency_margin = team_efficiency_margin,\n    opponent_rolling_o_eff = team_rolling_o_eff, \n    opponent_rolling_d_eff = team_rolling_d_eff,\n    opponent_srs = team_srs,\n    opponent_sos = team_sos,\n    opponent_luck = team_luck,\n    opponent_recency = team_recency\n  ) %&gt;%\n  mutate(\n    opponent_id = as.numeric(opponent_id)\n    )\n\ngames &lt;- team_side %&gt;% inner_join(opponent_side) %&gt;% mutate(\n  TeamResult = as.factor(case_when(\n    team_score &gt; opponent_score ~ \"W\",\n    opponent_score &gt; team_score ~ \"L\"\n))) %&gt;% na.omit()\n\ngames$TeamResult &lt;- relevel(games$TeamResult, ref=\"W\")\n\ncumulativesimplemodelgames &lt;- games %&gt;% select(game_id, game_date, team_short_display_name, opponent_short_display_name, season, opponent_efficiency_margin, team_efficiency_margin, team_sos, team_srs, opponent_sos, opponent_srs, team_luck, opponent_luck, team_recency, opponent_recency, TeamResult) \n\nwestround1games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Georgia State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Boise State\",\n  opponent_short_display_name=\"Memphis\"\n) %&gt;% add_row(\n  team_short_display_name=\"UConn\",\n  opponent_short_display_name=\"New Mexico St\"\n) %&gt;% add_row(\n  team_short_display_name=\"Arkansas\",\n  opponent_short_display_name=\"Vermont\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Notre Dame\"\n) %&gt;% add_row(\n  team_short_display_name=\"Texas Tech\",\n  opponent_short_display_name=\"Montana State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan State\",\n  opponent_short_display_name=\"Davidson\"\n) %&gt;% add_row(\n  team_short_display_name=\"Duke\",\n  opponent_short_display_name=\"CSU Fullerton\"\n)\n\nwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround1games)\n\nwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround1games) \n\nwestround1log &lt;- log_fit %&gt;% predict(new_data = westround1games) %&gt;%\n  bind_cols(westround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround1log &lt;- log_fit %&gt;% predict(new_data = westround1log, type=\"prob\") %&gt;%\n  bind_cols(westround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround2games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Memphis\"\n) %&gt;% add_row(\n  team_short_display_name=\"UConn\",\n  opponent_short_display_name=\"Arkansas\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Texas Tech\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan State\",\n  opponent_short_display_name=\"Duke\"\n)\n\nwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround2games)\n\nwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround2games) \n\nwestround2log &lt;- log_fit %&gt;% predict(new_data = westround2games) %&gt;%\n  bind_cols(westround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround2log &lt;- log_fit %&gt;% predict(new_data = westround2log, type=\"prob\") %&gt;%\n  bind_cols(westround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround3games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Arkansas\"\n) %&gt;% add_row(\n  team_short_display_name=\"Alabama\",\n  opponent_short_display_name=\"Duke\"\n) \n\nwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround3games)\n\nwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround3games) \n\nwestround3log &lt;- log_fit %&gt;% predict(new_data = westround3games) %&gt;%\n  bind_cols(westround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround3log &lt;- log_fit %&gt;% predict(new_data = westround3log, type=\"prob\") %&gt;%\n  bind_cols(westround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nwestround4games &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Alabama\"\n) \n\nwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(westround4games)\n\nwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(westround4games) \n\nwestround4log &lt;- log_fit %&gt;% predict(new_data = westround4games) %&gt;%\n  bind_cols(westround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nwestround4log &lt;- log_fit %&gt;% predict(new_data = westround4log, type=\"prob\") %&gt;%\n  bind_cols(westround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround1games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Norfolk State\"\n) %&gt;% add_row(\n  team_short_display_name=\"North Carolina\",\n  opponent_short_display_name=\"Marquette\"\n) %&gt;% add_row(\n  team_short_display_name=\"Saint Mary's\",\n  opponent_short_display_name=\"Indiana\"\n) %&gt;% add_row(\n  team_short_display_name=\"UCLA\",\n  opponent_short_display_name=\"Akron\"\n) %&gt;% add_row(\n  team_short_display_name=\"Texas\",\n  opponent_short_display_name=\"Virginia Tech\"\n) %&gt;% add_row(\n  team_short_display_name=\"Purdue\",\n  opponent_short_display_name=\"Yale\"\n) %&gt;% add_row(\n  team_short_display_name=\"Murray State\",\n  opponent_short_display_name=\"San Francisco\"\n) %&gt;% add_row(\n  team_short_display_name=\"Kentucky\",\n  opponent_short_display_name=\"Saint Peter's\"\n)\n\neastround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround1games)\n\neastround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround1games) \n\neastround1log &lt;- log_fit %&gt;% predict(new_data = eastround1games) %&gt;%\n  bind_cols(eastround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround1log &lt;- log_fit %&gt;% predict(new_data = eastround1log, type=\"prob\") %&gt;%\n  bind_cols(eastround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround2games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"North Carolina\"\n) %&gt;% add_row(\n  team_short_display_name=\"Indiana\",\n  opponent_short_display_name=\"UCLA\"\n)  %&gt;% add_row(\n  team_short_display_name=\"Texas\",\n  opponent_short_display_name=\"Purdue\"\n) %&gt;% add_row(\n  team_short_display_name=\"San Francisco\",\n  opponent_short_display_name=\"Kentucky\"\n) \n\neastround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround2games)\n\neastround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround2games) \n\neastround2log &lt;- log_fit %&gt;% predict(new_data = eastround2games) %&gt;%\n  bind_cols(eastround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround2log &lt;- log_fit %&gt;% predict(new_data = eastround2log, type=\"prob\") %&gt;%\n  bind_cols(eastround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround3games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"UCLA\"\n) %&gt;% add_row(\n  team_short_display_name=\"Purdue\",\n  opponent_short_display_name=\"Kentucky\"\n) \n\neastround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround3games)\n\neastround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround3games) \n\neastround3log &lt;- log_fit %&gt;% predict(new_data = eastround3games) %&gt;%\n  bind_cols(eastround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround3log &lt;- log_fit %&gt;% predict(new_data = eastround3log, type=\"prob\") %&gt;%\n  bind_cols(eastround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\neastround4games &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Purdue\"\n) \n\neastround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(eastround4games)\n\neastround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(eastround4games) \n\neastround4log &lt;- log_fit %&gt;% predict(new_data = eastround4games) %&gt;%\n  bind_cols(eastround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\neastround4log &lt;- log_fit %&gt;% predict(new_data = eastround4log, type=\"prob\") %&gt;%\n  bind_cols(eastround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround1games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Wright State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Seton Hall\",\n  opponent_short_display_name=\"TCU\"\n) %&gt;% add_row(\n  team_short_display_name=\"Houston\",\n  opponent_short_display_name=\"UAB\"\n) %&gt;% add_row(\n  team_short_display_name=\"Illinois\",\n  opponent_short_display_name=\"Chattanooga\"\n) %&gt;% add_row(\n  team_short_display_name=\"Colorado State\",\n  opponent_short_display_name=\"Michigan\"\n) %&gt;% add_row(\n  team_short_display_name=\"Tennessee\",\n  opponent_short_display_name=\"Longwood\"\n) %&gt;% add_row(\n  team_short_display_name=\"Ohio State\",\n  opponent_short_display_name=\"Loyola Chicago\"\n) %&gt;% add_row(\n  team_short_display_name=\"Villanova\",\n  opponent_short_display_name=\"Delaware\"\n)\n\nsouthround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround1games)\n\nsouthround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround1games) \n\nsouthround1log &lt;- log_fit %&gt;% predict(new_data = southround1games) %&gt;%\n  bind_cols(southround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround1log &lt;- log_fit %&gt;% predict(new_data = southround1log, type=\"prob\") %&gt;%\n  bind_cols(southround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround2games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Seton Hall\"\n) %&gt;% add_row(\n  team_short_display_name=\"Houston\",\n  opponent_short_display_name=\"Illinois\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Tennessee\"\n) %&gt;% add_row(\n  team_short_display_name=\"Ohio State\",\n  opponent_short_display_name=\"Villanova\"\n)\n\nsouthround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround2games)\n\nsouthround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround2games) \n\nsouthround2log &lt;- log_fit %&gt;% predict(new_data = southround2games) %&gt;%\n  bind_cols(southround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround2log &lt;- log_fit %&gt;% predict(new_data = southround2log, type=\"prob\") %&gt;%\n  bind_cols(southround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround3games &lt;- tibble(\n  team_short_display_name=\"Arizona\",\n  opponent_short_display_name=\"Illinois\"\n) %&gt;% add_row(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Villanova\"\n)\n\nsouthround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround3games)\n\nsouthround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround3games) \n\nsouthround3log &lt;- log_fit %&gt;% predict(new_data = southround3games) %&gt;%\n  bind_cols(southround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround3log &lt;- log_fit %&gt;% predict(new_data = southround3log, type=\"prob\") %&gt;%\n  bind_cols(southround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround4games &lt;- tibble(\n  team_short_display_name=\"Michigan\",\n  opponent_short_display_name=\"Illinois\"\n) \n\nsouthround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(southround4games)\n\nsouthround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(southround4games) \n\nsouthround4log &lt;- log_fit %&gt;% predict(new_data = southround4games) %&gt;%\n  bind_cols(southround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nsouthround4log &lt;- log_fit %&gt;% predict(new_data = southround4log, type=\"prob\") %&gt;%\n  bind_cols(southround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround1games &lt;- tibble(\n  team_short_display_name=\"Kansas\",\n  opponent_short_display_name=\"Texas Southern\"\n) %&gt;% add_row(\n  team_short_display_name=\"San Diego State\",\n  opponent_short_display_name=\"Creighton\"\n) %&gt;% add_row(\n  team_short_display_name=\"Iowa\",\n  opponent_short_display_name=\"Richmond\"\n) %&gt;% add_row(\n  team_short_display_name=\"Providence\",\n  opponent_short_display_name=\"S Dakota St\"\n) %&gt;% add_row(\n  team_short_display_name=\"LSU\",\n  opponent_short_display_name=\"Iowa State\"\n) %&gt;% add_row(\n  team_short_display_name=\"Wisconsin\",\n  opponent_short_display_name=\"Colgate\"\n) %&gt;% add_row(\n  team_short_display_name=\"USC\",\n  opponent_short_display_name=\"Miami\"\n) %&gt;% add_row(\n  team_short_display_name=\"Auburn\",\n  opponent_short_display_name=\"J'Ville St\"\n)\n\nmidwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround1games)\n\nmidwestround1games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround1games) \n\nmidwestround1log &lt;- log_fit %&gt;% predict(new_data = midwestround1games) %&gt;%\n  bind_cols(midwestround1games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround1log &lt;- log_fit %&gt;% predict(new_data = midwestround1log, type=\"prob\") %&gt;%\n  bind_cols(midwestround1log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround2games &lt;- tibble(\n  team_short_display_name=\"Kansas\",\n  opponent_short_display_name=\"Creighton\"\n) %&gt;% add_row(\n  team_short_display_name=\"Iowa\",\n  opponent_short_display_name=\"Providence\"\n) %&gt;% add_row(\n  team_short_display_name=\"LSU\",\n  opponent_short_display_name=\"Wisconsin\"\n) %&gt;% add_row(\n  team_short_display_name=\"USC\",\n  opponent_short_display_name=\"Auburn\"\n) \n\nmidwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround2games)\n\nmidwestround2games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround2games) \n\nmidwestround2log &lt;- log_fit %&gt;% predict(new_data = midwestround2games) %&gt;%\n  bind_cols(midwestround2games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround2log &lt;- log_fit %&gt;% predict(new_data = midwestround2log, type=\"prob\") %&gt;%\n  bind_cols(midwestround2log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround3games &lt;- tibble(\n  team_short_display_name=\"Creighton\",\n  opponent_short_display_name=\"Iowa\"\n) %&gt;% add_row(\n  team_short_display_name=\"Wisconsin\",\n  opponent_short_display_name=\"USC\"\n)\n\nmidwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround3games)\n\nmidwestround3games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround3games) \n\nmidwestround3log &lt;- log_fit %&gt;% predict(new_data = midwestround3games) %&gt;%\n  bind_cols(midwestround3games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround3log &lt;- log_fit %&gt;% predict(new_data = midwestround3log, type=\"prob\") %&gt;%\n  bind_cols(midwestround3log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround4games &lt;- tibble(\n  team_short_display_name=\"Creighton\",\n  opponent_short_display_name=\"USC\"\n)\n\nmidwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(midwestround4games)\n\nmidwestround4games &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(midwestround4games) \n\nmidwestround4log &lt;- log_fit %&gt;% predict(new_data = midwestround4games) %&gt;%\n  bind_cols(midwestround4games) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nmidwestround4log &lt;- log_fit %&gt;% predict(new_data = midwestround4log, type=\"prob\") %&gt;%\n  bind_cols(midwestround4log) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nfinalfourgames &lt;- tibble(\n  team_short_display_name=\"Gonzaga\",\n  opponent_short_display_name=\"Baylor\"\n) %&gt;% add_row(\n  team_short_display_name=\"Illinois\",\n  opponent_short_display_name=\"USC\"  \n)\n\nfinalfourgames &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(finalfourgames)\n\nfinalfourgames &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(finalfourgames) \n\nfinalfourlog &lt;- log_fit %&gt;% predict(new_data = finalfourgames) %&gt;%\n  bind_cols(finalfourgames) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nfinalfourlog &lt;- log_fit %&gt;% predict(new_data = finalfourlog, type=\"prob\") %&gt;%\n  bind_cols(finalfourlog) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\nchamps &lt;- tibble(\n  team_short_display_name=\"Baylor\",\n  opponent_short_display_name=\"Illinois\"\n) \n\nchamps &lt;- cumulativesimplemodelgames %&gt;% group_by(team_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% select(-TeamResult, -starts_with(\"opponent\")) %&gt;% right_join(champs)\n\nchamps &lt;- cumulativesimplemodelgames %&gt;% group_by(opponent_short_display_name) %&gt;% filter(game_date == max(game_date) & season == 2022) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(-TeamResult, -starts_with(\"team\"), -game_id, -game_date, -season) %&gt;% right_join(champs) \n\nchampslog &lt;- log_fit %&gt;% predict(new_data = champs) %&gt;%\n  bind_cols(champs) %&gt;% select(.pred_class, team_short_display_name, opponent_short_display_name, everything())\n\nchampslog &lt;- log_fit %&gt;% predict(new_data = champslog, type=\"prob\") %&gt;%\n  bind_cols(champslog) %&gt;% select(.pred_class, .pred_W, .pred_L, team_short_display_name, opponent_short_display_name, everything())\n\n\nTo make my predictors, I borrowed simple ratings and strength of schedule from Sports Reference, I calculated season long unweighted offensive and defensive efficiency margins, threw in KenPom’s luck metric and, at the last minute, decided to create a recency bias metric. What I did was compare the season-long efficiency margin of teams to the same metric from their last 10 games. If they were overplaying their season numbers, that gave them a positive measure. I was trying to capture teams who came into the tournament hot, or who were limping into the tournament based on their regular season resume but missing stars or just playing terrible.\n\n\nCode\nsummary(cumulative_recipe) %&gt;%\n  select(variable, role) %&gt;% \n  filter(role != \"ID\") %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"The predictors\",\n    subtitle = \"A mix of efficiencies, ratings and recency bias.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\")\n\n\nIn the run up to the tournament, I used both a logistic regression and support vector machine algorithm, but got very similar results, so I stuck with the more simple logistic regression.\nIn testing, my model was calling college basketball games correctly about 74 percent of the time, so I knew I was going to need to get lucky on a few games. But isn’t that filling out a bracket?\nResult? I did not get lucky.\nParticularly in the Midwest Regional. My models labored to produce a Creighton vs USC Elite Eight match-up that sent USC to the Final Four. While Creighton nearly upset Kansas with half a lineup, USC bombed out in the first round. The team that beat them ended up going to the Elite Eight – Miami – taking out other teams I had predicted to win along the way.\nHere’s what my model predicted in the Midwest Regional:\n\n\nCode\nmidwestround1log %&gt;% \n  select(team_short_display_name, .pred_class, .pred_W, opponent_short_display_name) %&gt;%\n  gt() %&gt;% \n  cols_label(\n    team_short_display_name = \"Team\",\n    .pred_class = \"Prediction\",\n    .pred_W = \"Win Confidence\",\n    opponent_short_display_name = \"Opponent\"\n  ) %&gt;%\n  tab_header(\n    title = \"Midwest Regional: Round 1\",\n    subtitle = \"I was sure Iowa State would lose and USC was Final Four bound.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;%\n    fmt_percent(\n    columns = c(.pred_W),\n    decimals = 1\n  )\n\n\nNormally I could survive an Iowa State and Miami win here … except I had both of their opponents moving on fairly deep.\nRound two was a complete disaster.\n\n\nCode\nmidwestround2log %&gt;% \n  select(team_short_display_name, .pred_class, .pred_W, opponent_short_display_name) %&gt;%\n  gt() %&gt;% \n  cols_label(\n    team_short_display_name = \"Team\",\n    .pred_class = \"Prediction\",\n    .pred_W = \"Win Confidence\",\n    opponent_short_display_name = \"Opponent\"\n  ) %&gt;%\n  tab_header(\n    title = \"Midwest Regional: Round 2\",\n    subtitle = \"Not one of these predictions were correct.\"\n  ) %&gt;%  \n  tab_source_note(\n    source_note = md(\"**By:** Matt Waite\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) %&gt;% \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) %&gt;%\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) %&gt;%\n  opt_row_striping() %&gt;% \n  opt_table_lines(\"none\") %&gt;%\n    fmt_percent(\n    columns = c(.pred_W),\n    decimals = 1\n  )\n\n\nMy models ended up with a Gonzaga v Baylor and Illinois v USC final four. That 0-4 on those. None of them made it. The Baylor repeat that I predicted died in the second round at the hands of North Carolina, the highest seeded team to make the Final Four.\nGoing into the Final Four, I’m in the 16th percentile of ESPN brackets, good enough for 14.6 millionth place. Last year, I was in the 38th percentile.\nThe best bracket in my class is in the 88th percentile, with another in the 86th. Only two students did worse than I did.\nWe are what they grow beyond.\nThinking about this bracket, I wanted to try rolling with something that didn’t just bite KenPom and make models out of his data. I wanted to see if I could get to a similar place without re-walking the same ground. I’ve got a year to work in this, but my energy is going to be focused on weighting competition and opponents throughout the season. If of two minds about this: As a Big Ten denizen, I have to wonder if beating up on each other for a whole season is why the Big Ten fades in the tournament. So I’m curious about a Fatigue Factor of some variety. At the same time, how does that explain St. Peter’s? Not sure it does, but I’m not sure there’s a model anywhere that’s going to.\nThe code I wrote to make this relied heavily on hoopR and tidymodels."
  },
  {
    "objectID": "posts/2012-05-10-if-you-were-teaching-a-course-in-data-visualization/index.html",
    "href": "posts/2012-05-10-if-you-were-teaching-a-course-in-data-visualization/index.html",
    "title": "If you were teaching a course in data visualization…",
    "section": "",
    "text": "… what would you include? I’m developing a course in data viz over the summer and am in the brainstorming phase now. Here’s what I’ve got. What would you do?\nCourse Description\nA written narrative is not always the best way to convey information. Sometimes, you have to see the data in order to see the meaning in it. With more data available than any other time in our history, being able to visualize data is becoming a vital communication skill. This course will cover a wide array of subjects related to gathering, analyzing, processing and visualizing data. Students will learn how to gather, clean and analyze data that already exists and will work with gathering their own data with cutting edge equipment. The class will also work on a real-world project.\nTopic ideas\nVisual communication theory\nTypes of data visualizations – when to use, how to read, how they can mislead\nBasic data science – data types and structures\nBasic data management – importing, cleaning\nBasic data analysis – grouping, counting, sorting, summing\nData vis 1 – using Excel charts\nData vis 2 – using Google Fusion Tables\nData vis 3 – using the web as a canvas for data vis (basic HTML/CSS)\nData vis 4 – using a JavaScript data vis library TBA (Raphael? Flot? Both? Neither?)\nGathering your own data with UAVs and remote sensors\nVisualizing data in real-time or near real time\nHow the basic of telling a story – a headline/title, a lead, context, content – apply to data visualizations\nBasic math (via @RobinJP)\nWhat not to visualize (via @knowtheory)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matt Waite",
    "section": "",
    "text": "Professor of Practice | Journalism | Technology\nI am a professor at the University of Nebraska-Lincoln and a data journalist. I build things, analyze data, and teach others how to do the same.\n\n\n\nProjects\n\n\n\nThe Art of Data Journalism\nInteractive tutorials teaching data journalism through R and the Tidyverse.\n\n\n\n\nSports Data Tutorials\nInteractive tutorials for analyzing and visualizing sports data using R and the Tidyverse.\n\n\n\n\nDrone Journalism\nWhat you need to know to get a drone license in the US and what separates a journalist from just someone with a controller in their hand.\n\n\n\n\n\nBlog\n\n\n\n\n\nParsing PDFs with Antigravity\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nNov 24, 2025\n\n\n\n\n\n\n\nAn R + LLM starter kit\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\nAn academic integrity-friendly code pal for R Studio\n\n\n\nAI\n\ncode\n\nr\n\neducation\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\n\n\n\n\n\nNebraska’s season long slide on offense\n\n\n\ncode\n\nfootball\n\nhuskers\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\n\n\n\n\n\nA simple example of AI agents(?) doing journalism(?) work\n\n\n\ncode\n\nanalysis\n\nAI\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\n\n\n\n\n\nAnother year, another attempt, another bracket disaster\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\n\n\n\n\n\nNebraska is not the best worst team in basketball again. They’re third best worst.\n\n\n\nhuskers\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\nIs Nebraska the best worst team in college basketball?\n\n\n\nhuskers\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\nHow I (poorly) filled out my NCAA bracket with machine learning\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n\n\n\n\n\nChoosing a World Cup Team to root for in each match: An algorithmic approach.\n\n\n\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\nNo matching items\n\nView Full Archive"
  },
  {
    "objectID": "nicar/2026-indianapolis-scraping-sports-with-r/index.html",
    "href": "nicar/2026-indianapolis-scraping-sports-with-r/index.html",
    "title": "Scraping Sports Data With R",
    "section": "",
    "text": "Scraping sports data with R can be categorized into Easy, Medium and Hard. Some sites just produce simple, clean and basic HTML for us to consume at our leisure. Some people work for the NCAA. That’s the two ends of our spectrum.\nThere is a github repo that has all this code in it as well here."
  },
  {
    "objectID": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#first-the-easy-way",
    "href": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#first-the-easy-way",
    "title": "Scraping Sports Data With R",
    "section": "First, the easy way",
    "text": "First, the easy way\nFirst, we need libraries\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nSometimes the Gods smile upon you and give you a page with simple HTML, nicely formatted with one header row. cfbstats.com is an example of that. Here is there page for scoring offense stats. Open it in a browser because we’re going to need it.\n\nScraping it\nFirst, define a URL variable and fill it with our cfbstats url.\n\nurl &lt;- \"https://cfbstats.com/2025/leader/national/team/offense/split01/category09/sort01.html\"\n\nNext, Rvest works in four steps when it’s a table.\n\nStart with your URL\nRun the whole request/response cycle with read_html()\nFind your table html with html_element and the xpath to it.\nConvert that to a dataframe\n\nLet’s start with the first two:\n\nurl |&gt; \n  read_html()\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;&lt;script asyn ...\n[2] &lt;body&gt;\\n&lt;div id=\"wrapper\"&gt;\\r\\n&lt;div id=\"breadcrumb\"&gt;\\r\\n  &lt;span class=\"lab ...\n\n\nOkay, we can see that we are getting HTML from our URL.\nBecause at NICAR, the network isn’t guaranteed, let’s save our HTML as results and use that instead of a fresh request every time.\n\nresults &lt;- url |&gt; \n  read_html()\n\nGoing forward, we will now use results instead of url |&gt; read_html() because they are the same thing without having to stress the network.\nNow we need to get some information from the page. Specifically, we need the xpath to our HTML table.\nIn Chrome, Firefox or Edge, right-click on the upper right corner of your html table and go down to Inspect. It will look like this:\n\n\n\nFirefox\n\n\n\n\n\nChrome\n\n\nNote: If you are on Safari, you will have to enable developer tools to make this work.\nNow, in the HTML window that pops up, locate the HTML &lt;table&gt; tag. It must be the &lt;table&gt; tag, not &lt;div class=\"table\"&gt;. One is a table, one is a division named table. The table will work, the division … could … work but will require a lot more code. This is the easy way, remember.\nClick on the table tag so that it is highlighted. Then Right Click on that table tag, go down to Copy and then to Xpath.\n\nNow that you have the Xpath copied, you’re ready for the next step – traversing the DOM (or Document Object Model) to isolate our table.\n\nresults |&gt; \n  html_element(xpath = '/html/body/div/div[3]/div[2]/table')\n\n{html_node}\n&lt;table class=\"leaders\"&gt;\n [1] &lt;tr&gt;\\n&lt;th scope=\"col\" class=\"rank\"&gt;&lt;/th&gt;\\r\\n    &lt;th scope=\"col\" class=\"t ...\n [2] &lt;tr&gt;\\n&lt;td&gt;1&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/497/sc ...\n [3] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;2&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\" ...\n [4] &lt;tr&gt;\\n&lt;td&gt;3&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/306/sc ...\n [5] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;4&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\" ...\n [6] &lt;tr&gt;\\n&lt;td&gt;5&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/651/sc ...\n [7] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;6&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\" ...\n [8] &lt;tr&gt;\\n&lt;td&gt;7&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/700/sc ...\n [9] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;8&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\" ...\n[10] &lt;tr&gt;\\n&lt;td&gt;9&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/317/sc ...\n[11] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;10&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href= ...\n[12] &lt;tr&gt;\\n&lt;td&gt;10&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/433/s ...\n[13] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;12&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href= ...\n[14] &lt;tr&gt;\\n&lt;td&gt;13&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/657/s ...\n[15] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;14&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href= ...\n[16] &lt;tr&gt;\\n&lt;td&gt;15&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/164/s ...\n[17] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;16&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href= ...\n[18] &lt;tr&gt;\\n&lt;td&gt;17&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/465/s ...\n[19] &lt;tr class=\"even-row\"&gt;\\n&lt;td&gt;17&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href= ...\n[20] &lt;tr&gt;\\n&lt;td&gt;19&lt;/td&gt;\\r\\n    &lt;td class=\"team-name\"&gt;&lt;a href=\"/2025/team/697/s ...\n...\n\n\nNote: Different HTML engines in browsers come up with different answers for the Xcode. Yours might look slightly different than mine and still work. Firefox and Chrome do this a lot.\nAlso note: Apostrophes around the Xpath, not quotes. Why? Because quotes can be a part of Xpaths.\nNow finally: Turn this all into a table.\n\nresults |&gt; \n  html_element(xpath = '/html/body/div/div[3]/div[2]/table') |&gt; \n  html_table()\n\n# A tibble: 136 × 10\n      `` Name              G    TD    FG `1XP` `2XP` Safety Points `Points/G`\n   &lt;int&gt; &lt;chr&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1     1 North Texas      14    85    12    77     4      0    631       45.1\n 2     2 Notre Dame       12    70     5    63     2      1    504       42  \n 3     3 Indiana          16    87    19    87     0      0    666       41.6\n 4     4 Utah             13    72    11    68     1      1    537       41.3\n 5     5 USF              13    67    18    62     2      2    526       40.5\n 6     6 Tennessee        13    68    14    67     0      0    517       39.8\n 7     7 Texas Tech       14    67    28    58     4      0    552       39.4\n 8     8 Vanderbilt       13    66    13    63     1      0    500       38.5\n 9     9 James Madison    14    68    14    65     2      0    519       37.1\n10    10 Oregon           15    71    19    67     1      1    554       36.9\n# ℹ 126 more rows\n\n\nVictory, right? Sort of. Note that the first column has no name. That will cause no end of problems for you down the road. Now is the time to fix that.\nWhen I teach scraping with rvest, the order of operations I give my students that works from a logical standpoint goes like this:\n\nGet the page.\nGet the data.\nClean the headers/column names.\nClean the data. Got columns with $ or % in them? Strip them. Funny name splits? Fix them here.\nSave the data to a new dataframe.\nAnalyze, visualize, profit.\n\nWe’ve done 1 and 2. If we now do 3, 4 is done for us because cfbstats gets it and 5 and 6 will be up to you.\nHow do we fix this? Because it’s just one column, let’s use the rename function from tidyr.\n\nresults |&gt; \n  html_element(xpath = '/html/body/div/div[3]/div[2]/table') |&gt; \n  html_table() |&gt; \n  rename(\n    Rank = 1\n  )\n\n# A tibble: 136 × 10\n    Rank Name              G    TD    FG `1XP` `2XP` Safety Points `Points/G`\n   &lt;int&gt; &lt;chr&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1     1 North Texas      14    85    12    77     4      0    631       45.1\n 2     2 Notre Dame       12    70     5    63     2      1    504       42  \n 3     3 Indiana          16    87    19    87     0      0    666       41.6\n 4     4 Utah             13    72    11    68     1      1    537       41.3\n 5     5 USF              13    67    18    62     2      2    526       40.5\n 6     6 Tennessee        13    68    14    67     0      0    517       39.8\n 7     7 Texas Tech       14    67    28    58     4      0    552       39.4\n 8     8 Vanderbilt       13    66    13    63     1      0    500       38.5\n 9     9 James Madison    14    68    14    65     2      0    519       37.1\n10    10 Oregon           15    71    19    67     1      1    554       36.9\n# ℹ 126 more rows\n\n\nI always think this is backwards from how my brain works, but rename says “name a column Rank and that’s column 1 you’re renaming.” My brain wants it the other way, but that won’t work.\nIt won’t always be this easy."
  },
  {
    "objectID": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#medium-level-more-spice",
    "href": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#medium-level-more-spice",
    "title": "Scraping Sports Data With R",
    "section": "Medium Level: more spice",
    "text": "Medium Level: more spice\nThe medium level mostly has to do with humans making decisions for humans. How dare they.\nFirst, we need libraries\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(janitor)\n\nFrequently, you will run into sports data providers who believe humans look at their tables of data. And they will make design decisions based on human eyes and reasoning abilities. So annoying. One of the best sports data sites on the planet – and one of the worst offenders of this Humans Look At Tables sin – is Sports Reference. Their sin? Two row header rows. Which results in header names repeating. Which is bad.\nIt’s March. Which means the sports world is fixated on college basketball. So let’s scrape some. Our target is the school stats page on Sport Reference’s Men’s College Basketball page (there is an identical version for the women’s game if you like).\nGo to that page, scroll down to the data and you’ll see the issue: Two row header rows, names that repeat.\n\nScraping and storing it\nLet’s fetch this page and store it so we’re not repeatedly requesting it.\n\nmbb &lt;- \"https://www.sports-reference.com/cbb/seasons/men/2026-school-stats.html\"\n\nresults &lt;- mbb |&gt; read_html()\n\nSimilar to the easy method, get the xpath for the table. The nice thing about Sports Reference: They use ids for their tables, which makes the xpath super simple.\n\nresults |&gt; \n  html_element(xpath = '//*[@id=\"basic_school_stats\"]') |&gt; \n  html_table()\n\n# A tibble: 402 × 38\n   ``    ``    Overall Overall Overall Overall Overall Overall ``    Conf. Conf.\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 Rk    Scho… G       W       L       W-L%    SRS     SOS     NA    W     L    \n 2 1     Abil… 27      13      14      .481    -5.89   -0.85   NA    5     9    \n 3 2     Air … 27      3       24      .111    -13.87  4.69    NA    0     16   \n 4 3     Akron 27      22      5       .815    8.80    -3.00   NA    13    1    \n 5 4     Alab… 27      20      7       .741    23.11   14.37   NA    10    4    \n 6 5     Alab… 28      16      12      .571    -13.09  -10.01  NA    9     6    \n 7 6     Alab… 28      9       19      .321    -13.35  -7.13   NA    6     9    \n 8 7     Alba… 28      10      18      .357    -13.51  -6.99   NA    6     7    \n 9 8     Alco… 26      7       19      .269    -18.86  -4.63   NA    6     8    \n10 9     Amer… 28      15      13      .536    -6.68   -5.72   NA    8     7    \n# ℹ 392 more rows\n# ℹ 27 more variables: `` &lt;chr&gt;, Home &lt;chr&gt;, Home &lt;chr&gt;, `` &lt;chr&gt;, Away &lt;chr&gt;,\n#   Away &lt;chr&gt;, `` &lt;chr&gt;, Points &lt;chr&gt;, Points &lt;chr&gt;, `` &lt;chr&gt;, Totals &lt;chr&gt;,\n#   Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;,\n#   Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;,\n#   Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;, Totals &lt;chr&gt;,\n#   Totals &lt;chr&gt;\n\n\nAnnnnd oof we have a car crash at the top.\n\n\nFixing headers\nFirst things first, we need to tell rvest to ignore the table headers. Then, we can bring some janitor functions to bear on this. Janitor will let us use a row of data to create headers. When we ignore the table headers, that row that has all of our column names in it will become row 2. Then, because Sports Reference is so human centered, we’ll use janitor’s clean_names() function to … clean up those names.\n\nresults |&gt; \n  html_element(xpath = '//*[@id=\"basic_school_stats\"]') |&gt; \n  html_table(header = FALSE) |&gt; \n  row_to_names(row_number = 2) |&gt; \n  clean_names()\n\nWarning: Row 2 does not provide unique names. Consider running clean_names()\nafter row_to_names().\n\n\n# A tibble: 401 × 38\n   rk    school      g     w     l     w_l_percent srs   sos   na    w_2   l_2  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Abilene Ch… 27    13    14    .481        -5.89 -0.85 NA    5     9    \n 2 2     Air Force   27    3     24    .111        -13.… 4.69  NA    0     16   \n 3 3     Akron       27    22    5     .815        8.80  -3.00 NA    13    1    \n 4 4     Alabama     27    20    7     .741        23.11 14.37 NA    10    4    \n 5 5     Alabama A&M 28    16    12    .571        -13.… -10.… NA    9     6    \n 6 6     Alabama St… 28    9     19    .321        -13.… -7.13 NA    6     9    \n 7 7     Albany (NY) 28    10    18    .357        -13.… -6.99 NA    6     7    \n 8 8     Alcorn Sta… 26    7     19    .269        -18.… -4.63 NA    6     8    \n 9 9     American    28    15    13    .536        -6.68 -5.72 NA    8     7    \n10 10    Appalachia… 30    19    11    .633        -2.67 -5.28 NA    11    6    \n# ℹ 391 more rows\n# ℹ 27 more variables: na_2 &lt;lgl&gt;, w_3 &lt;chr&gt;, l_3 &lt;chr&gt;, na_3 &lt;lgl&gt;, w_4 &lt;chr&gt;,\n#   l_4 &lt;chr&gt;, na_4 &lt;lgl&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;, na_5 &lt;lgl&gt;, mp &lt;chr&gt;, fg &lt;chr&gt;,\n#   fga &lt;chr&gt;, fg_percent &lt;chr&gt;, x3p &lt;chr&gt;, x3pa &lt;chr&gt;, x3p_percent &lt;chr&gt;,\n#   ft &lt;chr&gt;, fta &lt;chr&gt;, ft_percent &lt;chr&gt;, orb &lt;chr&gt;, trb &lt;chr&gt;, ast &lt;chr&gt;,\n#   stl &lt;chr&gt;, blk &lt;chr&gt;, tov &lt;chr&gt;, pf &lt;chr&gt;\n\n\nBetter, but not fixed yet\n\n\nFixing more problems, one at a time\nIf we look at our data, we have three more problems.\n\nThere are a bunch of empty columns there for aesthetic purposes and are a waste for us. Jantior can remove those automatically.\nIf you scroll down, you’ll see our header rows repeat, which is bad, but we can remove them with a filter.\nAll of our numeric data are characters. Why? Because of #2. We will need to fix that.\n\nLet’s do these one at a time.\nWe can remove empty rows and columns with the remove_empty function in janitor.\n\nresults |&gt; \n  html_element(xpath = '//*[@id=\"basic_school_stats\"]') |&gt; \n  html_table(header = FALSE) |&gt; \n  row_to_names(row_number = 2) |&gt; \n  clean_names() |&gt; \n  remove_empty(which = c(\"rows\", \"cols\"))\n\nWarning: Row 2 does not provide unique names. Consider running clean_names()\nafter row_to_names().\n\n\n# A tibble: 401 × 33\n   rk    school      g     w     l     w_l_percent srs   sos   w_2   l_2   w_3  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Abilene Ch… 27    13    14    .481        -5.89 -0.85 5     9     9    \n 2 2     Air Force   27    3     24    .111        -13.… 4.69  0     16    3    \n 3 3     Akron       27    22    5     .815        8.80  -3.00 13    1     13   \n 4 4     Alabama     27    20    7     .741        23.11 14.37 10    4     10   \n 5 5     Alabama A&M 28    16    12    .571        -13.… -10.… 9     6     11   \n 6 6     Alabama St… 28    9     19    .321        -13.… -7.13 6     9     5    \n 7 7     Albany (NY) 28    10    18    .357        -13.… -6.99 6     7     5    \n 8 8     Alcorn Sta… 26    7     19    .269        -18.… -4.63 6     8     4    \n 9 9     American    28    15    13    .536        -6.68 -5.72 8     7     11   \n10 10    Appalachia… 30    19    11    .633        -2.67 -5.28 11    6     11   \n# ℹ 391 more rows\n# ℹ 22 more variables: l_3 &lt;chr&gt;, w_4 &lt;chr&gt;, l_4 &lt;chr&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;,\n#   mp &lt;chr&gt;, fg &lt;chr&gt;, fga &lt;chr&gt;, fg_percent &lt;chr&gt;, x3p &lt;chr&gt;, x3pa &lt;chr&gt;,\n#   x3p_percent &lt;chr&gt;, ft &lt;chr&gt;, fta &lt;chr&gt;, ft_percent &lt;chr&gt;, orb &lt;chr&gt;,\n#   trb &lt;chr&gt;, ast &lt;chr&gt;, stl &lt;chr&gt;, blk &lt;chr&gt;, tov &lt;chr&gt;, pf &lt;chr&gt;\n\n\nI always use rows and cols, because janitor is conservative here – if there is literally anything in a column or row, it keeps it. So you’re losing nothing by running both. What happens? We lose no rows, but 5 columns of just blank data.\nNow we need to filter out extraneous column headers. If you scroll down to row 21, you’ll find our repeating header problem. We can use the g column to fix this with some != filters.\n\nresults |&gt; \n  html_element(xpath = '//*[@id=\"basic_school_stats\"]') |&gt; \n  html_table(header = FALSE) |&gt; \n  row_to_names(row_number = 2) |&gt; \n  clean_names() |&gt; \n  remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  filter(\n    g != \"Overall\" & g != \"G\"\n  )\n\nWarning: Row 2 does not provide unique names. Consider running clean_names()\nafter row_to_names().\n\n\n# A tibble: 365 × 33\n   rk    school      g     w     l     w_l_percent srs   sos   w_2   l_2   w_3  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Abilene Ch… 27    13    14    .481        -5.89 -0.85 5     9     9    \n 2 2     Air Force   27    3     24    .111        -13.… 4.69  0     16    3    \n 3 3     Akron       27    22    5     .815        8.80  -3.00 13    1     13   \n 4 4     Alabama     27    20    7     .741        23.11 14.37 10    4     10   \n 5 5     Alabama A&M 28    16    12    .571        -13.… -10.… 9     6     11   \n 6 6     Alabama St… 28    9     19    .321        -13.… -7.13 6     9     5    \n 7 7     Albany (NY) 28    10    18    .357        -13.… -6.99 6     7     5    \n 8 8     Alcorn Sta… 26    7     19    .269        -18.… -4.63 6     8     4    \n 9 9     American    28    15    13    .536        -6.68 -5.72 8     7     11   \n10 10    Appalachia… 30    19    11    .633        -2.67 -5.28 11    6     11   \n# ℹ 355 more rows\n# ℹ 22 more variables: l_3 &lt;chr&gt;, w_4 &lt;chr&gt;, l_4 &lt;chr&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;,\n#   mp &lt;chr&gt;, fg &lt;chr&gt;, fga &lt;chr&gt;, fg_percent &lt;chr&gt;, x3p &lt;chr&gt;, x3pa &lt;chr&gt;,\n#   x3p_percent &lt;chr&gt;, ft &lt;chr&gt;, fta &lt;chr&gt;, ft_percent &lt;chr&gt;, orb &lt;chr&gt;,\n#   trb &lt;chr&gt;, ast &lt;chr&gt;, stl &lt;chr&gt;, blk &lt;chr&gt;, tov &lt;chr&gt;, pf &lt;chr&gt;\n\n\nThe & operator here comes in handy because we want all rows that AREN’T “Overall” AND “G”.\nNow, we just have the number problem. We could use mutate and do each one that we need, but that’s a lot of code and a lot of knowing beforehand what we want. There’s an easier way: mutate_at. And since only column 2 is a character field, we can use the - operator to say mutate NOT column 2 into a numeric column.\n\nresults |&gt; \n  html_element(xpath = '//*[@id=\"basic_school_stats\"]') |&gt; \n  html_table(header = FALSE) |&gt; \n  row_to_names(row_number = 2) |&gt; \n  clean_names() |&gt; \n  remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  filter(\n    g != \"Overall\" & g != \"G\"\n  ) |&gt; \n  mutate_at(-2, as.numeric)\n\nWarning: Row 2 does not provide unique names. Consider running clean_names()\nafter row_to_names().\n\n\n# A tibble: 365 × 33\n      rk school        g     w     l w_l_percent    srs    sos   w_2   l_2   w_3\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 Abilene …    27    13    14       0.481  -5.89  -0.85     5     9     9\n 2     2 Air Force    27     3    24       0.111 -13.9    4.69     0    16     3\n 3     3 Akron        27    22     5       0.815   8.8   -3       13     1    13\n 4     4 Alabama      27    20     7       0.741  23.1   14.4     10     4    10\n 5     5 Alabama …    28    16    12       0.571 -13.1  -10.0      9     6    11\n 6     6 Alabama …    28     9    19       0.321 -13.4   -7.13     6     9     5\n 7     7 Albany (…    28    10    18       0.357 -13.5   -6.99     6     7     5\n 8     8 Alcorn S…    26     7    19       0.269 -18.9   -4.63     6     8     4\n 9     9 American     28    15    13       0.536  -6.68  -5.72     8     7    11\n10    10 Appalach…    30    19    11       0.633  -2.67  -5.28    11     6    11\n# ℹ 355 more rows\n# ℹ 22 more variables: l_3 &lt;dbl&gt;, w_4 &lt;dbl&gt;, l_4 &lt;dbl&gt;, tm &lt;dbl&gt;, opp &lt;dbl&gt;,\n#   mp &lt;dbl&gt;, fg &lt;dbl&gt;, fga &lt;dbl&gt;, fg_percent &lt;dbl&gt;, x3p &lt;dbl&gt;, x3pa &lt;dbl&gt;,\n#   x3p_percent &lt;dbl&gt;, ft &lt;dbl&gt;, fta &lt;dbl&gt;, ft_percent &lt;dbl&gt;, orb &lt;dbl&gt;,\n#   trb &lt;dbl&gt;, ast &lt;dbl&gt;, stl &lt;dbl&gt;, blk &lt;dbl&gt;, tov &lt;dbl&gt;, pf &lt;dbl&gt;\n\n\nMarch Madness charts await you, but please, please, please make sure you credit Sports Reference for your data."
  },
  {
    "objectID": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#the-hard-way-aka-the-ncaa",
    "href": "nicar/2026-indianapolis-scraping-sports-with-r/index.html#the-hard-way-aka-the-ncaa",
    "title": "Scraping Sports Data With R",
    "section": "The Hard Way aka the NCAA",
    "text": "The Hard Way aka the NCAA\nFor having such a huge footprint on the American sports scene, the NCAA is terrible at being a data provider for the general public. Maybe they have some secret system for accredited members, but for me, a guy who teaches a sports data class every semester, getting 30+ students accredited every semester seems insane. Why doesn’t the NCAA have a Sports Reference style site?\nThey do! It’s terrible. It’s slow, brittle, poorly made and not easy to scrape.\nI give you stats.ncaa.org\nIf you have some determination and pay very close attention to url patterns, it can be done, but be very careful. The NCAA can, does and will ban IP addresses of people being abusive, so one of our immediate concerns is going to be putting delays on each request – 3-5 seconds minimum.\nNebraska, my employer, is a volleyball school. We have one of the most high profile programs in the country, the players are local celebrities, and more people attend a single Nebraska volleyball players than many women’s volleyball programs get in a day. We even set a world record for the single most attended women’s sporting event ever when we had a volleyball match in our football stadium and 92,000 people showed up. So let’s look over my volleyball scraper. Why look it over? It’s unlikely this will work on a NICAR conference hotel network, but the pattern will work for many NCAA sports.\nWhat follows is a mix of my own trial and error over the years and some AI assistance to add some features like progress tracking and simplifying loops to be more tidy and less “I did this in Python years ago and this makes sense to me and no one else”.\nNOTE TO COPY AND PASTERS: I have eval = FALSE turned on for these blocks because I do not want them to run. You will want to remove that.\nFirst, we need libraries.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(progress)\nlibrary(janitor)\n\nNext, we’re going to set some things up. Because I have been scraping this data for years, I need to adjust to get the next season. I don’t want to be going through my code to change 2024 to 2025 a million times, so I create variables like the season and the file names with the season in them first, so every time I update, I only have to update this block.\n\nseason = 2025\n\nplayerfilename &lt;- paste0(\"ncaa_womens_volleyball_players_\", season, \".csv\")\nteamfilename &lt;- paste0(\"ncaa_womens_volleyball_teams_\", season, \".csv\")\n\nroot_url &lt;- \"https://stats.ncaa.org\"\n\nrankingurl &lt;- \"https://stats.ncaa.org/selection_rankings/nitty_gritties/45914\"\n\nNote the rankingurl. That is a URL the NCAA creates, you’ll have to find it under their rankings menu item and it’s one of the few permanent urls on the site. The others are the individual teams. And that’s how we’re going to attack this. We’re going to scrape this rankings page, extract the urls to each team, then get each team’s player and team stats from a single table and store those in two separate dataframes.\nProblem? We have to do that 350+ times. Each time we want it. With agressive pauses in it. It takes a while.\nThis next chunk will get that rankings page, extract the URL for each team and extract a table of team and conference names that will come in very handy later.\n\nranking_page &lt;- rankingurl |&gt; \n  read_html() \n\nteamurls &lt;- ranking_page |&gt;\n  html_element(xpath='//*[@id=\"selection_rankings_nitty_gritty_data_table\"]//tr/td[1]/a') |&gt;\n  html_attr(\"href\")\n\nteam_table &lt;- ranking_page |&gt; \n  html_element(xpath='//*[@id=\"selection_rankings_nitty_gritty_data_table\"]') |&gt;\n  html_table()\n\ndf &lt;- team_table |&gt; \n  select(Team, Conference) |&gt; \n  bind_cols(teamurls) |&gt; \n  rename(URL = 3) |&gt; \n  mutate(Team = gsub(\"(AQ)\", \"\", Team, fixed = TRUE))\n\nNow we’re ready to get our data. The first thing we’re going to scrape is the rosters. The rosters have some data that the team stats don’t that’s worth getting, so we are going to fire off ~350 requests to stats.ncaa.org to get those using the url fragments from our rankings page scrape and some purr functions.\nThings to note here: I am not this good at R functions. Claude is. I added a bunch of things here to make sure some important columns are getting cleaned up as I expect them because of some past calamities.\n\nget_roster &lt;- function(row, root_url) {\n  roster_url &lt;- paste0(root_url, row$URL, \"/roster\")\n  \n  tryCatch({\n    roster_page &lt;- read_html(roster_url)\n    \n    roster_table &lt;- roster_page |&gt; \n      html_elements(\"table\") |&gt; \n      html_table() |&gt;\n      pluck(1) |&gt;\n      mutate(\n        Team = gsub(\"(AQ)\", \"\", row$Team, fixed = TRUE),\n        Conference = row$Conference\n      )\n    \n    return(roster_table)\n  }, \n  error = function(e) {\n    warning(paste(\"Error fetching roster for team:\", row$Team))\n    return(NULL)\n  })\n}\n\nget_all_rosters &lt;- function(df, root_url) {\n  # Create progress bar\n  pb &lt;- progress_bar$new(\n    format = \"  downloading [:bar] :percent eta: :eta\",\n    total = nrow(df)\n  )\n  \n  # Add progress update to the function\n  get_roster_with_progress &lt;- function(row, root_url) {\n    result &lt;- get_roster(row, root_url)\n    pb$tick()\n    return(result)\n  }\n  \n  # Add rate limiting\n  slowly_get_roster &lt;- slowly(\n    get_roster_with_progress,\n    rate = rate_delay(3)\n  )\n  \n  # Run the function\n  df |&gt;\n    pmap_dfr(\n      ~slowly_get_roster(tibble(...), root_url),\n      .progress = FALSE  # Turn off purrr's progress since we're using our own\n    )\n}\n\n# Usage:\nrosters &lt;- get_all_rosters(df, root_url)\n\nrosters &lt;- rosters |&gt; \n  mutate(Season = season) |&gt; \n  mutate(Team = gsub(\"()\", \"\", Team, fixed = TRUE))\n\nrosters &lt;- rosters |&gt; \n  mutate(Team = gsub(\"()\", \"\", Team, fixed = TRUE))\n\nNext, the team stats. This is really done in two steps – first the fetching, then the cleaning and separating. Because the team data and the player data appears in the same table, it makes no sense to do this twice, so first we get the data and then we create the products we want out of it.\nThis too fill fire off ~350 requests to stats.ncaa.org, so once again, we slowly get this data.\n\nget_team_stats &lt;- function(team_name, conference, url) {\n  # Construct full URL\n  player_stats_url &lt;- paste0(root_url, url, \"/season_to_date_stats\")\n  \n  # Get and process the page\n  player_stats_page &lt;- player_stats_url |&gt; \n    read_html()\n  \n  # Get stats table\n  stats_table &lt;- player_stats_page |&gt; \n    html_elements(\"table\") |&gt; \n    html_table() |&gt; \n    pluck(1) \n  \n  stats_table &lt;- stats_table |&gt; \n    mutate(across(everything(), as.character)) |&gt; \n    mutate(\n      Team = team_name,\n      Conference = conference\n    ) \n    \n  return(stats_table)\n}\n\n# Add sleep between iterations\nslowly_get_stats &lt;- slowly(get_team_stats, rate = rate_delay(3))\n\nteam_stats &lt;- pmap_dfr(\n  list(\n    team_name = df$Team,\n    conference = df$Conference,\n    url = df$URL\n  ),\n  slowly_get_stats,\n  .progress = TRUE\n) |&gt; \n  relocate(Team, Conference)\n\nNow we make some data tables from our fetched data. First, team and opponent stats – offense and defense, really. Then, individual player stats.\n\nteam_totals &lt;- team_stats |&gt; filter(Player == \"Totals\") |&gt; \n  mutate(Season = season) |&gt; \n  select(Season, Team, Conference, 11:26) |&gt; \n  mutate_at(vars(4:19), ~as.numeric(gsub(\",\", \"\", .)))\n\nopponent_totals &lt;- team_stats |&gt; filter(Player == \"Opponent Totals\") |&gt; \n  mutate(Season = season) |&gt; \n  select(Season, Team, Conference, 11:26) |&gt; \n  mutate_at(vars(4:19), ~as.numeric(gsub(\",\", \"\", .))) |&gt; \n  rename_with(~paste0(\"Opponent_\", .), 4:19) \n\ntotal_stats &lt;- team_totals |&gt; inner_join(opponent_totals) |&gt; clean_names()\n\nplayer_stats &lt;- team_stats |&gt; \n  filter(Player != \"Opponent Totals\") |&gt; \n  filter(Player != \"Totals\") |&gt; \n  filter(Player != \"TEAM\") |&gt; \n  rename(Name = Player) |&gt; \n  mutate(Season = season) |&gt; \n  select(Season, everything()) |&gt; \n  mutate(`#` = as.numeric(`#`)) |&gt;  \n  mutate_at(vars(9:27), ~as.numeric(gsub(\",\", \"\", .))) |&gt; \n  select(1:27) |&gt; \n  mutate(Team = gsub(\"()\", \"\", Team, fixed = TRUE))\n\nLast but very much not least: Write these out as csvs. You do not want to be running this every time you need volleyball data. The NCAA doesn’t update these numbers in real time – it’s some hours after a match is played – and the rankings don’t update but once a week. How often you need to scrape this is up to you, but make sure you’re using csvs in your analysis and not the results of the scraper.\n\nwrite_csv(players, playerfilename)\n\nwrite_csv(total_stats, teamfilename)"
  }
]